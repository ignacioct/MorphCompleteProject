{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training an inflectional system\n",
        "\n",
        "## Inputs & Installs"
      ],
      "metadata": {
        "id": "AnmSkrVyFnc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L44JJRSjdF8S",
        "outputId": "2d2d8b0e-6460-470c-99dc-83ebc4a95f1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login aa0b9ecff47af231f410704977e504d7928ffb05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAAylMkWdHSj",
        "outputId": "555ae2d7-b7c1-497c-ea99-a04992e7564d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"danish-inflection-gamma\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "1W8fAQJEdaFW",
        "outputId": "cfbcb444-8ed3-427d-eb1a-aff2eba80ca8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mignacioct_\u001b[0m (\u001b[33mignacio_at_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240107_191757-so4qg0j6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ignacio_at_ai/danish-inflection-gamma/runs/so4qg0j6' target=\"_blank\">light-dust-1</a></strong> to <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-gamma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-gamma' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection-gamma</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-gamma/runs/so4qg0j6' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection-gamma/runs/so4qg0j6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ignacio_at_ai/danish-inflection-gamma/runs/so4qg0j6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7a00618729e0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBSGWunsEwRs",
        "outputId": "1fa4cac2-5bcf-4ea4-b611-92b30958c034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.7)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291814 sha256=a93ef5765ef801fccf3ebbcdf07eba6b564c64ca3936ee0e69662a1656415e4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=cd21841b2207ad4ca39b95caf70b2faa3b90543eb3870745dafe1b1d9c4448c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OafxnvaWYdsS",
        "outputId": "1b779f43-9147-4554-8c02-4f3a4253d606"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data"
      ],
      "metadata": {
        "id": "OR7TeZKoF_Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh dan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0d-GDOTGBlD",
        "outputId": "fc9e98bf-4c91-4b89-a09f-8b19b4ebd71a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-07 19:19:41.876677: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-07 19:19:41.876733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-07 19:19:41.877674: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-07 19:19:41.882741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-07 19:19:42.761772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='dan.input', target_lang='dan.output', trainpref='train', validpref='dev', testpref=None, align_suffix=None, destdir='data-bin/dan', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 56 types\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.input] train.dan.input: 1399 sents, 20228 tokens, 0.084% replaced (by <unk>)\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 56 types\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.input] dev.dan.input: 300 sents, 4455 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.output] train.dan.output: 1399 sents, 17361 tokens, 0.0922% replaced (by <unk>)\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | [dan.output] dev.dan.output: 300 sents, 3855 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-07 19:19:45 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/dan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Train with default parameters, roughly the baseline in SIGMORPHON 2020 shared task\n",
        "Let this run until the loss on the validation (dev) test no longer improves. (Maybe 10 minutes with a GPU)."
      ],
      "metadata": {
        "id": "6i4VwjnlGHG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./train.sh dan --patience 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1iVwFdGIPz",
        "outputId": "24f4b7e0-a690-4d81-f2a3-f13ff3bed8b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-07 19:19:56.654392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-07 19:19:56.654445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-07 19:19:56.655352: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-07 19:19:56.660220: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-07 19:19:57.672010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-07 19:19:58 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-07 19:20:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/dan-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/dan-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/dan', source_lang='dan.input', target_lang='dan.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/dan', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-07 19:20:01 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 56 types\n",
            "2024-01-07 19:20:01 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(56, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | num. shared model params: 10,553,344 (num. trained: 10,553,344)\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-07 19:20:02 | INFO | fairseq.data.data_utils | loaded 300 examples from: data-bin/dan/valid.dan.input-dan.output.dan.input\n",
            "2024-01-07 19:20:02 | INFO | fairseq.data.data_utils | loaded 300 examples from: data-bin/dan/valid.dan.input-dan.output.dan.output\n",
            "2024-01-07 19:20:02 | INFO | fairseq.tasks.translation | data-bin/dan valid dan.input-dan.output 300 examples\n",
            "2024-01-07 19:20:02 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-07 19:20:02 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:20:02 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:20:02 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-07 19:20:02 | INFO | fairseq.data.data_utils | loaded 1,399 examples from: data-bin/dan/train.dan.input-dan.output.dan.input\n",
            "2024-01-07 19:20:02 | INFO | fairseq.data.data_utils | loaded 1,399 examples from: data-bin/dan/train.dan.input-dan.output.dan.output\n",
            "2024-01-07 19:20:02 | INFO | fairseq.tasks.translation | data-bin/dan train dan.input-dan.output 1399 examples\n",
            "2024-01-07 19:20:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 001:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:20:02 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-07 19:20:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:  80% 4/5 [00:12<00:03,  3.22s/it]2024-01-07 19:20:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:20:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.745 | nll_loss 11.76 | ppl 3467.28 | wps 2524.4 | wpb 1927.5 | bsz 150 | num_updates 5\n",
            "2024-01-07 19:20:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 5 updates\n",
            "2024-01-07 19:20:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 1 @ 5 updates, score 11.745) (writing took 0.27979251900001145 seconds)\n",
            "2024-01-07 19:20:20 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-07 19:20:20 | INFO | train | epoch 001 | loss 10.623 | nll_loss 10.641 | ppl 1596.36 | wps 885.6 | ups 0.3 | wpb 3472.2 | bsz 279.8 | num_updates 5 | lr 5e-06 | gnorm 7.421 | clip 100 | train_wall 17 | wall 19\n",
            "2024-01-07 19:20:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 002:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:20:20 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-07 19:20:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  80% 4/5 [00:12<00:03,  3.24s/it]2024-01-07 19:20:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:20:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.915 | nll_loss 10.917 | ppl 1933.85 | wps 2841.4 | wpb 1927.5 | bsz 150 | num_updates 10 | best_loss 10.915\n",
            "2024-01-07 19:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 10 updates\n",
            "2024-01-07 19:20:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 2 @ 10 updates, score 10.915) (writing took 0.38160655500001894 seconds)\n",
            "2024-01-07 19:20:40 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-07 19:20:40 | INFO | train | epoch 002 | loss 10.368 | nll_loss 10.381 | ppl 1333.77 | wps 910.8 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 10 | lr 1e-05 | gnorm 7.391 | clip 100 | train_wall 17 | wall 38\n",
            "2024-01-07 19:20:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 003:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:20:40 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-07 19:20:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  80% 4/5 [00:11<00:02,  2.39s/it]2024-01-07 19:20:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.61s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:20:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.704 | nll_loss 9.688 | ppl 824.82 | wps 2085.3 | wpb 1927.5 | bsz 150 | num_updates 15 | best_loss 9.704\n",
            "2024-01-07 19:20:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 15 updates\n",
            "2024-01-07 19:20:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:20:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 3 @ 15 updates, score 9.704) (writing took 0.4614741350000031 seconds)\n",
            "2024-01-07 19:20:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-07 19:20:58 | INFO | train | epoch 003 | loss 9.848 | nll_loss 9.85 | ppl 922.77 | wps 955.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 15 | lr 1.5e-05 | gnorm 7.149 | clip 100 | train_wall 16 | wall 56\n",
            "2024-01-07 19:20:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 004:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:20:58 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-07 19:20:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  80% 4/5 [00:12<00:03,  3.77s/it]2024-01-07 19:21:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:21:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.449 | nll_loss 8.414 | ppl 341.07 | wps 2646.9 | wpb 1927.5 | bsz 150 | num_updates 20 | best_loss 8.449\n",
            "2024-01-07 19:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 20 updates\n",
            "2024-01-07 19:21:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 4 @ 20 updates, score 8.449) (writing took 0.34715854399996715 seconds)\n",
            "2024-01-07 19:21:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-07 19:21:16 | INFO | train | epoch 004 | loss 8.99 | nll_loss 8.973 | ppl 502.56 | wps 963.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 20 | lr 2e-05 | gnorm 6.513 | clip 100 | train_wall 16 | wall 74\n",
            "2024-01-07 19:21:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 005:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:21:16 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-07 19:21:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  80% 4/5 [00:11<00:03,  3.20s/it]2024-01-07 19:21:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.78s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:21:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.387 | nll_loss 7.333 | ppl 161.2 | wps 2847.2 | wpb 1927.5 | bsz 150 | num_updates 25 | best_loss 7.387\n",
            "2024-01-07 19:21:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 25 updates\n",
            "2024-01-07 19:21:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 5 @ 25 updates, score 7.387) (writing took 0.35020015300000296 seconds)\n",
            "2024-01-07 19:21:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-07 19:21:34 | INFO | train | epoch 005 | loss 8.088 | nll_loss 8.053 | ppl 265.63 | wps 947.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 25 | lr 2.5e-05 | gnorm 5.408 | clip 100 | train_wall 16 | wall 92\n",
            "2024-01-07 19:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 006:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:21:34 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-07 19:21:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  80% 4/5 [00:15<00:04,  4.05s/it]2024-01-07 19:21:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:21:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.575 | nll_loss 6.502 | ppl 90.64 | wps 2812 | wpb 1927.5 | bsz 150 | num_updates 30 | best_loss 6.575\n",
            "2024-01-07 19:21:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 30 updates\n",
            "2024-01-07 19:21:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:21:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 6 @ 30 updates, score 6.575) (writing took 0.33364610300003505 seconds)\n",
            "2024-01-07 19:21:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-07 19:21:52 | INFO | train | epoch 006 | loss 7.35 | nll_loss 7.3 | ppl 157.53 | wps 971.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 30 | lr 3e-05 | gnorm 4.048 | clip 100 | train_wall 16 | wall 110\n",
            "2024-01-07 19:21:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 007:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:21:52 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-01-07 19:21:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  80% 4/5 [00:12<00:03,  3.57s/it]2024-01-07 19:22:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:22:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.981 | nll_loss 5.889 | ppl 59.26 | wps 2835.4 | wpb 1927.5 | bsz 150 | num_updates 35 | best_loss 5.981\n",
            "2024-01-07 19:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 35 updates\n",
            "2024-01-07 19:22:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 7 @ 35 updates, score 5.981) (writing took 0.33529532299996845 seconds)\n",
            "2024-01-07 19:22:10 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-01-07 19:22:10 | INFO | train | epoch 007 | loss 6.534 | nll_loss 6.462 | ppl 88.16 | wps 947.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 35 | lr 3.5e-05 | gnorm 3.032 | clip 100 | train_wall 16 | wall 129\n",
            "2024-01-07 19:22:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 008:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:22:10 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-01-07 19:22:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  80% 4/5 [00:15<00:04,  4.17s/it]2024-01-07 19:22:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:22:28 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.535 | nll_loss 5.422 | ppl 42.87 | wps 2583.9 | wpb 1927.5 | bsz 150 | num_updates 40 | best_loss 5.535\n",
            "2024-01-07 19:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 40 updates\n",
            "2024-01-07 19:22:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 8 @ 40 updates, score 5.535) (writing took 0.3452775899999665 seconds)\n",
            "2024-01-07 19:22:28 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-01-07 19:22:28 | INFO | train | epoch 008 | loss 6.03 | nll_loss 5.939 | ppl 61.37 | wps 961.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 40 | lr 4e-05 | gnorm 2.302 | clip 100 | train_wall 16 | wall 147\n",
            "2024-01-07 19:22:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 009:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:22:28 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-01-07 19:22:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  80% 4/5 [00:12<00:02,  2.39s/it]2024-01-07 19:22:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:22:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.212 | nll_loss 5.077 | ppl 33.77 | wps 2818.5 | wpb 1927.5 | bsz 150 | num_updates 45 | best_loss 5.212\n",
            "2024-01-07 19:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 45 updates\n",
            "2024-01-07 19:22:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:22:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 9 @ 45 updates, score 5.212) (writing took 0.32750350300000264 seconds)\n",
            "2024-01-07 19:22:47 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-01-07 19:22:47 | INFO | train | epoch 009 | loss 5.593 | nll_loss 5.482 | ppl 44.69 | wps 948.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 45 | lr 4.5e-05 | gnorm 1.771 | clip 100 | train_wall 16 | wall 165\n",
            "2024-01-07 19:22:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 010:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:22:47 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-01-07 19:22:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  80% 4/5 [00:12<00:03,  3.06s/it]2024-01-07 19:23:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:23:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.982 | nll_loss 4.827 | ppl 28.38 | wps 2839.6 | wpb 1927.5 | bsz 150 | num_updates 50 | best_loss 4.982\n",
            "2024-01-07 19:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 50 updates\n",
            "2024-01-07 19:23:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 10 @ 50 updates, score 4.982) (writing took 0.33257129800000484 seconds)\n",
            "2024-01-07 19:23:04 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-01-07 19:23:04 | INFO | train | epoch 010 | loss 5.262 | nll_loss 5.129 | ppl 34.99 | wps 969.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 50 | lr 5e-05 | gnorm 1.386 | clip 100 | train_wall 16 | wall 183\n",
            "2024-01-07 19:23:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 011:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:23:04 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-01-07 19:23:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  80% 4/5 [00:12<00:02,  2.70s/it]2024-01-07 19:23:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:23:22 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.816 | nll_loss 4.642 | ppl 24.96 | wps 2793.4 | wpb 1927.5 | bsz 150 | num_updates 55 | best_loss 4.816\n",
            "2024-01-07 19:23:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 55 updates\n",
            "2024-01-07 19:23:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 11 @ 55 updates, score 4.816) (writing took 0.34011348699999644 seconds)\n",
            "2024-01-07 19:23:23 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-01-07 19:23:23 | INFO | train | epoch 011 | loss 5.049 | nll_loss 4.897 | ppl 29.79 | wps 953 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 55 | lr 5.5e-05 | gnorm 1.124 | clip 40 | train_wall 16 | wall 201\n",
            "2024-01-07 19:23:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 012:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:23:23 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-01-07 19:23:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  80% 4/5 [00:11<00:02,  2.76s/it]2024-01-07 19:23:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:23:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.707 | nll_loss 4.518 | ppl 22.91 | wps 1928.6 | wpb 1927.5 | bsz 150 | num_updates 60 | best_loss 4.707\n",
            "2024-01-07 19:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 60 updates\n",
            "2024-01-07 19:23:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 12 @ 60 updates, score 4.707) (writing took 0.49813434199995754 seconds)\n",
            "2024-01-07 19:23:41 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-01-07 19:23:41 | INFO | train | epoch 012 | loss 4.872 | nll_loss 4.7 | ppl 26 | wps 961.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 60 | lr 6e-05 | gnorm 0.98 | clip 40 | train_wall 16 | wall 219\n",
            "2024-01-07 19:23:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 013:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:23:41 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-01-07 19:23:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  80% 4/5 [00:13<00:03,  3.14s/it]2024-01-07 19:23:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:23:59 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.624 | nll_loss 4.424 | ppl 21.46 | wps 2578.8 | wpb 1927.5 | bsz 150 | num_updates 65 | best_loss 4.624\n",
            "2024-01-07 19:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 65 updates\n",
            "2024-01-07 19:23:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 13 @ 65 updates, score 4.624) (writing took 0.3662861029999931 seconds)\n",
            "2024-01-07 19:23:59 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-01-07 19:23:59 | INFO | train | epoch 013 | loss 4.776 | nll_loss 4.593 | ppl 24.13 | wps 949 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 65 | lr 6.5e-05 | gnorm 0.9 | clip 40 | train_wall 16 | wall 237\n",
            "2024-01-07 19:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 014:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:23:59 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-01-07 19:23:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  80% 4/5 [00:12<00:02,  2.95s/it]2024-01-07 19:24:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.82s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:24:17 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.557 | nll_loss 4.349 | ppl 20.37 | wps 2035.2 | wpb 1927.5 | bsz 150 | num_updates 70 | best_loss 4.557\n",
            "2024-01-07 19:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 70 updates\n",
            "2024-01-07 19:24:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 14 @ 70 updates, score 4.557) (writing took 0.3363918969999986 seconds)\n",
            "2024-01-07 19:24:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-01-07 19:24:17 | INFO | train | epoch 014 | loss 4.688 | nll_loss 4.494 | ppl 22.53 | wps 944.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 70 | lr 7e-05 | gnorm 0.884 | clip 40 | train_wall 16 | wall 256\n",
            "2024-01-07 19:24:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 015:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:24:17 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-01-07 19:24:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  80% 4/5 [00:12<00:03,  3.12s/it]2024-01-07 19:24:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:24:35 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.494 | nll_loss 4.277 | ppl 19.39 | wps 2698.1 | wpb 1927.5 | bsz 150 | num_updates 75 | best_loss 4.494\n",
            "2024-01-07 19:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 75 updates\n",
            "2024-01-07 19:24:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 15 @ 75 updates, score 4.494) (writing took 0.33049069899999495 seconds)\n",
            "2024-01-07 19:24:35 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-01-07 19:24:35 | INFO | train | epoch 015 | loss 4.607 | nll_loss 4.403 | ppl 21.16 | wps 962 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 75 | lr 7.5e-05 | gnorm 0.848 | clip 40 | train_wall 16 | wall 274\n",
            "2024-01-07 19:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 016:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:24:36 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-01-07 19:24:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  80% 4/5 [00:12<00:03,  3.35s/it]2024-01-07 19:24:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.50s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:24:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.432 | nll_loss 4.208 | ppl 18.49 | wps 2871.5 | wpb 1927.5 | bsz 150 | num_updates 80 | best_loss 4.432\n",
            "2024-01-07 19:24:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 80 updates\n",
            "2024-01-07 19:24:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 16 @ 80 updates, score 4.432) (writing took 0.3742861800000128 seconds)\n",
            "2024-01-07 19:24:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-01-07 19:24:54 | INFO | train | epoch 016 | loss 4.545 | nll_loss 4.333 | ppl 20.16 | wps 947.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 80 | lr 8e-05 | gnorm 0.853 | clip 20 | train_wall 16 | wall 292\n",
            "2024-01-07 19:24:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 017:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:24:54 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-01-07 19:24:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  80% 4/5 [00:11<00:03,  3.26s/it]2024-01-07 19:25:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:25:11 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.362 | nll_loss 4.128 | ppl 17.49 | wps 2859.2 | wpb 1927.5 | bsz 150 | num_updates 85 | best_loss 4.362\n",
            "2024-01-07 19:25:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 85 updates\n",
            "2024-01-07 19:25:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 17 @ 85 updates, score 4.362) (writing took 0.3498089830000026 seconds)\n",
            "2024-01-07 19:25:12 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-01-07 19:25:12 | INFO | train | epoch 017 | loss 4.484 | nll_loss 4.265 | ppl 19.23 | wps 966.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 85 | lr 8.5e-05 | gnorm 0.832 | clip 20 | train_wall 16 | wall 310\n",
            "2024-01-07 19:25:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 018:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:25:12 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-01-07 19:25:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  80% 4/5 [00:11<00:02,  3.00s/it]2024-01-07 19:25:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:25:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.277 | nll_loss 4.031 | ppl 16.35 | wps 2585.4 | wpb 1927.5 | bsz 150 | num_updates 90 | best_loss 4.277\n",
            "2024-01-07 19:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 90 updates\n",
            "2024-01-07 19:25:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 18 @ 90 updates, score 4.277) (writing took 0.339650593999977 seconds)\n",
            "2024-01-07 19:25:30 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-01-07 19:25:30 | INFO | train | epoch 018 | loss 4.415 | nll_loss 4.187 | ppl 18.21 | wps 947.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 90 | lr 9e-05 | gnorm 0.835 | clip 20 | train_wall 16 | wall 328\n",
            "2024-01-07 19:25:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 019:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:25:30 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-01-07 19:25:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  80% 4/5 [00:12<00:03,  3.49s/it]2024-01-07 19:25:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:25:48 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.175 | nll_loss 3.911 | ppl 15.04 | wps 2620.1 | wpb 1927.5 | bsz 150 | num_updates 95 | best_loss 4.175\n",
            "2024-01-07 19:25:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 95 updates\n",
            "2024-01-07 19:25:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 19 @ 95 updates, score 4.175) (writing took 0.3303691749999871 seconds)\n",
            "2024-01-07 19:25:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-01-07 19:25:48 | INFO | train | epoch 019 | loss 4.337 | nll_loss 4.098 | ppl 17.13 | wps 970 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 95 | lr 9.5e-05 | gnorm 0.869 | clip 20 | train_wall 16 | wall 346\n",
            "2024-01-07 19:25:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 020:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:25:48 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-01-07 19:25:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  80% 4/5 [00:12<00:03,  3.26s/it]2024-01-07 19:26:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:26:06 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.068 | nll_loss 3.78 | ppl 13.74 | wps 2896.1 | wpb 1927.5 | bsz 150 | num_updates 100 | best_loss 4.068\n",
            "2024-01-07 19:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 100 updates\n",
            "2024-01-07 19:26:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 20 @ 100 updates, score 4.068) (writing took 0.3190650649999043 seconds)\n",
            "2024-01-07 19:26:06 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-01-07 19:26:06 | INFO | train | epoch 020 | loss 4.231 | nll_loss 3.975 | ppl 15.72 | wps 950.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 100 | lr 0.0001 | gnorm 0.863 | clip 20 | train_wall 16 | wall 365\n",
            "2024-01-07 19:26:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 021:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:26:06 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-01-07 19:26:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  80% 4/5 [00:17<00:04,  4.17s/it]2024-01-07 19:26:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:26:26 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.971 | nll_loss 3.645 | ppl 12.51 | wps 2149.1 | wpb 1927.5 | bsz 150 | num_updates 105 | best_loss 3.971\n",
            "2024-01-07 19:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 105 updates\n",
            "2024-01-07 19:26:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 21 @ 105 updates, score 3.971) (writing took 0.7645488989999194 seconds)\n",
            "2024-01-07 19:26:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-01-07 19:26:26 | INFO | train | epoch 021 | loss 4.139 | nll_loss 3.864 | ppl 14.56 | wps 867.6 | ups 0.25 | wpb 3472.2 | bsz 279.8 | num_updates 105 | lr 0.000105 | gnorm 0.869 | clip 20 | train_wall 18 | wall 385\n",
            "2024-01-07 19:26:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 022:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:26:26 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-01-07 19:26:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  80% 4/5 [00:13<00:03,  3.84s/it]2024-01-07 19:26:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:26:44 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.9 | nll_loss 3.534 | ppl 11.58 | wps 2690.1 | wpb 1927.5 | bsz 150 | num_updates 110 | best_loss 3.9\n",
            "2024-01-07 19:26:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 110 updates\n",
            "2024-01-07 19:26:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 22 @ 110 updates, score 3.9) (writing took 0.34081970299996556 seconds)\n",
            "2024-01-07 19:26:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-01-07 19:26:45 | INFO | train | epoch 022 | loss 4.026 | nll_loss 3.717 | ppl 13.15 | wps 945.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 110 | lr 0.00011 | gnorm 0.912 | clip 20 | train_wall 16 | wall 403\n",
            "2024-01-07 19:26:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 023:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:26:45 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-01-07 19:26:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  80% 4/5 [00:11<00:03,  3.23s/it]2024-01-07 19:27:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:27:03 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.826 | nll_loss 3.428 | ppl 10.76 | wps 2903.8 | wpb 1927.5 | bsz 150 | num_updates 115 | best_loss 3.826\n",
            "2024-01-07 19:27:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 115 updates\n",
            "2024-01-07 19:27:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 23 @ 115 updates, score 3.826) (writing took 0.3701393259999577 seconds)\n",
            "2024-01-07 19:27:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-01-07 19:27:03 | INFO | train | epoch 023 | loss 3.93 | nll_loss 3.591 | ppl 12.05 | wps 937.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 115 | lr 0.000115 | gnorm 0.824 | clip 40 | train_wall 16 | wall 422\n",
            "2024-01-07 19:27:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 024:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:27:03 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-01-07 19:27:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  80% 4/5 [00:15<00:04,  4.04s/it]2024-01-07 19:27:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:27:21 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.754 | nll_loss 3.337 | ppl 10.11 | wps 2811.5 | wpb 1927.5 | bsz 150 | num_updates 120 | best_loss 3.754\n",
            "2024-01-07 19:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 120 updates\n",
            "2024-01-07 19:27:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 24 @ 120 updates, score 3.754) (writing took 0.34988230199996906 seconds)\n",
            "2024-01-07 19:27:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-01-07 19:27:21 | INFO | train | epoch 024 | loss 3.876 | nll_loss 3.518 | ppl 11.46 | wps 967.1 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 120 | lr 0.00012 | gnorm 0.927 | clip 20 | train_wall 16 | wall 440\n",
            "2024-01-07 19:27:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 025:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:27:21 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-01-07 19:27:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  80% 4/5 [00:12<00:03,  3.60s/it]2024-01-07 19:27:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:27:39 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.699 | nll_loss 3.284 | ppl 9.74 | wps 2765.9 | wpb 1927.5 | bsz 150 | num_updates 125 | best_loss 3.699\n",
            "2024-01-07 19:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 125 updates\n",
            "2024-01-07 19:27:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 25 @ 125 updates, score 3.699) (writing took 0.4245558739999069 seconds)\n",
            "2024-01-07 19:27:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-01-07 19:27:40 | INFO | train | epoch 025 | loss 3.822 | nll_loss 3.463 | ppl 11.03 | wps 937.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 125 | lr 0.000125 | gnorm 1.081 | clip 40 | train_wall 17 | wall 458\n",
            "2024-01-07 19:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 026:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:27:40 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-01-07 19:27:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  80% 4/5 [00:12<00:03,  3.03s/it]2024-01-07 19:27:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:27:57 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.68 | nll_loss 3.24 | ppl 9.45 | wps 2838.7 | wpb 1927.5 | bsz 150 | num_updates 130 | best_loss 3.68\n",
            "2024-01-07 19:27:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 130 updates\n",
            "2024-01-07 19:27:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:27:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 26 @ 130 updates, score 3.68) (writing took 0.3483811120000837 seconds)\n",
            "2024-01-07 19:27:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-01-07 19:27:58 | INFO | train | epoch 026 | loss 3.757 | nll_loss 3.382 | ppl 10.42 | wps 966 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 130 | lr 0.00013 | gnorm 0.979 | clip 40 | train_wall 16 | wall 476\n",
            "2024-01-07 19:27:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 027:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:27:58 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-01-07 19:27:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  80% 4/5 [00:12<00:02,  2.36s/it]2024-01-07 19:28:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:28:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.606 | nll_loss 3.165 | ppl 8.97 | wps 2883.6 | wpb 1927.5 | bsz 150 | num_updates 135 | best_loss 3.606\n",
            "2024-01-07 19:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 135 updates\n",
            "2024-01-07 19:28:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 27 @ 135 updates, score 3.606) (writing took 0.3477667519999841 seconds)\n",
            "2024-01-07 19:28:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-01-07 19:28:16 | INFO | train | epoch 027 | loss 3.696 | nll_loss 3.305 | ppl 9.89 | wps 945.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 135 | lr 0.000135 | gnorm 0.953 | clip 20 | train_wall 16 | wall 494\n",
            "2024-01-07 19:28:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 028:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:28:16 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-01-07 19:28:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  80% 4/5 [00:12<00:03,  3.36s/it]2024-01-07 19:28:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:28:34 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.555 | nll_loss 3.08 | ppl 8.45 | wps 2924.2 | wpb 1927.5 | bsz 150 | num_updates 140 | best_loss 3.555\n",
            "2024-01-07 19:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 140 updates\n",
            "2024-01-07 19:28:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 28 @ 140 updates, score 3.555) (writing took 0.38197403599997415 seconds)\n",
            "2024-01-07 19:28:34 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-01-07 19:28:34 | INFO | train | epoch 028 | loss 3.655 | nll_loss 3.256 | ppl 9.56 | wps 962.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 140 | lr 0.00014 | gnorm 0.965 | clip 40 | train_wall 16 | wall 512\n",
            "2024-01-07 19:28:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 029:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:28:34 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-01-07 19:28:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  80% 4/5 [00:16<00:04,  4.17s/it]2024-01-07 19:28:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:28:52 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.494 | nll_loss 3.009 | ppl 8.05 | wps 2792.3 | wpb 1927.5 | bsz 150 | num_updates 145 | best_loss 3.494\n",
            "2024-01-07 19:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 145 updates\n",
            "2024-01-07 19:28:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:28:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 29 @ 145 updates, score 3.494) (writing took 0.4157148229999166 seconds)\n",
            "2024-01-07 19:28:53 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-01-07 19:28:53 | INFO | train | epoch 029 | loss 3.583 | nll_loss 3.164 | ppl 8.97 | wps 935.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 145 | lr 0.000145 | gnorm 0.889 | clip 20 | train_wall 17 | wall 531\n",
            "2024-01-07 19:28:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 030:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:28:53 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-01-07 19:28:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  80% 4/5 [00:11<00:03,  3.00s/it]2024-01-07 19:29:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.61s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:29:10 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.444 | nll_loss 2.935 | ppl 7.65 | wps 1894.5 | wpb 1927.5 | bsz 150 | num_updates 150 | best_loss 3.444\n",
            "2024-01-07 19:29:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 150 updates\n",
            "2024-01-07 19:29:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 30 @ 150 updates, score 3.444) (writing took 0.5983161760000257 seconds)\n",
            "2024-01-07 19:29:11 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-01-07 19:29:11 | INFO | train | epoch 030 | loss 3.531 | nll_loss 3.095 | ppl 8.54 | wps 941 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 150 | lr 0.00015 | gnorm 0.867 | clip 40 | train_wall 16 | wall 549\n",
            "2024-01-07 19:29:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 031:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:29:11 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-01-07 19:29:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  80% 4/5 [00:12<00:03,  3.74s/it]2024-01-07 19:29:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:29:29 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.39 | nll_loss 2.871 | ppl 7.32 | wps 2574.5 | wpb 1927.5 | bsz 150 | num_updates 155 | best_loss 3.39\n",
            "2024-01-07 19:29:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 155 updates\n",
            "2024-01-07 19:29:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 31 @ 155 updates, score 3.39) (writing took 0.39465160800000376 seconds)\n",
            "2024-01-07 19:29:29 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-01-07 19:29:29 | INFO | train | epoch 031 | loss 3.475 | nll_loss 3.034 | ppl 8.19 | wps 958.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 155 | lr 0.000155 | gnorm 0.714 | clip 0 | train_wall 16 | wall 567\n",
            "2024-01-07 19:29:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 032:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:29:29 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-01-07 19:29:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  80% 4/5 [00:16<00:03,  3.82s/it]2024-01-07 19:29:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.74s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:29:47 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.333 | nll_loss 2.798 | ppl 6.96 | wps 2701.4 | wpb 1927.5 | bsz 150 | num_updates 160 | best_loss 3.333\n",
            "2024-01-07 19:29:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 160 updates\n",
            "2024-01-07 19:29:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:29:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 32 @ 160 updates, score 3.333) (writing took 0.434442353999998 seconds)\n",
            "2024-01-07 19:29:48 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-01-07 19:29:48 | INFO | train | epoch 032 | loss 3.415 | nll_loss 2.965 | ppl 7.81 | wps 935.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 160 | lr 0.00016 | gnorm 0.657 | clip 20 | train_wall 16 | wall 586\n",
            "2024-01-07 19:29:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 033:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:29:48 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-01-07 19:29:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  80% 4/5 [00:12<00:03,  3.41s/it]2024-01-07 19:30:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:30:05 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.308 | nll_loss 2.762 | ppl 6.78 | wps 2917.2 | wpb 1927.5 | bsz 150 | num_updates 165 | best_loss 3.308\n",
            "2024-01-07 19:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 165 updates\n",
            "2024-01-07 19:30:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 33 @ 165 updates, score 3.308) (writing took 0.4114614080000365 seconds)\n",
            "2024-01-07 19:30:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-01-07 19:30:06 | INFO | train | epoch 033 | loss 3.349 | nll_loss 2.879 | ppl 7.36 | wps 956.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 165 | lr 0.000165 | gnorm 0.71 | clip 0 | train_wall 16 | wall 604\n",
            "2024-01-07 19:30:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 034:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:30:06 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-01-07 19:30:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  80% 4/5 [00:12<00:03,  3.02s/it]2024-01-07 19:30:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:30:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.257 | nll_loss 2.69 | ppl 6.45 | wps 2699.5 | wpb 1927.5 | bsz 150 | num_updates 170 | best_loss 3.257\n",
            "2024-01-07 19:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 170 updates\n",
            "2024-01-07 19:30:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 34 @ 170 updates, score 3.257) (writing took 0.39620391900007235 seconds)\n",
            "2024-01-07 19:30:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-01-07 19:30:24 | INFO | train | epoch 034 | loss 3.313 | nll_loss 2.846 | ppl 7.19 | wps 938.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 170 | lr 0.00017 | gnorm 0.93 | clip 20 | train_wall 17 | wall 623\n",
            "2024-01-07 19:30:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 035:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:30:24 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-01-07 19:30:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  80% 4/5 [00:11<00:02,  2.45s/it]2024-01-07 19:30:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:30:42 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.158 | nll_loss 2.57 | ppl 5.94 | wps 2803.4 | wpb 1927.5 | bsz 150 | num_updates 175 | best_loss 3.158\n",
            "2024-01-07 19:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 175 updates\n",
            "2024-01-07 19:30:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:30:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 35 @ 175 updates, score 3.158) (writing took 0.45111876500004655 seconds)\n",
            "2024-01-07 19:30:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-01-07 19:30:42 | INFO | train | epoch 035 | loss 3.247 | nll_loss 2.766 | ppl 6.8 | wps 957.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 175 | lr 0.000175 | gnorm 0.881 | clip 20 | train_wall 16 | wall 641\n",
            "2024-01-07 19:30:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 036:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:30:42 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-01-07 19:30:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  80% 4/5 [00:13<00:03,  3.02s/it]2024-01-07 19:30:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:31:01 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.114 | nll_loss 2.504 | ppl 5.67 | wps 2855 | wpb 1927.5 | bsz 150 | num_updates 180 | best_loss 3.114\n",
            "2024-01-07 19:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 180 updates\n",
            "2024-01-07 19:31:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:31:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 36 @ 180 updates, score 3.114) (writing took 0.3501868439999498 seconds)\n",
            "2024-01-07 19:31:01 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-01-07 19:31:01 | INFO | train | epoch 036 | loss 3.192 | nll_loss 2.691 | ppl 6.46 | wps 939.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 180 | lr 0.00018 | gnorm 0.777 | clip 20 | train_wall 17 | wall 659\n",
            "2024-01-07 19:31:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 037:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:31:01 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-01-07 19:31:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  80% 4/5 [00:12<00:02,  2.77s/it]2024-01-07 19:31:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:31:19 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.019 | nll_loss 2.423 | ppl 5.36 | wps 2907.9 | wpb 1927.5 | bsz 150 | num_updates 185 | best_loss 3.019\n",
            "2024-01-07 19:31:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 185 updates\n",
            "2024-01-07 19:31:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 37 @ 185 updates, score 3.019) (writing took 0.3854385199999797 seconds)\n",
            "2024-01-07 19:31:19 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-01-07 19:31:19 | INFO | train | epoch 037 | loss 3.13 | nll_loss 2.62 | ppl 6.15 | wps 963.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 185 | lr 0.000185 | gnorm 0.838 | clip 20 | train_wall 16 | wall 677\n",
            "2024-01-07 19:31:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 038:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:31:19 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-01-07 19:31:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  80% 4/5 [00:12<00:02,  2.39s/it]2024-01-07 19:31:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:31:37 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.086 | nll_loss 2.449 | ppl 5.46 | wps 2773.2 | wpb 1927.5 | bsz 150 | num_updates 190 | best_loss 3.019\n",
            "2024-01-07 19:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 190 updates\n",
            "2024-01-07 19:31:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 38 @ 190 updates, score 3.086) (writing took 0.19280629000002136 seconds)\n",
            "2024-01-07 19:31:37 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-01-07 19:31:37 | INFO | train | epoch 038 | loss 3.104 | nll_loss 2.595 | ppl 6.04 | wps 953.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 190 | lr 0.00019 | gnorm 1.104 | clip 60 | train_wall 16 | wall 696\n",
            "2024-01-07 19:31:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 039:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:31:37 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-01-07 19:31:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  80% 4/5 [00:12<00:02,  2.58s/it]2024-01-07 19:31:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.63s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:31:55 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.024 | nll_loss 2.385 | ppl 5.22 | wps 2147.8 | wpb 1927.5 | bsz 150 | num_updates 195 | best_loss 3.019\n",
            "2024-01-07 19:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 195 updates\n",
            "2024-01-07 19:31:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 39 @ 195 updates, score 3.024) (writing took 0.2403582979999328 seconds)\n",
            "2024-01-07 19:31:55 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-01-07 19:31:55 | INFO | train | epoch 039 | loss 3.071 | nll_loss 2.534 | ppl 5.79 | wps 964.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 195 | lr 0.000195 | gnorm 1.263 | clip 80 | train_wall 16 | wall 714\n",
            "2024-01-07 19:31:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 040:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:31:55 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-01-07 19:31:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  80% 4/5 [00:12<00:02,  2.91s/it]2024-01-07 19:32:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:32:13 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 2.9 | nll_loss 2.272 | ppl 4.83 | wps 2815.7 | wpb 1927.5 | bsz 150 | num_updates 200 | best_loss 2.9\n",
            "2024-01-07 19:32:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 200 updates\n",
            "2024-01-07 19:32:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 40 @ 200 updates, score 2.9) (writing took 0.3587445130000333 seconds)\n",
            "2024-01-07 19:32:13 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-01-07 19:32:13 | INFO | train | epoch 040 | loss 3.019 | nll_loss 2.489 | ppl 5.61 | wps 956.1 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 200 | lr 0.0002 | gnorm 1.517 | clip 80 | train_wall 16 | wall 732\n",
            "2024-01-07 19:32:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 041:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:32:13 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-01-07 19:32:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  80% 4/5 [00:12<00:03,  3.38s/it]2024-01-07 19:32:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.79s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:32:31 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 2.816 | nll_loss 2.16 | ppl 4.47 | wps 2904.5 | wpb 1927.5 | bsz 150 | num_updates 205 | best_loss 2.816\n",
            "2024-01-07 19:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 205 updates\n",
            "2024-01-07 19:32:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 41 @ 205 updates, score 2.816) (writing took 0.3650086930000498 seconds)\n",
            "2024-01-07 19:32:32 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-01-07 19:32:32 | INFO | train | epoch 041 | loss 2.976 | nll_loss 2.443 | ppl 5.44 | wps 948 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 205 | lr 0.000205 | gnorm 1.232 | clip 60 | train_wall 16 | wall 750\n",
            "2024-01-07 19:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 042:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:32:32 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-01-07 19:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  80% 4/5 [00:15<00:04,  4.00s/it]2024-01-07 19:32:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:32:49 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 2.762 | nll_loss 2.079 | ppl 4.23 | wps 2727.6 | wpb 1927.5 | bsz 150 | num_updates 210 | best_loss 2.762\n",
            "2024-01-07 19:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 210 updates\n",
            "2024-01-07 19:32:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:32:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 42 @ 210 updates, score 2.762) (writing took 0.3301351300000306 seconds)\n",
            "2024-01-07 19:32:50 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-01-07 19:32:50 | INFO | train | epoch 042 | loss 2.917 | nll_loss 2.375 | ppl 5.19 | wps 969 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 210 | lr 0.00021 | gnorm 1.068 | clip 40 | train_wall 16 | wall 768\n",
            "2024-01-07 19:32:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 043:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:32:50 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-01-07 19:32:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  80% 4/5 [00:16<00:03,  3.96s/it]2024-01-07 19:33:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:33:08 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 2.728 | nll_loss 2.035 | ppl 4.1 | wps 2882.7 | wpb 1927.5 | bsz 150 | num_updates 215 | best_loss 2.728\n",
            "2024-01-07 19:33:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 215 updates\n",
            "2024-01-07 19:33:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 43 @ 215 updates, score 2.728) (writing took 0.3350107090000165 seconds)\n",
            "2024-01-07 19:33:08 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-01-07 19:33:08 | INFO | train | epoch 043 | loss 2.878 | nll_loss 2.297 | ppl 4.91 | wps 945 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 215 | lr 0.000215 | gnorm 1.111 | clip 60 | train_wall 16 | wall 786\n",
            "2024-01-07 19:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 044:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:33:08 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-01-07 19:33:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  80% 4/5 [00:12<00:03,  3.54s/it]2024-01-07 19:33:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.40s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:33:25 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 2.627 | nll_loss 1.93 | ppl 3.81 | wps 2847.5 | wpb 1927.5 | bsz 150 | num_updates 220 | best_loss 2.627\n",
            "2024-01-07 19:33:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 220 updates\n",
            "2024-01-07 19:33:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 44 @ 220 updates, score 2.627) (writing took 0.33642922900003214 seconds)\n",
            "2024-01-07 19:33:26 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-01-07 19:33:26 | INFO | train | epoch 044 | loss 2.809 | nll_loss 2.229 | ppl 4.69 | wps 971.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 220 | lr 0.00022 | gnorm 1.149 | clip 20 | train_wall 16 | wall 804\n",
            "2024-01-07 19:33:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 045:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:33:26 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-01-07 19:33:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  80% 4/5 [00:12<00:02,  2.62s/it]2024-01-07 19:33:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:33:44 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 2.571 | nll_loss 1.841 | ppl 3.58 | wps 2754.4 | wpb 1927.5 | bsz 150 | num_updates 225 | best_loss 2.571\n",
            "2024-01-07 19:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 225 updates\n",
            "2024-01-07 19:33:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 45 @ 225 updates, score 2.571) (writing took 0.36158326299982946 seconds)\n",
            "2024-01-07 19:33:44 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-01-07 19:33:44 | INFO | train | epoch 045 | loss 2.755 | nll_loss 2.18 | ppl 4.53 | wps 938.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 225 | lr 0.000225 | gnorm 1.005 | clip 40 | train_wall 17 | wall 823\n",
            "2024-01-07 19:33:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 046:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:33:44 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-01-07 19:33:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  80% 4/5 [00:12<00:02,  2.44s/it]2024-01-07 19:34:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:34:02 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 2.499 | nll_loss 1.747 | ppl 3.36 | wps 2798.9 | wpb 1927.5 | bsz 150 | num_updates 230 | best_loss 2.499\n",
            "2024-01-07 19:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 230 updates\n",
            "2024-01-07 19:34:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 46 @ 230 updates, score 2.499) (writing took 0.3250425759999871 seconds)\n",
            "2024-01-07 19:34:02 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-01-07 19:34:02 | INFO | train | epoch 046 | loss 2.693 | nll_loss 2.09 | ppl 4.26 | wps 966.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 230 | lr 0.00023 | gnorm 1.133 | clip 20 | train_wall 16 | wall 841\n",
            "2024-01-07 19:34:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 047:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:34:02 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-01-07 19:34:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  80% 4/5 [00:12<00:03,  3.15s/it]2024-01-07 19:34:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:34:20 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 2.405 | nll_loss 1.644 | ppl 3.12 | wps 2806.9 | wpb 1927.5 | bsz 150 | num_updates 235 | best_loss 2.405\n",
            "2024-01-07 19:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 235 updates\n",
            "2024-01-07 19:34:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 47 @ 235 updates, score 2.405) (writing took 0.3322144249998473 seconds)\n",
            "2024-01-07 19:34:21 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-01-07 19:34:21 | INFO | train | epoch 047 | loss 2.627 | nll_loss 2.012 | ppl 4.03 | wps 940.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 235 | lr 0.000235 | gnorm 0.966 | clip 20 | train_wall 17 | wall 859\n",
            "2024-01-07 19:34:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 048:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:34:21 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-01-07 19:34:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  80% 4/5 [00:11<00:03,  3.40s/it]2024-01-07 19:34:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:34:39 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 2.334 | nll_loss 1.553 | ppl 2.93 | wps 2229.4 | wpb 1927.5 | bsz 150 | num_updates 240 | best_loss 2.334\n",
            "2024-01-07 19:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 240 updates\n",
            "2024-01-07 19:34:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 48 @ 240 updates, score 2.334) (writing took 0.4733489449999979 seconds)\n",
            "2024-01-07 19:34:39 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-01-07 19:34:39 | INFO | train | epoch 048 | loss 2.56 | nll_loss 1.947 | ppl 3.86 | wps 945.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 240 | lr 0.00024 | gnorm 1.002 | clip 20 | train_wall 16 | wall 877\n",
            "2024-01-07 19:34:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 049:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:34:39 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-01-07 19:34:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  80% 4/5 [00:12<00:02,  2.64s/it]2024-01-07 19:34:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:34:57 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 2.236 | nll_loss 1.429 | ppl 2.69 | wps 2809 | wpb 1927.5 | bsz 150 | num_updates 245 | best_loss 2.236\n",
            "2024-01-07 19:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 245 updates\n",
            "2024-01-07 19:34:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:34:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 49 @ 245 updates, score 2.236) (writing took 0.33752959599996757 seconds)\n",
            "2024-01-07 19:34:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-01-07 19:34:57 | INFO | train | epoch 049 | loss 2.528 | nll_loss 1.893 | ppl 3.71 | wps 953.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 245 | lr 0.000245 | gnorm 1.384 | clip 80 | train_wall 16 | wall 896\n",
            "2024-01-07 19:34:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 050:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:34:57 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-01-07 19:34:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  80% 4/5 [00:12<00:02,  2.43s/it]2024-01-07 19:35:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.77s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:35:15 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 2.194 | nll_loss 1.372 | ppl 2.59 | wps 2842.1 | wpb 1927.5 | bsz 150 | num_updates 250 | best_loss 2.194\n",
            "2024-01-07 19:35:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 250 updates\n",
            "2024-01-07 19:35:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 50 @ 250 updates, score 2.194) (writing took 0.36125729500008674 seconds)\n",
            "2024-01-07 19:35:16 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-01-07 19:35:16 | INFO | train | epoch 050 | loss 2.457 | nll_loss 1.811 | ppl 3.51 | wps 939.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 250 | lr 0.00025 | gnorm 1.196 | clip 20 | train_wall 16 | wall 914\n",
            "2024-01-07 19:35:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 051:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:35:16 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-01-07 19:35:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  80% 4/5 [00:15<00:03,  3.85s/it]2024-01-07 19:35:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:35:33 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 2.16 | nll_loss 1.309 | ppl 2.48 | wps 2801.8 | wpb 1927.5 | bsz 150 | num_updates 255 | best_loss 2.16\n",
            "2024-01-07 19:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 255 updates\n",
            "2024-01-07 19:35:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 51 @ 255 updates, score 2.16) (writing took 0.3449117259999639 seconds)\n",
            "2024-01-07 19:35:34 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-01-07 19:35:34 | INFO | train | epoch 051 | loss 2.396 | nll_loss 1.736 | ppl 3.33 | wps 969.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 255 | lr 0.000255 | gnorm 1.102 | clip 40 | train_wall 16 | wall 932\n",
            "2024-01-07 19:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 052:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:35:34 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-01-07 19:35:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  80% 4/5 [00:12<00:02,  2.42s/it]2024-01-07 19:35:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:35:52 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 2.114 | nll_loss 1.267 | ppl 2.41 | wps 2821.9 | wpb 1927.5 | bsz 150 | num_updates 260 | best_loss 2.114\n",
            "2024-01-07 19:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 260 updates\n",
            "2024-01-07 19:35:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 52 @ 260 updates, score 2.114) (writing took 0.32693902699998034 seconds)\n",
            "2024-01-07 19:35:52 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-01-07 19:35:52 | INFO | train | epoch 052 | loss 2.345 | nll_loss 1.676 | ppl 3.19 | wps 944.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 260 | lr 0.00026 | gnorm 1.187 | clip 40 | train_wall 16 | wall 950\n",
            "2024-01-07 19:35:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 053:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:35:52 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-01-07 19:35:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  80% 4/5 [00:11<00:03,  3.31s/it]2024-01-07 19:36:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:36:10 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 2.107 | nll_loss 1.218 | ppl 2.33 | wps 2809.5 | wpb 1927.5 | bsz 150 | num_updates 265 | best_loss 2.107\n",
            "2024-01-07 19:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 265 updates\n",
            "2024-01-07 19:36:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:36:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 53 @ 265 updates, score 2.107) (writing took 0.361072484000033 seconds)\n",
            "2024-01-07 19:36:10 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-01-07 19:36:10 | INFO | train | epoch 053 | loss 2.314 | nll_loss 1.634 | ppl 3.1 | wps 956.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 265 | lr 0.000265 | gnorm 1.158 | clip 60 | train_wall 16 | wall 969\n",
            "2024-01-07 19:36:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 054:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:36:10 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-01-07 19:36:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  80% 4/5 [00:11<00:03,  3.21s/it]2024-01-07 19:36:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:36:31 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 2.139 | nll_loss 1.244 | ppl 2.37 | wps 2685.6 | wpb 1927.5 | bsz 150 | num_updates 270 | best_loss 2.107\n",
            "2024-01-07 19:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 270 updates\n",
            "2024-01-07 19:36:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 54 @ 270 updates, score 2.139) (writing took 0.36477305500011425 seconds)\n",
            "2024-01-07 19:36:31 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-01-07 19:36:31 | INFO | train | epoch 054 | loss 2.283 | nll_loss 1.606 | ppl 3.04 | wps 837.8 | ups 0.24 | wpb 3472.2 | bsz 279.8 | num_updates 270 | lr 0.00027 | gnorm 1.462 | clip 60 | train_wall 19 | wall 989\n",
            "2024-01-07 19:36:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 055:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:36:31 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-01-07 19:36:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  80% 4/5 [00:16<00:03,  3.83s/it]2024-01-07 19:36:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:36:49 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 1.94 | nll_loss 1.014 | ppl 2.02 | wps 2352.7 | wpb 1927.5 | bsz 150 | num_updates 275 | best_loss 1.94\n",
            "2024-01-07 19:36:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 275 updates\n",
            "2024-01-07 19:36:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:36:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:36:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 55 @ 275 updates, score 1.94) (writing took 0.42676454000002195 seconds)\n",
            "2024-01-07 19:36:49 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-01-07 19:36:49 | INFO | train | epoch 055 | loss 2.221 | nll_loss 1.524 | ppl 2.87 | wps 952.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 275 | lr 0.000275 | gnorm 1.259 | clip 60 | train_wall 16 | wall 1008\n",
            "2024-01-07 19:36:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 056:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:36:49 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-01-07 19:36:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  80% 4/5 [00:12<00:03,  3.23s/it]2024-01-07 19:37:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:37:07 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 1.895 | nll_loss 0.957 | ppl 1.94 | wps 2520.6 | wpb 1927.5 | bsz 150 | num_updates 280 | best_loss 1.895\n",
            "2024-01-07 19:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 280 updates\n",
            "2024-01-07 19:37:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 56 @ 280 updates, score 1.895) (writing took 0.4249360239998623 seconds)\n",
            "2024-01-07 19:37:08 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-01-07 19:37:08 | INFO | train | epoch 056 | loss 2.158 | nll_loss 1.45 | ppl 2.73 | wps 928.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 280 | lr 0.00028 | gnorm 1.222 | clip 20 | train_wall 17 | wall 1026\n",
            "2024-01-07 19:37:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 057:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:37:08 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-01-07 19:37:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  80% 4/5 [00:13<00:03,  3.43s/it]2024-01-07 19:37:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.72s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:37:27 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 1.866 | nll_loss 0.91 | ppl 1.88 | wps 2782.8 | wpb 1927.5 | bsz 150 | num_updates 285 | best_loss 1.866\n",
            "2024-01-07 19:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 285 updates\n",
            "2024-01-07 19:37:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 57 @ 285 updates, score 1.866) (writing took 0.40663360600001397 seconds)\n",
            "2024-01-07 19:37:27 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-01-07 19:37:27 | INFO | train | epoch 057 | loss 2.114 | nll_loss 1.399 | ppl 2.64 | wps 903 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 285 | lr 0.000285 | gnorm 1.146 | clip 60 | train_wall 17 | wall 1045\n",
            "2024-01-07 19:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 058:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:37:27 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-01-07 19:37:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  80% 4/5 [00:12<00:03,  3.02s/it]2024-01-07 19:37:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:37:45 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 1.81 | nll_loss 0.841 | ppl 1.79 | wps 2700.8 | wpb 1927.5 | bsz 150 | num_updates 290 | best_loss 1.81\n",
            "2024-01-07 19:37:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 290 updates\n",
            "2024-01-07 19:37:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 58 @ 290 updates, score 1.81) (writing took 0.3557913089998692 seconds)\n",
            "2024-01-07 19:37:45 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-01-07 19:37:45 | INFO | train | epoch 058 | loss 2.066 | nll_loss 1.341 | ppl 2.53 | wps 957.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 290 | lr 0.00029 | gnorm 1.216 | clip 20 | train_wall 16 | wall 1064\n",
            "2024-01-07 19:37:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 059:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:37:45 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-01-07 19:37:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  80% 4/5 [00:12<00:03,  3.30s/it]2024-01-07 19:38:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:38:03 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 1.765 | nll_loss 0.788 | ppl 1.73 | wps 2871.3 | wpb 1927.5 | bsz 150 | num_updates 295 | best_loss 1.765\n",
            "2024-01-07 19:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 295 updates\n",
            "2024-01-07 19:38:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 59 @ 295 updates, score 1.765) (writing took 0.3526229300000523 seconds)\n",
            "2024-01-07 19:38:04 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-01-07 19:38:04 | INFO | train | epoch 059 | loss 2.021 | nll_loss 1.282 | ppl 2.43 | wps 941.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 295 | lr 0.000295 | gnorm 1.093 | clip 20 | train_wall 17 | wall 1082\n",
            "2024-01-07 19:38:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 060:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:38:04 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-01-07 19:38:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  80% 4/5 [00:13<00:03,  3.31s/it]2024-01-07 19:38:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:38:22 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 1.778 | nll_loss 0.772 | ppl 1.71 | wps 2614.9 | wpb 1927.5 | bsz 150 | num_updates 300 | best_loss 1.765\n",
            "2024-01-07 19:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 300 updates\n",
            "2024-01-07 19:38:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 60 @ 300 updates, score 1.778) (writing took 0.17390059500007737 seconds)\n",
            "2024-01-07 19:38:22 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-01-07 19:38:22 | INFO | train | epoch 060 | loss 2.002 | nll_loss 1.264 | ppl 2.4 | wps 961.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 300 | lr 0.0003 | gnorm 1.1 | clip 40 | train_wall 16 | wall 1100\n",
            "2024-01-07 19:38:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 061:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:38:22 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-01-07 19:38:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  80% 4/5 [00:11<00:03,  3.13s/it]2024-01-07 19:38:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:38:40 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 1.758 | nll_loss 0.748 | ppl 1.68 | wps 2604.6 | wpb 1927.5 | bsz 150 | num_updates 305 | best_loss 1.758\n",
            "2024-01-07 19:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 305 updates\n",
            "2024-01-07 19:38:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 61 @ 305 updates, score 1.758) (writing took 0.4220078639998519 seconds)\n",
            "2024-01-07 19:38:40 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-01-07 19:38:40 | INFO | train | epoch 061 | loss 1.949 | nll_loss 1.196 | ppl 2.29 | wps 932.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 305 | lr 0.000305 | gnorm 1.054 | clip 40 | train_wall 17 | wall 1119\n",
            "2024-01-07 19:38:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 062:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:38:40 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-01-07 19:38:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  80% 4/5 [00:15<00:03,  3.98s/it]2024-01-07 19:38:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:38:58 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 1.747 | nll_loss 0.722 | ppl 1.65 | wps 2719.1 | wpb 1927.5 | bsz 150 | num_updates 310 | best_loss 1.747\n",
            "2024-01-07 19:38:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 310 updates\n",
            "2024-01-07 19:38:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:38:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 62 @ 310 updates, score 1.747) (writing took 0.4456487709999237 seconds)\n",
            "2024-01-07 19:38:58 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-01-07 19:38:58 | INFO | train | epoch 062 | loss 1.896 | nll_loss 1.136 | ppl 2.2 | wps 958.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 310 | lr 0.00031 | gnorm 0.992 | clip 20 | train_wall 16 | wall 1137\n",
            "2024-01-07 19:38:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 063:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:38:58 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-01-07 19:38:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  80% 4/5 [00:16<00:04,  4.11s/it]2024-01-07 19:39:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:39:16 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 1.642 | nll_loss 0.614 | ppl 1.53 | wps 2892.5 | wpb 1927.5 | bsz 150 | num_updates 315 | best_loss 1.642\n",
            "2024-01-07 19:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 315 updates\n",
            "2024-01-07 19:39:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 63 @ 315 updates, score 1.642) (writing took 0.45553600600010213 seconds)\n",
            "2024-01-07 19:39:17 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-01-07 19:39:17 | INFO | train | epoch 063 | loss 1.867 | nll_loss 1.099 | ppl 2.14 | wps 942.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 315 | lr 0.000315 | gnorm 1.067 | clip 40 | train_wall 16 | wall 1155\n",
            "2024-01-07 19:39:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 064:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:39:17 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-01-07 19:39:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  80% 4/5 [00:12<00:02,  2.95s/it]2024-01-07 19:39:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.71s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:39:35 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 1.681 | nll_loss 0.631 | ppl 1.55 | wps 2051.1 | wpb 1927.5 | bsz 150 | num_updates 320 | best_loss 1.642\n",
            "2024-01-07 19:39:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 320 updates\n",
            "2024-01-07 19:39:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:39:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 64 @ 320 updates, score 1.681) (writing took 0.29218550800010235 seconds)\n",
            "2024-01-07 19:39:35 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-01-07 19:39:35 | INFO | train | epoch 064 | loss 1.847 | nll_loss 1.083 | ppl 2.12 | wps 954.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 320 | lr 0.00032 | gnorm 1.182 | clip 20 | train_wall 16 | wall 1173\n",
            "2024-01-07 19:39:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 065:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:39:35 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-01-07 19:39:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  80% 4/5 [00:16<00:04,  4.06s/it]2024-01-07 19:39:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:39:53 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 1.707 | nll_loss 0.65 | ppl 1.57 | wps 2794.2 | wpb 1927.5 | bsz 150 | num_updates 325 | best_loss 1.642\n",
            "2024-01-07 19:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 325 updates\n",
            "2024-01-07 19:39:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 65 @ 325 updates, score 1.707) (writing took 0.170889046999946 seconds)\n",
            "2024-01-07 19:39:53 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-01-07 19:39:53 | INFO | train | epoch 065 | loss 1.802 | nll_loss 1.02 | ppl 2.03 | wps 970.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 325 | lr 0.000325 | gnorm 1.308 | clip 40 | train_wall 16 | wall 1191\n",
            "2024-01-07 19:39:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 066:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:39:53 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-01-07 19:39:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  80% 4/5 [00:12<00:03,  3.49s/it]2024-01-07 19:40:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  50% 1/2 [00:02<00:02,  2.21s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:40:11 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 1.595 | nll_loss 0.527 | ppl 1.44 | wps 2863.3 | wpb 1927.5 | bsz 150 | num_updates 330 | best_loss 1.595\n",
            "2024-01-07 19:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 330 updates\n",
            "2024-01-07 19:40:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 66 @ 330 updates, score 1.595) (writing took 0.3784915619999083 seconds)\n",
            "2024-01-07 19:40:12 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-01-07 19:40:12 | INFO | train | epoch 066 | loss 1.797 | nll_loss 1.008 | ppl 2.01 | wps 918.1 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 330 | lr 0.00033 | gnorm 1.222 | clip 40 | train_wall 16 | wall 1210\n",
            "2024-01-07 19:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 067:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:40:12 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-01-07 19:40:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  80% 4/5 [00:11<00:03,  3.03s/it]2024-01-07 19:40:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:40:29 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 1.573 | nll_loss 0.518 | ppl 1.43 | wps 2786.7 | wpb 1927.5 | bsz 150 | num_updates 335 | best_loss 1.573\n",
            "2024-01-07 19:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 335 updates\n",
            "2024-01-07 19:40:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:40:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 67 @ 335 updates, score 1.573) (writing took 0.37517834200002653 seconds)\n",
            "2024-01-07 19:40:30 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-01-07 19:40:30 | INFO | train | epoch 067 | loss 1.784 | nll_loss 0.991 | ppl 1.99 | wps 963.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 335 | lr 0.000335 | gnorm 1.35 | clip 40 | train_wall 16 | wall 1228\n",
            "2024-01-07 19:40:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 068:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:40:30 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-01-07 19:40:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  80% 4/5 [00:12<00:03,  3.43s/it]2024-01-07 19:40:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:40:48 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 1.597 | nll_loss 0.536 | ppl 1.45 | wps 2573.5 | wpb 1927.5 | bsz 150 | num_updates 340 | best_loss 1.573\n",
            "2024-01-07 19:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 340 updates\n",
            "2024-01-07 19:40:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:40:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 68 @ 340 updates, score 1.597) (writing took 0.2092340740000509 seconds)\n",
            "2024-01-07 19:40:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-01-07 19:40:48 | INFO | train | epoch 068 | loss 1.757 | nll_loss 0.972 | ppl 1.96 | wps 954.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 340 | lr 0.00034 | gnorm 1.468 | clip 60 | train_wall 16 | wall 1246\n",
            "2024-01-07 19:40:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 069:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:40:48 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-01-07 19:40:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  80% 4/5 [00:11<00:03,  3.34s/it]2024-01-07 19:41:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:41:06 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 1.569 | nll_loss 0.499 | ppl 1.41 | wps 2777.9 | wpb 1927.5 | bsz 150 | num_updates 345 | best_loss 1.569\n",
            "2024-01-07 19:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 345 updates\n",
            "2024-01-07 19:41:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 69 @ 345 updates, score 1.569) (writing took 0.30712322699992 seconds)\n",
            "2024-01-07 19:41:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-01-07 19:41:06 | INFO | train | epoch 069 | loss 1.721 | nll_loss 0.926 | ppl 1.9 | wps 966.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 345 | lr 0.000345 | gnorm 1.089 | clip 40 | train_wall 16 | wall 1264\n",
            "2024-01-07 19:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 070:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:41:06 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-01-07 19:41:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  80% 4/5 [00:12<00:03,  3.58s/it]2024-01-07 19:41:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:41:24 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 1.545 | nll_loss 0.467 | ppl 1.38 | wps 2625.6 | wpb 1927.5 | bsz 150 | num_updates 350 | best_loss 1.545\n",
            "2024-01-07 19:41:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 350 updates\n",
            "2024-01-07 19:41:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:41:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:41:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 70 @ 350 updates, score 1.545) (writing took 0.33953131899988875 seconds)\n",
            "2024-01-07 19:41:24 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-01-07 19:41:24 | INFO | train | epoch 070 | loss 1.7 | nll_loss 0.903 | ppl 1.87 | wps 950.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 350 | lr 0.00035 | gnorm 1.102 | clip 40 | train_wall 16 | wall 1283\n",
            "2024-01-07 19:41:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 071:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:41:24 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-01-07 19:41:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  80% 4/5 [00:12<00:03,  3.23s/it]2024-01-07 19:41:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:41:42 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 1.597 | nll_loss 0.512 | ppl 1.43 | wps 2736.7 | wpb 1927.5 | bsz 150 | num_updates 355 | best_loss 1.545\n",
            "2024-01-07 19:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 355 updates\n",
            "2024-01-07 19:41:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:41:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 71 @ 355 updates, score 1.597) (writing took 0.15396396599999207 seconds)\n",
            "2024-01-07 19:41:42 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-01-07 19:41:42 | INFO | train | epoch 071 | loss 1.671 | nll_loss 0.864 | ppl 1.82 | wps 970.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 355 | lr 0.000355 | gnorm 0.99 | clip 20 | train_wall 16 | wall 1301\n",
            "2024-01-07 19:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 072:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:41:42 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-01-07 19:41:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  80% 4/5 [00:11<00:02,  2.29s/it]2024-01-07 19:41:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:42:00 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 1.534 | nll_loss 0.447 | ppl 1.36 | wps 2904.3 | wpb 1927.5 | bsz 150 | num_updates 360 | best_loss 1.534\n",
            "2024-01-07 19:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 360 updates\n",
            "2024-01-07 19:42:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 72 @ 360 updates, score 1.534) (writing took 0.34449450900001466 seconds)\n",
            "2024-01-07 19:42:01 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-01-07 19:42:01 | INFO | train | epoch 072 | loss 1.659 | nll_loss 0.845 | ppl 1.8 | wps 945 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 360 | lr 0.00036 | gnorm 1.025 | clip 40 | train_wall 16 | wall 1319\n",
            "2024-01-07 19:42:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 073:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:42:01 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-01-07 19:42:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  80% 4/5 [00:12<00:03,  3.04s/it]2024-01-07 19:42:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:42:18 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 1.537 | nll_loss 0.46 | ppl 1.38 | wps 2184.3 | wpb 1927.5 | bsz 150 | num_updates 365 | best_loss 1.534\n",
            "2024-01-07 19:42:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 365 updates\n",
            "2024-01-07 19:42:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 73 @ 365 updates, score 1.537) (writing took 0.26649547600004553 seconds)\n",
            "2024-01-07 19:42:18 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-01-07 19:42:18 | INFO | train | epoch 073 | loss 1.67 | nll_loss 0.865 | ppl 1.82 | wps 967.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 365 | lr 0.000365 | gnorm 1.303 | clip 40 | train_wall 16 | wall 1337\n",
            "2024-01-07 19:42:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 074:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:42:18 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-01-07 19:42:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  80% 4/5 [00:12<00:02,  2.57s/it]2024-01-07 19:42:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:42:36 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 1.533 | nll_loss 0.455 | ppl 1.37 | wps 2579.6 | wpb 1927.5 | bsz 150 | num_updates 370 | best_loss 1.533\n",
            "2024-01-07 19:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 370 updates\n",
            "2024-01-07 19:42:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 74 @ 370 updates, score 1.533) (writing took 0.41850840200004313 seconds)\n",
            "2024-01-07 19:42:37 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-01-07 19:42:37 | INFO | train | epoch 074 | loss 1.623 | nll_loss 0.816 | ppl 1.76 | wps 944.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 370 | lr 0.00037 | gnorm 1.075 | clip 20 | train_wall 16 | wall 1355\n",
            "2024-01-07 19:42:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 075:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:42:37 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-01-07 19:42:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  80% 4/5 [00:12<00:03,  3.33s/it]2024-01-07 19:42:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.83s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:42:55 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 1.498 | nll_loss 0.399 | ppl 1.32 | wps 2332.9 | wpb 1927.5 | bsz 150 | num_updates 375 | best_loss 1.498\n",
            "2024-01-07 19:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 375 updates\n",
            "2024-01-07 19:42:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:42:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 75 @ 375 updates, score 1.498) (writing took 0.3777635559999908 seconds)\n",
            "2024-01-07 19:42:55 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-01-07 19:42:55 | INFO | train | epoch 075 | loss 1.621 | nll_loss 0.809 | ppl 1.75 | wps 935.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 375 | lr 0.000375 | gnorm 1.012 | clip 40 | train_wall 16 | wall 1374\n",
            "2024-01-07 19:42:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 076:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:42:55 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-01-07 19:42:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  80% 4/5 [00:15<00:03,  3.94s/it]2024-01-07 19:43:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:43:13 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 1.534 | nll_loss 0.449 | ppl 1.37 | wps 2856.9 | wpb 1927.5 | bsz 150 | num_updates 380 | best_loss 1.498\n",
            "2024-01-07 19:43:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 380 updates\n",
            "2024-01-07 19:43:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:43:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 76 @ 380 updates, score 1.534) (writing took 0.1699293049998687 seconds)\n",
            "2024-01-07 19:43:13 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-01-07 19:43:13 | INFO | train | epoch 076 | loss 1.582 | nll_loss 0.757 | ppl 1.69 | wps 972.6 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 380 | lr 0.00038 | gnorm 1.125 | clip 40 | train_wall 16 | wall 1392\n",
            "2024-01-07 19:43:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 077:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:43:13 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-01-07 19:43:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  80% 4/5 [00:12<00:03,  3.41s/it]2024-01-07 19:43:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:43:31 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 1.475 | nll_loss 0.395 | ppl 1.32 | wps 2546.6 | wpb 1927.5 | bsz 150 | num_updates 385 | best_loss 1.475\n",
            "2024-01-07 19:43:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 385 updates\n",
            "2024-01-07 19:43:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:43:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 77 @ 385 updates, score 1.475) (writing took 0.3777998170000956 seconds)\n",
            "2024-01-07 19:43:32 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-01-07 19:43:32 | INFO | train | epoch 077 | loss 1.576 | nll_loss 0.759 | ppl 1.69 | wps 940.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 385 | lr 0.000385 | gnorm 0.92 | clip 20 | train_wall 16 | wall 1410\n",
            "2024-01-07 19:43:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 078:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:43:32 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-01-07 19:43:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  80% 4/5 [00:11<00:03,  3.05s/it]2024-01-07 19:43:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:43:49 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 1.482 | nll_loss 0.391 | ppl 1.31 | wps 2844.7 | wpb 1927.5 | bsz 150 | num_updates 390 | best_loss 1.475\n",
            "2024-01-07 19:43:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 390 updates\n",
            "2024-01-07 19:43:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 78 @ 390 updates, score 1.482) (writing took 0.22361686499993993 seconds)\n",
            "2024-01-07 19:43:50 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-01-07 19:43:50 | INFO | train | epoch 078 | loss 1.569 | nll_loss 0.743 | ppl 1.67 | wps 967.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 390 | lr 0.00039 | gnorm 1.158 | clip 20 | train_wall 16 | wall 1428\n",
            "2024-01-07 19:43:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 079:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:43:50 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-01-07 19:43:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  80% 4/5 [00:16<00:04,  4.13s/it]2024-01-07 19:44:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:44:08 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 1.544 | nll_loss 0.468 | ppl 1.38 | wps 2617.3 | wpb 1927.5 | bsz 150 | num_updates 395 | best_loss 1.475\n",
            "2024-01-07 19:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 395 updates\n",
            "2024-01-07 19:44:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:44:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 79 @ 395 updates, score 1.544) (writing took 0.1884713090000787 seconds)\n",
            "2024-01-07 19:44:08 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-01-07 19:44:08 | INFO | train | epoch 079 | loss 1.566 | nll_loss 0.742 | ppl 1.67 | wps 955.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 395 | lr 0.000395 | gnorm 1.365 | clip 40 | train_wall 16 | wall 1446\n",
            "2024-01-07 19:44:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 080:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:44:08 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-01-07 19:44:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  80% 4/5 [00:12<00:03,  3.64s/it]2024-01-07 19:44:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:44:25 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 1.484 | nll_loss 0.399 | ppl 1.32 | wps 2764.1 | wpb 1927.5 | bsz 150 | num_updates 400 | best_loss 1.475\n",
            "2024-01-07 19:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 400 updates\n",
            "2024-01-07 19:44:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:44:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 80 @ 400 updates, score 1.484) (writing took 0.16443916200000785 seconds)\n",
            "2024-01-07 19:44:26 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-01-07 19:44:26 | INFO | train | epoch 080 | loss 1.601 | nll_loss 0.796 | ppl 1.74 | wps 975.1 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 400 | lr 0.0004 | gnorm 1.24 | clip 60 | train_wall 16 | wall 1464\n",
            "2024-01-07 19:44:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 081:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:44:26 | INFO | fairseq.trainer | begin training epoch 81\n",
            "2024-01-07 19:44:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 081:  80% 4/5 [00:13<00:03,  3.62s/it]2024-01-07 19:44:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 081 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:44:44 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 1.451 | nll_loss 0.366 | ppl 1.29 | wps 2637.2 | wpb 1927.5 | bsz 150 | num_updates 405 | best_loss 1.451\n",
            "2024-01-07 19:44:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 405 updates\n",
            "2024-01-07 19:44:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:44:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:44:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 81 @ 405 updates, score 1.451) (writing took 0.36033350800016706 seconds)\n",
            "2024-01-07 19:44:45 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
            "2024-01-07 19:44:45 | INFO | train | epoch 081 | loss 1.61 | nll_loss 0.775 | ppl 1.71 | wps 913.5 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 405 | lr 0.000405 | gnorm 1.454 | clip 100 | train_wall 17 | wall 1483\n",
            "2024-01-07 19:44:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 082:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:44:45 | INFO | fairseq.trainer | begin training epoch 82\n",
            "2024-01-07 19:44:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 082:  80% 4/5 [00:11<00:03,  3.22s/it]2024-01-07 19:45:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 082 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.50s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:45:02 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 1.482 | nll_loss 0.4 | ppl 1.32 | wps 1981.8 | wpb 1927.5 | bsz 150 | num_updates 410 | best_loss 1.451\n",
            "2024-01-07 19:45:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 410 updates\n",
            "2024-01-07 19:45:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 82 @ 410 updates, score 1.482) (writing took 0.37785938699994404 seconds)\n",
            "2024-01-07 19:45:03 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
            "2024-01-07 19:45:03 | INFO | train | epoch 082 | loss 1.63 | nll_loss 0.824 | ppl 1.77 | wps 962.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 410 | lr 0.00041 | gnorm 1.45 | clip 80 | train_wall 16 | wall 1501\n",
            "2024-01-07 19:45:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 083:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:45:03 | INFO | fairseq.trainer | begin training epoch 83\n",
            "2024-01-07 19:45:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 083:  80% 4/5 [00:11<00:03,  3.33s/it]2024-01-07 19:45:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 083 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:45:21 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 1.438 | nll_loss 0.367 | ppl 1.29 | wps 2745.6 | wpb 1927.5 | bsz 150 | num_updates 415 | best_loss 1.438\n",
            "2024-01-07 19:45:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 415 updates\n",
            "2024-01-07 19:45:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:45:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:45:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 83 @ 415 updates, score 1.438) (writing took 0.35136841000007735 seconds)\n",
            "2024-01-07 19:45:21 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
            "2024-01-07 19:45:21 | INFO | train | epoch 083 | loss 1.553 | nll_loss 0.735 | ppl 1.66 | wps 947.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 415 | lr 0.000415 | gnorm 1.153 | clip 40 | train_wall 16 | wall 1519\n",
            "2024-01-07 19:45:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 084:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:45:21 | INFO | fairseq.trainer | begin training epoch 84\n",
            "2024-01-07 19:45:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 084:  80% 4/5 [00:11<00:03,  3.36s/it]2024-01-07 19:45:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 084 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:45:39 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 1.474 | nll_loss 0.397 | ppl 1.32 | wps 2081.6 | wpb 1927.5 | bsz 150 | num_updates 420 | best_loss 1.438\n",
            "2024-01-07 19:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 420 updates\n",
            "2024-01-07 19:45:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 84 @ 420 updates, score 1.474) (writing took 0.2836670619999495 seconds)\n",
            "2024-01-07 19:45:39 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
            "2024-01-07 19:45:39 | INFO | train | epoch 084 | loss 1.532 | nll_loss 0.71 | ppl 1.64 | wps 949.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 420 | lr 0.00042 | gnorm 0.937 | clip 20 | train_wall 16 | wall 1538\n",
            "2024-01-07 19:45:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 085:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:45:39 | INFO | fairseq.trainer | begin training epoch 85\n",
            "2024-01-07 19:45:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 085:  80% 4/5 [00:15<00:03,  3.89s/it]2024-01-07 19:45:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 085 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:45:57 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 1.448 | nll_loss 0.367 | ppl 1.29 | wps 2754.4 | wpb 1927.5 | bsz 150 | num_updates 425 | best_loss 1.438\n",
            "2024-01-07 19:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 425 updates\n",
            "2024-01-07 19:45:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:45:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 85 @ 425 updates, score 1.448) (writing took 0.17432833299994854 seconds)\n",
            "2024-01-07 19:45:57 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
            "2024-01-07 19:45:57 | INFO | train | epoch 085 | loss 1.524 | nll_loss 0.693 | ppl 1.62 | wps 976.1 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 425 | lr 0.000425 | gnorm 0.859 | clip 20 | train_wall 16 | wall 1555\n",
            "2024-01-07 19:45:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 086:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:45:57 | INFO | fairseq.trainer | begin training epoch 86\n",
            "2024-01-07 19:45:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 086:  80% 4/5 [00:13<00:02,  2.69s/it]2024-01-07 19:46:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 086 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:46:15 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 1.483 | nll_loss 0.391 | ppl 1.31 | wps 2656.9 | wpb 1927.5 | bsz 150 | num_updates 430 | best_loss 1.438\n",
            "2024-01-07 19:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 430 updates\n",
            "2024-01-07 19:46:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 86 @ 430 updates, score 1.483) (writing took 0.17218111300007877 seconds)\n",
            "2024-01-07 19:46:15 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
            "2024-01-07 19:46:15 | INFO | train | epoch 086 | loss 1.503 | nll_loss 0.675 | ppl 1.6 | wps 945.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 430 | lr 0.00043 | gnorm 1.043 | clip 20 | train_wall 16 | wall 1574\n",
            "2024-01-07 19:46:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 087:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:46:15 | INFO | fairseq.trainer | begin training epoch 87\n",
            "2024-01-07 19:46:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 087:  80% 4/5 [00:12<00:02,  2.56s/it]2024-01-07 19:46:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 087 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.96s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:46:34 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 1.415 | nll_loss 0.343 | ppl 1.27 | wps 1452.9 | wpb 1927.5 | bsz 150 | num_updates 435 | best_loss 1.415\n",
            "2024-01-07 19:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 435 updates\n",
            "2024-01-07 19:46:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:46:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:46:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 87 @ 435 updates, score 1.415) (writing took 0.869429165999918 seconds)\n",
            "2024-01-07 19:46:35 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
            "2024-01-07 19:46:35 | INFO | train | epoch 087 | loss 1.487 | nll_loss 0.644 | ppl 1.56 | wps 903.8 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 435 | lr 0.000435 | gnorm 0.921 | clip 20 | train_wall 16 | wall 1593\n",
            "2024-01-07 19:46:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 088:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:46:35 | INFO | fairseq.trainer | begin training epoch 88\n",
            "2024-01-07 19:46:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 088:  80% 4/5 [00:15<00:04,  4.37s/it]2024-01-07 19:46:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 088 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:46:56 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 1.442 | nll_loss 0.352 | ppl 1.28 | wps 2798 | wpb 1927.5 | bsz 150 | num_updates 440 | best_loss 1.415\n",
            "2024-01-07 19:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 440 updates\n",
            "2024-01-07 19:46:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 88 @ 440 updates, score 1.442) (writing took 0.20064821100004337 seconds)\n",
            "2024-01-07 19:46:56 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
            "2024-01-07 19:46:56 | INFO | train | epoch 088 | loss 1.483 | nll_loss 0.647 | ppl 1.57 | wps 818.9 | ups 0.24 | wpb 3472.2 | bsz 279.8 | num_updates 440 | lr 0.00044 | gnorm 0.894 | clip 20 | train_wall 19 | wall 1614\n",
            "2024-01-07 19:46:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 089:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:46:56 | INFO | fairseq.trainer | begin training epoch 89\n",
            "2024-01-07 19:46:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 089:  80% 4/5 [00:13<00:03,  3.55s/it]2024-01-07 19:47:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 089 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:47:14 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 1.398 | nll_loss 0.32 | ppl 1.25 | wps 2042.8 | wpb 1927.5 | bsz 150 | num_updates 445 | best_loss 1.398\n",
            "2024-01-07 19:47:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 445 updates\n",
            "2024-01-07 19:47:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:47:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:47:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 89 @ 445 updates, score 1.398) (writing took 0.5117963899999722 seconds)\n",
            "2024-01-07 19:47:14 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
            "2024-01-07 19:47:14 | INFO | train | epoch 089 | loss 1.482 | nll_loss 0.654 | ppl 1.57 | wps 939.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 445 | lr 0.000445 | gnorm 0.985 | clip 20 | train_wall 16 | wall 1633\n",
            "2024-01-07 19:47:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 090:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:47:14 | INFO | fairseq.trainer | begin training epoch 90\n",
            "2024-01-07 19:47:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 090:  80% 4/5 [00:11<00:02,  2.34s/it]2024-01-07 19:47:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 090 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:47:32 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 1.392 | nll_loss 0.316 | ppl 1.25 | wps 2801.1 | wpb 1927.5 | bsz 150 | num_updates 450 | best_loss 1.392\n",
            "2024-01-07 19:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 450 updates\n",
            "2024-01-07 19:47:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:47:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:47:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 90 @ 450 updates, score 1.392) (writing took 0.381669243000033 seconds)\n",
            "2024-01-07 19:47:33 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
            "2024-01-07 19:47:33 | INFO | train | epoch 090 | loss 1.442 | nll_loss 0.615 | ppl 1.53 | wps 937.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 450 | lr 0.00045 | gnorm 0.784 | clip 20 | train_wall 17 | wall 1651\n",
            "2024-01-07 19:47:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 091:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:47:33 | INFO | fairseq.trainer | begin training epoch 91\n",
            "2024-01-07 19:47:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 091:  80% 4/5 [00:11<00:02,  2.34s/it]2024-01-07 19:47:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 091 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.74s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:47:51 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 1.413 | nll_loss 0.322 | ppl 1.25 | wps 2781.5 | wpb 1927.5 | bsz 150 | num_updates 455 | best_loss 1.392\n",
            "2024-01-07 19:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 455 updates\n",
            "2024-01-07 19:47:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:47:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 91 @ 455 updates, score 1.413) (writing took 0.2708887240000877 seconds)\n",
            "2024-01-07 19:47:51 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
            "2024-01-07 19:47:51 | INFO | train | epoch 091 | loss 1.441 | nll_loss 0.601 | ppl 1.52 | wps 939.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 455 | lr 0.000455 | gnorm 0.916 | clip 20 | train_wall 16 | wall 1670\n",
            "2024-01-07 19:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 092:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:47:51 | INFO | fairseq.trainer | begin training epoch 92\n",
            "2024-01-07 19:47:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 092:  80% 4/5 [00:13<00:03,  3.10s/it]2024-01-07 19:48:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 092 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:48:09 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 1.416 | nll_loss 0.322 | ppl 1.25 | wps 2780 | wpb 1927.5 | bsz 150 | num_updates 460 | best_loss 1.392\n",
            "2024-01-07 19:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 460 updates\n",
            "2024-01-07 19:48:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 92 @ 460 updates, score 1.416) (writing took 0.17999417199985146 seconds)\n",
            "2024-01-07 19:48:09 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
            "2024-01-07 19:48:09 | INFO | train | epoch 092 | loss 1.434 | nll_loss 0.587 | ppl 1.5 | wps 966.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 460 | lr 0.00046 | gnorm 0.817 | clip 20 | train_wall 16 | wall 1688\n",
            "2024-01-07 19:48:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 093:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:48:09 | INFO | fairseq.trainer | begin training epoch 93\n",
            "2024-01-07 19:48:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 093:  80% 4/5 [00:12<00:03,  3.71s/it]2024-01-07 19:48:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 093 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:48:27 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 1.389 | nll_loss 0.305 | ppl 1.24 | wps 2558.8 | wpb 1927.5 | bsz 150 | num_updates 465 | best_loss 1.389\n",
            "2024-01-07 19:48:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 465 updates\n",
            "2024-01-07 19:48:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 93 @ 465 updates, score 1.389) (writing took 0.435863837999932 seconds)\n",
            "2024-01-07 19:48:28 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
            "2024-01-07 19:48:28 | INFO | train | epoch 093 | loss 1.43 | nll_loss 0.582 | ppl 1.5 | wps 938.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 465 | lr 0.000465 | gnorm 0.853 | clip 20 | train_wall 17 | wall 1706\n",
            "2024-01-07 19:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 094:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:48:28 | INFO | fairseq.trainer | begin training epoch 94\n",
            "2024-01-07 19:48:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 094:  80% 4/5 [00:12<00:03,  3.47s/it]2024-01-07 19:48:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 094 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:48:46 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 1.364 | nll_loss 0.289 | ppl 1.22 | wps 2652.4 | wpb 1927.5 | bsz 150 | num_updates 470 | best_loss 1.364\n",
            "2024-01-07 19:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 470 updates\n",
            "2024-01-07 19:48:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:48:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 94 @ 470 updates, score 1.364) (writing took 0.339968676999888 seconds)\n",
            "2024-01-07 19:48:46 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
            "2024-01-07 19:48:46 | INFO | train | epoch 094 | loss 1.43 | nll_loss 0.596 | ppl 1.51 | wps 948.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 470 | lr 0.00047 | gnorm 0.995 | clip 20 | train_wall 16 | wall 1724\n",
            "2024-01-07 19:48:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 095:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:48:46 | INFO | fairseq.trainer | begin training epoch 95\n",
            "2024-01-07 19:48:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 095:  80% 4/5 [00:16<00:04,  4.14s/it]2024-01-07 19:49:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 095 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:49:04 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 1.367 | nll_loss 0.296 | ppl 1.23 | wps 2772.5 | wpb 1927.5 | bsz 150 | num_updates 475 | best_loss 1.364\n",
            "2024-01-07 19:49:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 475 updates\n",
            "2024-01-07 19:49:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 95 @ 475 updates, score 1.367) (writing took 0.15590801399980592 seconds)\n",
            "2024-01-07 19:49:04 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
            "2024-01-07 19:49:04 | INFO | train | epoch 095 | loss 1.402 | nll_loss 0.562 | ppl 1.48 | wps 953.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 475 | lr 0.000475 | gnorm 0.859 | clip 20 | train_wall 16 | wall 1743\n",
            "2024-01-07 19:49:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 096:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:49:04 | INFO | fairseq.trainer | begin training epoch 96\n",
            "2024-01-07 19:49:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 096:  80% 4/5 [00:11<00:03,  3.07s/it]2024-01-07 19:49:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 096 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:49:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 1.403 | nll_loss 0.329 | ppl 1.26 | wps 2613.9 | wpb 1927.5 | bsz 150 | num_updates 480 | best_loss 1.364\n",
            "2024-01-07 19:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 480 updates\n",
            "2024-01-07 19:49:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 96 @ 480 updates, score 1.403) (writing took 0.1579433249999056 seconds)\n",
            "2024-01-07 19:49:22 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
            "2024-01-07 19:49:22 | INFO | train | epoch 096 | loss 1.417 | nll_loss 0.579 | ppl 1.49 | wps 971.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 480 | lr 0.00048 | gnorm 0.931 | clip 40 | train_wall 16 | wall 1761\n",
            "2024-01-07 19:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 097:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:49:22 | INFO | fairseq.trainer | begin training epoch 97\n",
            "2024-01-07 19:49:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 097:  80% 4/5 [00:11<00:02,  2.40s/it]2024-01-07 19:49:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 097 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:49:40 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 1.384 | nll_loss 0.322 | ppl 1.25 | wps 2691.5 | wpb 1927.5 | bsz 150 | num_updates 485 | best_loss 1.364\n",
            "2024-01-07 19:49:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 485 updates\n",
            "2024-01-07 19:49:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 97 @ 485 updates, score 1.384) (writing took 0.23964269700013574 seconds)\n",
            "2024-01-07 19:49:40 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
            "2024-01-07 19:49:40 | INFO | train | epoch 097 | loss 1.426 | nll_loss 0.583 | ppl 1.5 | wps 948.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 485 | lr 0.000485 | gnorm 1.035 | clip 20 | train_wall 17 | wall 1779\n",
            "2024-01-07 19:49:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 098:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:49:40 | INFO | fairseq.trainer | begin training epoch 98\n",
            "2024-01-07 19:49:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 098:  80% 4/5 [00:12<00:02,  2.76s/it]2024-01-07 19:49:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 098 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.63s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:49:58 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 1.365 | nll_loss 0.302 | ppl 1.23 | wps 2290.7 | wpb 1927.5 | bsz 150 | num_updates 490 | best_loss 1.364\n",
            "2024-01-07 19:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 490 updates\n",
            "2024-01-07 19:49:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 98 @ 490 updates, score 1.365) (writing took 0.2677565760000107 seconds)\n",
            "2024-01-07 19:49:59 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
            "2024-01-07 19:49:59 | INFO | train | epoch 098 | loss 1.392 | nll_loss 0.555 | ppl 1.47 | wps 952.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 490 | lr 0.00049 | gnorm 0.828 | clip 20 | train_wall 16 | wall 1797\n",
            "2024-01-07 19:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 099:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:49:59 | INFO | fairseq.trainer | begin training epoch 99\n",
            "2024-01-07 19:49:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 099:  80% 4/5 [00:12<00:02,  2.52s/it]2024-01-07 19:50:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 099 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:50:17 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 1.486 | nll_loss 0.441 | ppl 1.36 | wps 2771.5 | wpb 1927.5 | bsz 150 | num_updates 495 | best_loss 1.364\n",
            "2024-01-07 19:50:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 495 updates\n",
            "2024-01-07 19:50:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 99 @ 495 updates, score 1.486) (writing took 0.15686346600000434 seconds)\n",
            "2024-01-07 19:50:17 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
            "2024-01-07 19:50:17 | INFO | train | epoch 099 | loss 1.398 | nll_loss 0.559 | ppl 1.47 | wps 965.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 495 | lr 0.000495 | gnorm 0.839 | clip 20 | train_wall 16 | wall 1815\n",
            "2024-01-07 19:50:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 100:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:50:17 | INFO | fairseq.trainer | begin training epoch 100\n",
            "2024-01-07 19:50:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 100:  80% 4/5 [00:12<00:03,  3.50s/it]2024-01-07 19:50:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 100 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.84s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:50:35 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 1.367 | nll_loss 0.333 | ppl 1.26 | wps 2624.1 | wpb 1927.5 | bsz 150 | num_updates 500 | best_loss 1.364\n",
            "2024-01-07 19:50:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 500 updates\n",
            "2024-01-07 19:50:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 100 @ 500 updates, score 1.367) (writing took 0.15182601600008638 seconds)\n",
            "2024-01-07 19:50:35 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
            "2024-01-07 19:50:35 | INFO | train | epoch 100 | loss 1.419 | nll_loss 0.586 | ppl 1.5 | wps 950.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 500 | lr 0.0005 | gnorm 0.996 | clip 20 | train_wall 16 | wall 1833\n",
            "2024-01-07 19:50:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 101:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:50:35 | INFO | fairseq.trainer | begin training epoch 101\n",
            "2024-01-07 19:50:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 101:  80% 4/5 [00:12<00:03,  3.25s/it]2024-01-07 19:50:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 101 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:50:53 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 1.395 | nll_loss 0.31 | ppl 1.24 | wps 2802.8 | wpb 1927.5 | bsz 150 | num_updates 505 | best_loss 1.364\n",
            "2024-01-07 19:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 505 updates\n",
            "2024-01-07 19:50:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:50:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 101 @ 505 updates, score 1.395) (writing took 0.15222992600001817 seconds)\n",
            "2024-01-07 19:50:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
            "2024-01-07 19:50:53 | INFO | train | epoch 101 | loss 1.406 | nll_loss 0.575 | ppl 1.49 | wps 974.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 505 | lr 0.000505 | gnorm 0.868 | clip 20 | train_wall 16 | wall 1851\n",
            "2024-01-07 19:50:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 102:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:50:53 | INFO | fairseq.trainer | begin training epoch 102\n",
            "2024-01-07 19:50:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 102:  80% 4/5 [00:12<00:02,  2.74s/it]2024-01-07 19:51:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 102 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.62s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:51:11 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 1.35 | nll_loss 0.291 | ppl 1.22 | wps 2746.1 | wpb 1927.5 | bsz 150 | num_updates 510 | best_loss 1.35\n",
            "2024-01-07 19:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 510 updates\n",
            "2024-01-07 19:51:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 102 @ 510 updates, score 1.35) (writing took 0.31986342100026377 seconds)\n",
            "2024-01-07 19:51:11 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
            "2024-01-07 19:51:11 | INFO | train | epoch 102 | loss 1.386 | nll_loss 0.524 | ppl 1.44 | wps 937.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 510 | lr 0.00051 | gnorm 0.966 | clip 20 | train_wall 16 | wall 1870\n",
            "2024-01-07 19:51:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 103:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:51:11 | INFO | fairseq.trainer | begin training epoch 103\n",
            "2024-01-07 19:51:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 103:  80% 4/5 [00:12<00:02,  2.61s/it]2024-01-07 19:51:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 103 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:51:29 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 1.356 | nll_loss 0.298 | ppl 1.23 | wps 2740.7 | wpb 1927.5 | bsz 150 | num_updates 515 | best_loss 1.35\n",
            "2024-01-07 19:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 515 updates\n",
            "2024-01-07 19:51:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:51:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:51:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 103 @ 515 updates, score 1.356) (writing took 0.16361028500023167 seconds)\n",
            "2024-01-07 19:51:29 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
            "2024-01-07 19:51:29 | INFO | train | epoch 103 | loss 1.375 | nll_loss 0.532 | ppl 1.45 | wps 967.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 515 | lr 0.000515 | gnorm 0.912 | clip 20 | train_wall 16 | wall 1888\n",
            "2024-01-07 19:51:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 104:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:51:29 | INFO | fairseq.trainer | begin training epoch 104\n",
            "2024-01-07 19:51:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 104:  80% 4/5 [00:12<00:03,  3.55s/it]2024-01-07 19:51:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 104 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:51:47 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 1.319 | nll_loss 0.274 | ppl 1.21 | wps 2663.7 | wpb 1927.5 | bsz 150 | num_updates 520 | best_loss 1.319\n",
            "2024-01-07 19:51:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 520 updates\n",
            "2024-01-07 19:51:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:51:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:51:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 104 @ 520 updates, score 1.319) (writing took 0.40876630299999306 seconds)\n",
            "2024-01-07 19:51:48 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
            "2024-01-07 19:51:48 | INFO | train | epoch 104 | loss 1.361 | nll_loss 0.523 | ppl 1.44 | wps 943.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 520 | lr 0.00052 | gnorm 0.633 | clip 0 | train_wall 16 | wall 1906\n",
            "2024-01-07 19:51:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 105:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:51:48 | INFO | fairseq.trainer | begin training epoch 105\n",
            "2024-01-07 19:51:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 105:  80% 4/5 [00:11<00:03,  3.08s/it]2024-01-07 19:52:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 105 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:52:05 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 1.334 | nll_loss 0.272 | ppl 1.21 | wps 2728.5 | wpb 1927.5 | bsz 150 | num_updates 525 | best_loss 1.319\n",
            "2024-01-07 19:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 525 updates\n",
            "2024-01-07 19:52:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 105 @ 525 updates, score 1.334) (writing took 0.20800016200018945 seconds)\n",
            "2024-01-07 19:52:06 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
            "2024-01-07 19:52:06 | INFO | train | epoch 105 | loss 1.352 | nll_loss 0.514 | ppl 1.43 | wps 970.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 525 | lr 0.000525 | gnorm 0.816 | clip 20 | train_wall 16 | wall 1924\n",
            "2024-01-07 19:52:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 106:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:52:06 | INFO | fairseq.trainer | begin training epoch 106\n",
            "2024-01-07 19:52:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 106:  80% 4/5 [00:12<00:03,  3.37s/it]2024-01-07 19:52:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 106 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:52:24 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 1.348 | nll_loss 0.286 | ppl 1.22 | wps 2739 | wpb 1927.5 | bsz 150 | num_updates 530 | best_loss 1.319\n",
            "2024-01-07 19:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 530 updates\n",
            "2024-01-07 19:52:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:52:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:52:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 106 @ 530 updates, score 1.348) (writing took 0.20498387199995705 seconds)\n",
            "2024-01-07 19:52:24 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
            "2024-01-07 19:52:24 | INFO | train | epoch 106 | loss 1.337 | nll_loss 0.481 | ppl 1.4 | wps 953.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 530 | lr 0.00053 | gnorm 0.682 | clip 20 | train_wall 16 | wall 1942\n",
            "2024-01-07 19:52:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 107:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:52:24 | INFO | fairseq.trainer | begin training epoch 107\n",
            "2024-01-07 19:52:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 107:  80% 4/5 [00:12<00:02,  2.65s/it]2024-01-07 19:52:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 107 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:52:41 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 1.307 | nll_loss 0.246 | ppl 1.19 | wps 2793.6 | wpb 1927.5 | bsz 150 | num_updates 535 | best_loss 1.307\n",
            "2024-01-07 19:52:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 535 updates\n",
            "2024-01-07 19:52:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 107 @ 535 updates, score 1.307) (writing took 0.36522580800010473 seconds)\n",
            "2024-01-07 19:52:42 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
            "2024-01-07 19:52:42 | INFO | train | epoch 107 | loss 1.352 | nll_loss 0.507 | ppl 1.42 | wps 966.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 535 | lr 0.000535 | gnorm 0.685 | clip 20 | train_wall 16 | wall 1960\n",
            "2024-01-07 19:52:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 108:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:52:42 | INFO | fairseq.trainer | begin training epoch 108\n",
            "2024-01-07 19:52:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 108:  80% 4/5 [00:12<00:03,  3.13s/it]2024-01-07 19:52:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 108 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:53:00 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 1.293 | nll_loss 0.253 | ppl 1.19 | wps 2543 | wpb 1927.5 | bsz 150 | num_updates 540 | best_loss 1.293\n",
            "2024-01-07 19:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 540 updates\n",
            "2024-01-07 19:53:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 108 @ 540 updates, score 1.293) (writing took 0.3681021970000984 seconds)\n",
            "2024-01-07 19:53:00 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
            "2024-01-07 19:53:00 | INFO | train | epoch 108 | loss 1.319 | nll_loss 0.473 | ppl 1.39 | wps 943.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 540 | lr 0.00054 | gnorm 0.672 | clip 20 | train_wall 16 | wall 1978\n",
            "2024-01-07 19:53:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 109:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:53:00 | INFO | fairseq.trainer | begin training epoch 109\n",
            "2024-01-07 19:53:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 109:  80% 4/5 [00:12<00:03,  3.43s/it]2024-01-07 19:53:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 109 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.72s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:53:18 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 1.325 | nll_loss 0.261 | ppl 1.2 | wps 2242.2 | wpb 1927.5 | bsz 150 | num_updates 545 | best_loss 1.293\n",
            "2024-01-07 19:53:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 545 updates\n",
            "2024-01-07 19:53:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 109 @ 545 updates, score 1.325) (writing took 0.2748699660000966 seconds)\n",
            "2024-01-07 19:53:18 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
            "2024-01-07 19:53:18 | INFO | train | epoch 109 | loss 1.319 | nll_loss 0.488 | ppl 1.4 | wps 946.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 545 | lr 0.000545 | gnorm 0.6 | clip 0 | train_wall 16 | wall 1997\n",
            "2024-01-07 19:53:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 110:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:53:18 | INFO | fairseq.trainer | begin training epoch 110\n",
            "2024-01-07 19:53:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 110:  80% 4/5 [00:16<00:03,  3.94s/it]2024-01-07 19:53:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 110 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:53:36 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 1.31 | nll_loss 0.252 | ppl 1.19 | wps 2734.4 | wpb 1927.5 | bsz 150 | num_updates 550 | best_loss 1.293\n",
            "2024-01-07 19:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 550 updates\n",
            "2024-01-07 19:53:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 110 @ 550 updates, score 1.31) (writing took 0.17221370399965963 seconds)\n",
            "2024-01-07 19:53:37 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
            "2024-01-07 19:53:37 | INFO | train | epoch 110 | loss 1.319 | nll_loss 0.464 | ppl 1.38 | wps 959.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 550 | lr 0.00055 | gnorm 0.726 | clip 20 | train_wall 16 | wall 2015\n",
            "2024-01-07 19:53:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 111:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:53:37 | INFO | fairseq.trainer | begin training epoch 111\n",
            "2024-01-07 19:53:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 111:  80% 4/5 [00:16<00:03,  3.83s/it]2024-01-07 19:53:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 111 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.74s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:53:55 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 1.333 | nll_loss 0.288 | ppl 1.22 | wps 2722.1 | wpb 1927.5 | bsz 150 | num_updates 555 | best_loss 1.293\n",
            "2024-01-07 19:53:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 555 updates\n",
            "2024-01-07 19:53:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:53:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 111 @ 555 updates, score 1.333) (writing took 0.17542862499976764 seconds)\n",
            "2024-01-07 19:53:55 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
            "2024-01-07 19:53:55 | INFO | train | epoch 111 | loss 1.328 | nll_loss 0.484 | ppl 1.4 | wps 949 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 555 | lr 0.000555 | gnorm 0.715 | clip 20 | train_wall 16 | wall 2033\n",
            "2024-01-07 19:53:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 112:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:53:55 | INFO | fairseq.trainer | begin training epoch 112\n",
            "2024-01-07 19:53:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 112:  80% 4/5 [00:15<00:04,  4.03s/it]2024-01-07 19:54:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 112 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:54:12 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 1.287 | nll_loss 0.267 | ppl 1.2 | wps 2785.1 | wpb 1927.5 | bsz 150 | num_updates 560 | best_loss 1.287\n",
            "2024-01-07 19:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 560 updates\n",
            "2024-01-07 19:54:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:54:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:54:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 112 @ 560 updates, score 1.287) (writing took 0.41993485700004385 seconds)\n",
            "2024-01-07 19:54:13 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
            "2024-01-07 19:54:13 | INFO | train | epoch 112 | loss 1.343 | nll_loss 0.498 | ppl 1.41 | wps 964.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 560 | lr 0.00056 | gnorm 0.903 | clip 20 | train_wall 16 | wall 2051\n",
            "2024-01-07 19:54:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 113:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:54:13 | INFO | fairseq.trainer | begin training epoch 113\n",
            "2024-01-07 19:54:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 113:  80% 4/5 [00:11<00:02,  2.67s/it]2024-01-07 19:54:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 113 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:54:31 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 1.336 | nll_loss 0.274 | ppl 1.21 | wps 2557 | wpb 1927.5 | bsz 150 | num_updates 565 | best_loss 1.287\n",
            "2024-01-07 19:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 565 updates\n",
            "2024-01-07 19:54:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 113 @ 565 updates, score 1.336) (writing took 0.2252125619997969 seconds)\n",
            "2024-01-07 19:54:31 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
            "2024-01-07 19:54:31 | INFO | train | epoch 113 | loss 1.383 | nll_loss 0.559 | ppl 1.47 | wps 946 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 565 | lr 0.000565 | gnorm 1.015 | clip 40 | train_wall 17 | wall 2070\n",
            "2024-01-07 19:54:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 114:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:54:31 | INFO | fairseq.trainer | begin training epoch 114\n",
            "2024-01-07 19:54:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 114:  80% 4/5 [00:11<00:02,  2.86s/it]2024-01-07 19:54:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 114 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:54:49 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 1.345 | nll_loss 0.279 | ppl 1.21 | wps 2683.8 | wpb 1927.5 | bsz 150 | num_updates 570 | best_loss 1.287\n",
            "2024-01-07 19:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 570 updates\n",
            "2024-01-07 19:54:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 114 @ 570 updates, score 1.345) (writing took 0.3147091159999036 seconds)\n",
            "2024-01-07 19:54:49 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
            "2024-01-07 19:54:49 | INFO | train | epoch 114 | loss 1.337 | nll_loss 0.477 | ppl 1.39 | wps 970 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 570 | lr 0.00057 | gnorm 0.928 | clip 40 | train_wall 16 | wall 2087\n",
            "2024-01-07 19:54:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 115:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:54:49 | INFO | fairseq.trainer | begin training epoch 115\n",
            "2024-01-07 19:54:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 115:  80% 4/5 [00:13<00:03,  3.65s/it]2024-01-07 19:55:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 115 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:55:07 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 1.279 | nll_loss 0.241 | ppl 1.18 | wps 2785 | wpb 1927.5 | bsz 150 | num_updates 575 | best_loss 1.279\n",
            "2024-01-07 19:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 575 updates\n",
            "2024-01-07 19:55:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:55:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 115 @ 575 updates, score 1.279) (writing took 0.4037295380003343 seconds)\n",
            "2024-01-07 19:55:08 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
            "2024-01-07 19:55:08 | INFO | train | epoch 115 | loss 1.326 | nll_loss 0.47 | ppl 1.39 | wps 933.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 575 | lr 0.000575 | gnorm 0.744 | clip 20 | train_wall 17 | wall 2106\n",
            "2024-01-07 19:55:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 116:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:55:08 | INFO | fairseq.trainer | begin training epoch 116\n",
            "2024-01-07 19:55:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 116:  80% 4/5 [00:12<00:02,  2.60s/it]2024-01-07 19:55:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 116 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:55:25 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 1.305 | nll_loss 0.284 | ppl 1.22 | wps 2731.8 | wpb 1927.5 | bsz 150 | num_updates 580 | best_loss 1.279\n",
            "2024-01-07 19:55:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 580 updates\n",
            "2024-01-07 19:55:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:55:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:55:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 116 @ 580 updates, score 1.305) (writing took 0.17458028599958197 seconds)\n",
            "2024-01-07 19:55:26 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
            "2024-01-07 19:55:26 | INFO | train | epoch 116 | loss 1.296 | nll_loss 0.454 | ppl 1.37 | wps 967.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 580 | lr 0.00058 | gnorm 0.684 | clip 20 | train_wall 16 | wall 2124\n",
            "2024-01-07 19:55:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 117:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:55:26 | INFO | fairseq.trainer | begin training epoch 117\n",
            "2024-01-07 19:55:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 117:  80% 4/5 [00:12<00:03,  3.41s/it]2024-01-07 19:55:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 117 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:55:44 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 1.297 | nll_loss 0.272 | ppl 1.21 | wps 2503.7 | wpb 1927.5 | bsz 150 | num_updates 585 | best_loss 1.279\n",
            "2024-01-07 19:55:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 585 updates\n",
            "2024-01-07 19:55:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 117 @ 585 updates, score 1.297) (writing took 0.17248847699966063 seconds)\n",
            "2024-01-07 19:55:44 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
            "2024-01-07 19:55:44 | INFO | train | epoch 117 | loss 1.327 | nll_loss 0.495 | ppl 1.41 | wps 948.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 585 | lr 0.000585 | gnorm 0.765 | clip 20 | train_wall 17 | wall 2142\n",
            "2024-01-07 19:55:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 118:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:55:44 | INFO | fairseq.trainer | begin training epoch 118\n",
            "2024-01-07 19:55:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 118:  80% 4/5 [00:11<00:02,  2.32s/it]2024-01-07 19:56:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 118 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.51s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:56:02 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 1.308 | nll_loss 0.277 | ppl 1.21 | wps 2131.4 | wpb 1927.5 | bsz 150 | num_updates 590 | best_loss 1.279\n",
            "2024-01-07 19:56:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 590 updates\n",
            "2024-01-07 19:56:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 118 @ 590 updates, score 1.308) (writing took 0.3020594970003003 seconds)\n",
            "2024-01-07 19:56:02 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
            "2024-01-07 19:56:02 | INFO | train | epoch 118 | loss 1.31 | nll_loss 0.469 | ppl 1.38 | wps 963.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 590 | lr 0.00059 | gnorm 0.925 | clip 40 | train_wall 16 | wall 2160\n",
            "2024-01-07 19:56:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 119:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:56:02 | INFO | fairseq.trainer | begin training epoch 119\n",
            "2024-01-07 19:56:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 119:  80% 4/5 [00:12<00:03,  3.44s/it]2024-01-07 19:56:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 119 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:56:20 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 1.305 | nll_loss 0.247 | ppl 1.19 | wps 2775.8 | wpb 1927.5 | bsz 150 | num_updates 595 | best_loss 1.279\n",
            "2024-01-07 19:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 595 updates\n",
            "2024-01-07 19:56:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 119 @ 595 updates, score 1.305) (writing took 0.16643294699997568 seconds)\n",
            "2024-01-07 19:56:20 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
            "2024-01-07 19:56:20 | INFO | train | epoch 119 | loss 1.319 | nll_loss 0.477 | ppl 1.39 | wps 949.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 595 | lr 0.000595 | gnorm 0.747 | clip 20 | train_wall 16 | wall 2179\n",
            "2024-01-07 19:56:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 120:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:56:20 | INFO | fairseq.trainer | begin training epoch 120\n",
            "2024-01-07 19:56:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 120:  80% 4/5 [00:11<00:03,  3.40s/it]2024-01-07 19:56:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 120 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:56:38 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 1.288 | nll_loss 0.244 | ppl 1.18 | wps 2222.8 | wpb 1927.5 | bsz 150 | num_updates 600 | best_loss 1.279\n",
            "2024-01-07 19:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 600 updates\n",
            "2024-01-07 19:56:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:56:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 120 @ 600 updates, score 1.288) (writing took 0.2589636700004121 seconds)\n",
            "2024-01-07 19:56:38 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
            "2024-01-07 19:56:38 | INFO | train | epoch 120 | loss 1.288 | nll_loss 0.428 | ppl 1.34 | wps 948.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 600 | lr 0.0006 | gnorm 0.661 | clip 20 | train_wall 16 | wall 2197\n",
            "2024-01-07 19:56:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 121:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:56:39 | INFO | fairseq.trainer | begin training epoch 121\n",
            "2024-01-07 19:56:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 121:  80% 4/5 [00:14<00:02,  2.85s/it]2024-01-07 19:56:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 121 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:56:59 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 1.278 | nll_loss 0.242 | ppl 1.18 | wps 2836.9 | wpb 1927.5 | bsz 150 | num_updates 605 | best_loss 1.278\n",
            "2024-01-07 19:56:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 605 updates\n",
            "2024-01-07 19:56:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:56:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 121 @ 605 updates, score 1.278) (writing took 0.5067596710000544 seconds)\n",
            "2024-01-07 19:56:59 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
            "2024-01-07 19:56:59 | INFO | train | epoch 121 | loss 1.272 | nll_loss 0.418 | ppl 1.34 | wps 831.9 | ups 0.24 | wpb 3472.2 | bsz 279.8 | num_updates 605 | lr 0.000605 | gnorm 0.609 | clip 20 | train_wall 19 | wall 2218\n",
            "2024-01-07 19:56:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 122:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:56:59 | INFO | fairseq.trainer | begin training epoch 122\n",
            "2024-01-07 19:56:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 122:  80% 4/5 [00:11<00:02,  2.76s/it]2024-01-07 19:57:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 122 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:57:17 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 1.267 | nll_loss 0.237 | ppl 1.18 | wps 2842.8 | wpb 1927.5 | bsz 150 | num_updates 610 | best_loss 1.267\n",
            "2024-01-07 19:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 610 updates\n",
            "2024-01-07 19:57:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 122 @ 610 updates, score 1.267) (writing took 0.4424896750001608 seconds)\n",
            "2024-01-07 19:57:18 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
            "2024-01-07 19:57:18 | INFO | train | epoch 122 | loss 1.265 | nll_loss 0.416 | ppl 1.33 | wps 940.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 610 | lr 0.00061 | gnorm 0.556 | clip 0 | train_wall 16 | wall 2236\n",
            "2024-01-07 19:57:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 123:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:57:18 | INFO | fairseq.trainer | begin training epoch 123\n",
            "2024-01-07 19:57:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 123:  80% 4/5 [00:15<00:04,  4.05s/it]2024-01-07 19:57:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 123 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:57:35 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 1.28 | nll_loss 0.25 | ppl 1.19 | wps 2773.1 | wpb 1927.5 | bsz 150 | num_updates 615 | best_loss 1.267\n",
            "2024-01-07 19:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 615 updates\n",
            "2024-01-07 19:57:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:57:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 123 @ 615 updates, score 1.28) (writing took 0.22677263199966546 seconds)\n",
            "2024-01-07 19:57:36 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
            "2024-01-07 19:57:36 | INFO | train | epoch 123 | loss 1.251 | nll_loss 0.402 | ppl 1.32 | wps 970.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 615 | lr 0.000615 | gnorm 0.524 | clip 20 | train_wall 16 | wall 2254\n",
            "2024-01-07 19:57:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 124:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:57:36 | INFO | fairseq.trainer | begin training epoch 124\n",
            "2024-01-07 19:57:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 124:  80% 4/5 [00:11<00:02,  2.67s/it]2024-01-07 19:57:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 124 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:57:54 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 1.271 | nll_loss 0.237 | ppl 1.18 | wps 2845 | wpb 1927.5 | bsz 150 | num_updates 620 | best_loss 1.267\n",
            "2024-01-07 19:57:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 620 updates\n",
            "2024-01-07 19:57:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 124 @ 620 updates, score 1.271) (writing took 0.3127660449999894 seconds)\n",
            "2024-01-07 19:57:54 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
            "2024-01-07 19:57:54 | INFO | train | epoch 124 | loss 1.251 | nll_loss 0.4 | ppl 1.32 | wps 939.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 620 | lr 0.00062 | gnorm 0.57 | clip 20 | train_wall 17 | wall 2273\n",
            "2024-01-07 19:57:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 125:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:57:54 | INFO | fairseq.trainer | begin training epoch 125\n",
            "2024-01-07 19:57:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 125:  80% 4/5 [00:11<00:03,  3.20s/it]2024-01-07 19:58:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 125 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.71s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:58:12 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 1.289 | nll_loss 0.275 | ppl 1.21 | wps 2275 | wpb 1927.5 | bsz 150 | num_updates 625 | best_loss 1.267\n",
            "2024-01-07 19:58:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 625 updates\n",
            "2024-01-07 19:58:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 125 @ 625 updates, score 1.289) (writing took 0.280122589000257 seconds)\n",
            "2024-01-07 19:58:12 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
            "2024-01-07 19:58:12 | INFO | train | epoch 125 | loss 1.278 | nll_loss 0.436 | ppl 1.35 | wps 952.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 625 | lr 0.000625 | gnorm 0.709 | clip 20 | train_wall 16 | wall 2291\n",
            "2024-01-07 19:58:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 126:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:58:12 | INFO | fairseq.trainer | begin training epoch 126\n",
            "2024-01-07 19:58:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 126:  80% 4/5 [00:12<00:03,  3.54s/it]2024-01-07 19:58:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 126 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:58:30 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 1.279 | nll_loss 0.261 | ppl 1.2 | wps 2697.7 | wpb 1927.5 | bsz 150 | num_updates 630 | best_loss 1.267\n",
            "2024-01-07 19:58:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 630 updates\n",
            "2024-01-07 19:58:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 126 @ 630 updates, score 1.279) (writing took 0.17832105700017564 seconds)\n",
            "2024-01-07 19:58:30 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
            "2024-01-07 19:58:30 | INFO | train | epoch 126 | loss 1.291 | nll_loss 0.456 | ppl 1.37 | wps 968.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 630 | lr 0.00063 | gnorm 0.63 | clip 0 | train_wall 16 | wall 2309\n",
            "2024-01-07 19:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 127:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:58:30 | INFO | fairseq.trainer | begin training epoch 127\n",
            "2024-01-07 19:58:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 127:  80% 4/5 [00:11<00:03,  3.26s/it]2024-01-07 19:58:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 127 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:58:49 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 1.321 | nll_loss 0.296 | ppl 1.23 | wps 2743.3 | wpb 1927.5 | bsz 150 | num_updates 635 | best_loss 1.267\n",
            "2024-01-07 19:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 635 updates\n",
            "2024-01-07 19:58:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 127 @ 635 updates, score 1.321) (writing took 0.30261951900001804 seconds)\n",
            "2024-01-07 19:58:49 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
            "2024-01-07 19:58:49 | INFO | train | epoch 127 | loss 1.275 | nll_loss 0.429 | ppl 1.35 | wps 937 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 635 | lr 0.000635 | gnorm 0.826 | clip 20 | train_wall 16 | wall 2327\n",
            "2024-01-07 19:58:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 128:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:58:49 | INFO | fairseq.trainer | begin training epoch 128\n",
            "2024-01-07 19:58:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 128:  80% 4/5 [00:12<00:03,  3.12s/it]2024-01-07 19:59:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 128 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:59:06 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 1.247 | nll_loss 0.221 | ppl 1.17 | wps 2820.3 | wpb 1927.5 | bsz 150 | num_updates 640 | best_loss 1.247\n",
            "2024-01-07 19:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 640 updates\n",
            "2024-01-07 19:59:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:59:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 19:59:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 128 @ 640 updates, score 1.247) (writing took 0.39428797399978066 seconds)\n",
            "2024-01-07 19:59:07 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
            "2024-01-07 19:59:07 | INFO | train | epoch 128 | loss 1.286 | nll_loss 0.438 | ppl 1.35 | wps 965.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 640 | lr 0.00064 | gnorm 0.69 | clip 0 | train_wall 16 | wall 2345\n",
            "2024-01-07 19:59:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 129:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:59:07 | INFO | fairseq.trainer | begin training epoch 129\n",
            "2024-01-07 19:59:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 129:  80% 4/5 [00:12<00:02,  2.40s/it]2024-01-07 19:59:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 129 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:59:25 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 1.278 | nll_loss 0.267 | ppl 1.2 | wps 2690.6 | wpb 1927.5 | bsz 150 | num_updates 645 | best_loss 1.247\n",
            "2024-01-07 19:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 645 updates\n",
            "2024-01-07 19:59:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 129 @ 645 updates, score 1.278) (writing took 0.30021542900021814 seconds)\n",
            "2024-01-07 19:59:25 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
            "2024-01-07 19:59:25 | INFO | train | epoch 129 | loss 1.265 | nll_loss 0.417 | ppl 1.34 | wps 949.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 645 | lr 0.000645 | gnorm 0.601 | clip 0 | train_wall 16 | wall 2364\n",
            "2024-01-07 19:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 130:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:59:25 | INFO | fairseq.trainer | begin training epoch 130\n",
            "2024-01-07 19:59:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 130:  80% 4/5 [00:12<00:03,  3.77s/it]2024-01-07 19:59:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 130 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 19:59:43 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 1.257 | nll_loss 0.238 | ppl 1.18 | wps 2740.8 | wpb 1927.5 | bsz 150 | num_updates 650 | best_loss 1.247\n",
            "2024-01-07 19:59:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 650 updates\n",
            "2024-01-07 19:59:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:59:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 19:59:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 130 @ 650 updates, score 1.257) (writing took 0.20641062599997895 seconds)\n",
            "2024-01-07 19:59:43 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
            "2024-01-07 19:59:43 | INFO | train | epoch 130 | loss 1.275 | nll_loss 0.439 | ppl 1.36 | wps 970.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 650 | lr 0.00065 | gnorm 0.637 | clip 0 | train_wall 16 | wall 2381\n",
            "2024-01-07 19:59:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 131:   0% 0/5 [00:00<?, ?it/s]2024-01-07 19:59:43 | INFO | fairseq.trainer | begin training epoch 131\n",
            "2024-01-07 19:59:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 131:  80% 4/5 [00:12<00:03,  3.06s/it]2024-01-07 20:00:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 131 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 131 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:00:01 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 1.263 | nll_loss 0.254 | ppl 1.19 | wps 2714.5 | wpb 1927.5 | bsz 150 | num_updates 655 | best_loss 1.247\n",
            "2024-01-07 20:00:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 655 updates\n",
            "2024-01-07 20:00:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 131 @ 655 updates, score 1.263) (writing took 0.19449456700021983 seconds)\n",
            "2024-01-07 20:00:01 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)\n",
            "2024-01-07 20:00:01 | INFO | train | epoch 131 | loss 1.307 | nll_loss 0.468 | ppl 1.38 | wps 946 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 655 | lr 0.000655 | gnorm 0.885 | clip 40 | train_wall 17 | wall 2400\n",
            "2024-01-07 20:00:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 132:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:00:01 | INFO | fairseq.trainer | begin training epoch 132\n",
            "2024-01-07 20:00:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 132:  80% 4/5 [00:12<00:03,  3.19s/it]2024-01-07 20:00:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 132 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 132 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:00:19 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 1.285 | nll_loss 0.269 | ppl 1.2 | wps 2580.5 | wpb 1927.5 | bsz 150 | num_updates 660 | best_loss 1.247\n",
            "2024-01-07 20:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 660 updates\n",
            "2024-01-07 20:00:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 132 @ 660 updates, score 1.285) (writing took 0.17823509800018655 seconds)\n",
            "2024-01-07 20:00:19 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)\n",
            "2024-01-07 20:00:19 | INFO | train | epoch 132 | loss 1.289 | nll_loss 0.448 | ppl 1.36 | wps 957.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 660 | lr 0.00066 | gnorm 0.771 | clip 20 | train_wall 16 | wall 2418\n",
            "2024-01-07 20:00:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 133:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:00:20 | INFO | fairseq.trainer | begin training epoch 133\n",
            "2024-01-07 20:00:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 133:  80% 4/5 [00:13<00:03,  3.90s/it]2024-01-07 20:00:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 133 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 133 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:00:38 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 1.248 | nll_loss 0.25 | ppl 1.19 | wps 2810.3 | wpb 1927.5 | bsz 150 | num_updates 665 | best_loss 1.247\n",
            "2024-01-07 20:00:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 665 updates\n",
            "2024-01-07 20:00:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 133 @ 665 updates, score 1.248) (writing took 0.1932728470001166 seconds)\n",
            "2024-01-07 20:00:38 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)\n",
            "2024-01-07 20:00:38 | INFO | train | epoch 133 | loss 1.277 | nll_loss 0.436 | ppl 1.35 | wps 933.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 665 | lr 0.000665 | gnorm 0.683 | clip 20 | train_wall 17 | wall 2436\n",
            "2024-01-07 20:00:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 134:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:00:38 | INFO | fairseq.trainer | begin training epoch 134\n",
            "2024-01-07 20:00:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 134:  80% 4/5 [00:12<00:02,  2.58s/it]2024-01-07 20:00:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 134 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 134 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:00:56 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 1.253 | nll_loss 0.253 | ppl 1.19 | wps 1358.8 | wpb 1927.5 | bsz 150 | num_updates 670 | best_loss 1.247\n",
            "2024-01-07 20:00:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 670 updates\n",
            "2024-01-07 20:00:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:00:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 134 @ 670 updates, score 1.253) (writing took 0.26902959200015175 seconds)\n",
            "2024-01-07 20:00:56 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)\n",
            "2024-01-07 20:00:56 | INFO | train | epoch 134 | loss 1.275 | nll_loss 0.442 | ppl 1.36 | wps 958.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 670 | lr 0.00067 | gnorm 0.697 | clip 20 | train_wall 16 | wall 2455\n",
            "2024-01-07 20:00:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 135:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:00:56 | INFO | fairseq.trainer | begin training epoch 135\n",
            "2024-01-07 20:00:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 135:  80% 4/5 [00:12<00:03,  3.16s/it]2024-01-07 20:01:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 135 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 135 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:01:14 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 1.254 | nll_loss 0.243 | ppl 1.18 | wps 2720.7 | wpb 1927.5 | bsz 150 | num_updates 675 | best_loss 1.247\n",
            "2024-01-07 20:01:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 675 updates\n",
            "2024-01-07 20:01:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 135 @ 675 updates, score 1.254) (writing took 0.19140761700009534 seconds)\n",
            "2024-01-07 20:01:14 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)\n",
            "2024-01-07 20:01:14 | INFO | train | epoch 135 | loss 1.283 | nll_loss 0.446 | ppl 1.36 | wps 954.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 675 | lr 0.000675 | gnorm 0.825 | clip 20 | train_wall 16 | wall 2473\n",
            "2024-01-07 20:01:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 136:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:01:14 | INFO | fairseq.trainer | begin training epoch 136\n",
            "2024-01-07 20:01:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 136:  80% 4/5 [00:12<00:03,  3.66s/it]2024-01-07 20:01:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 136 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 136 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.86s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:01:33 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 1.25 | nll_loss 0.235 | ppl 1.18 | wps 2819.8 | wpb 1927.5 | bsz 150 | num_updates 680 | best_loss 1.247\n",
            "2024-01-07 20:01:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 680 updates\n",
            "2024-01-07 20:01:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:01:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 136 @ 680 updates, score 1.25) (writing took 0.19028818700007832 seconds)\n",
            "2024-01-07 20:01:33 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)\n",
            "2024-01-07 20:01:33 | INFO | train | epoch 136 | loss 1.248 | nll_loss 0.392 | ppl 1.31 | wps 947.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 680 | lr 0.00068 | gnorm 0.609 | clip 20 | train_wall 16 | wall 2491\n",
            "2024-01-07 20:01:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 137:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:01:33 | INFO | fairseq.trainer | begin training epoch 137\n",
            "2024-01-07 20:01:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 137:  80% 4/5 [00:11<00:03,  3.19s/it]2024-01-07 20:01:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 137 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 137 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:01:50 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 1.241 | nll_loss 0.236 | ppl 1.18 | wps 2705.8 | wpb 1927.5 | bsz 150 | num_updates 685 | best_loss 1.241\n",
            "2024-01-07 20:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 685 updates\n",
            "2024-01-07 20:01:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:01:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 137 @ 685 updates, score 1.241) (writing took 0.3652397050000218 seconds)\n",
            "2024-01-07 20:01:51 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)\n",
            "2024-01-07 20:01:51 | INFO | train | epoch 137 | loss 1.235 | nll_loss 0.391 | ppl 1.31 | wps 962.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 685 | lr 0.000685 | gnorm 0.568 | clip 20 | train_wall 16 | wall 2509\n",
            "2024-01-07 20:01:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 138:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:01:51 | INFO | fairseq.trainer | begin training epoch 138\n",
            "2024-01-07 20:01:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 138:  80% 4/5 [00:16<00:03,  3.95s/it]2024-01-07 20:02:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 138 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 138 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:02:09 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 1.246 | nll_loss 0.24 | ppl 1.18 | wps 2690.2 | wpb 1927.5 | bsz 150 | num_updates 690 | best_loss 1.241\n",
            "2024-01-07 20:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 690 updates\n",
            "2024-01-07 20:02:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:02:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:02:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 138 @ 690 updates, score 1.246) (writing took 0.17672135800012256 seconds)\n",
            "2024-01-07 20:02:09 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)\n",
            "2024-01-07 20:02:09 | INFO | train | epoch 138 | loss 1.219 | nll_loss 0.37 | ppl 1.29 | wps 947.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 690 | lr 0.00069 | gnorm 0.563 | clip 20 | train_wall 16 | wall 2527\n",
            "2024-01-07 20:02:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 139:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:02:09 | INFO | fairseq.trainer | begin training epoch 139\n",
            "2024-01-07 20:02:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 139:  80% 4/5 [00:12<00:03,  3.63s/it]2024-01-07 20:02:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 139 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 139 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:02:27 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 1.228 | nll_loss 0.225 | ppl 1.17 | wps 2061.3 | wpb 1927.5 | bsz 150 | num_updates 695 | best_loss 1.228\n",
            "2024-01-07 20:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 695 updates\n",
            "2024-01-07 20:02:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:02:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 139 @ 695 updates, score 1.228) (writing took 0.4103708530001313 seconds)\n",
            "2024-01-07 20:02:27 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)\n",
            "2024-01-07 20:02:27 | INFO | train | epoch 139 | loss 1.243 | nll_loss 0.402 | ppl 1.32 | wps 958.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 695 | lr 0.000695 | gnorm 0.537 | clip 0 | train_wall 16 | wall 2546\n",
            "2024-01-07 20:02:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 140:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:02:27 | INFO | fairseq.trainer | begin training epoch 140\n",
            "2024-01-07 20:02:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 140:  80% 4/5 [00:12<00:02,  2.56s/it]2024-01-07 20:02:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 140 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 140 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:02:45 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 1.228 | nll_loss 0.229 | ppl 1.17 | wps 2800.4 | wpb 1927.5 | bsz 150 | num_updates 700 | best_loss 1.228\n",
            "2024-01-07 20:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 700 updates\n",
            "2024-01-07 20:02:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 140 @ 700 updates, score 1.228) (writing took 0.45295493999992686 seconds)\n",
            "2024-01-07 20:02:46 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)\n",
            "2024-01-07 20:02:46 | INFO | train | epoch 140 | loss 1.214 | nll_loss 0.371 | ppl 1.29 | wps 936.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 700 | lr 0.0007 | gnorm 0.54 | clip 20 | train_wall 16 | wall 2564\n",
            "2024-01-07 20:02:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 141:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:02:46 | INFO | fairseq.trainer | begin training epoch 141\n",
            "2024-01-07 20:02:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 141:  80% 4/5 [00:13<00:02,  2.64s/it]2024-01-07 20:03:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 141 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 141 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:03:03 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 1.252 | nll_loss 0.235 | ppl 1.18 | wps 2658 | wpb 1927.5 | bsz 150 | num_updates 705 | best_loss 1.228\n",
            "2024-01-07 20:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 705 updates\n",
            "2024-01-07 20:03:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:03:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 141 @ 705 updates, score 1.252) (writing took 0.1914754559998073 seconds)\n",
            "2024-01-07 20:03:04 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)\n",
            "2024-01-07 20:03:04 | INFO | train | epoch 141 | loss 1.219 | nll_loss 0.372 | ppl 1.29 | wps 967.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 705 | lr 0.000705 | gnorm 0.604 | clip 20 | train_wall 16 | wall 2582\n",
            "2024-01-07 20:03:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 142:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:03:04 | INFO | fairseq.trainer | begin training epoch 142\n",
            "2024-01-07 20:03:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 142:  80% 4/5 [00:12<00:02,  2.36s/it]2024-01-07 20:03:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 142 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 142 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:03:22 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 1.228 | nll_loss 0.22 | ppl 1.16 | wps 2923.3 | wpb 1927.5 | bsz 150 | num_updates 710 | best_loss 1.228\n",
            "2024-01-07 20:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 710 updates\n",
            "2024-01-07 20:03:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 142 @ 710 updates, score 1.228) (writing took 0.35956764399998065 seconds)\n",
            "2024-01-07 20:03:22 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)\n",
            "2024-01-07 20:03:22 | INFO | train | epoch 142 | loss 1.218 | nll_loss 0.357 | ppl 1.28 | wps 943.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 710 | lr 0.00071 | gnorm 0.575 | clip 20 | train_wall 16 | wall 2600\n",
            "2024-01-07 20:03:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 143:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:03:22 | INFO | fairseq.trainer | begin training epoch 143\n",
            "2024-01-07 20:03:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 143:  80% 4/5 [00:15<00:04,  4.05s/it]2024-01-07 20:03:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 143 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 143 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:03:40 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 1.221 | nll_loss 0.229 | ppl 1.17 | wps 2742.6 | wpb 1927.5 | bsz 150 | num_updates 715 | best_loss 1.221\n",
            "2024-01-07 20:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 715 updates\n",
            "2024-01-07 20:03:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 143 @ 715 updates, score 1.221) (writing took 0.40261450100024376 seconds)\n",
            "2024-01-07 20:03:40 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)\n",
            "2024-01-07 20:03:40 | INFO | train | epoch 143 | loss 1.205 | nll_loss 0.359 | ppl 1.28 | wps 964.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 715 | lr 0.000715 | gnorm 0.458 | clip 0 | train_wall 16 | wall 2618\n",
            "2024-01-07 20:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 144:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:03:40 | INFO | fairseq.trainer | begin training epoch 144\n",
            "2024-01-07 20:03:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 144:  80% 4/5 [00:11<00:02,  2.69s/it]2024-01-07 20:03:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 144 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 144 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:03:58 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 1.204 | nll_loss 0.211 | ppl 1.16 | wps 2699.8 | wpb 1927.5 | bsz 150 | num_updates 720 | best_loss 1.204\n",
            "2024-01-07 20:03:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 720 updates\n",
            "2024-01-07 20:03:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 144 @ 720 updates, score 1.204) (writing took 0.39170984199972736 seconds)\n",
            "2024-01-07 20:03:59 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)\n",
            "2024-01-07 20:03:59 | INFO | train | epoch 144 | loss 1.201 | nll_loss 0.358 | ppl 1.28 | wps 938.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 720 | lr 0.00072 | gnorm 0.423 | clip 0 | train_wall 16 | wall 2637\n",
            "2024-01-07 20:03:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 145:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:03:59 | INFO | fairseq.trainer | begin training epoch 145\n",
            "2024-01-07 20:03:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 145:  80% 4/5 [00:12<00:03,  3.77s/it]2024-01-07 20:04:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 145 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 145 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:04:17 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 1.21 | nll_loss 0.205 | ppl 1.15 | wps 2728.3 | wpb 1927.5 | bsz 150 | num_updates 725 | best_loss 1.204\n",
            "2024-01-07 20:04:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 725 updates\n",
            "2024-01-07 20:04:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 145 @ 725 updates, score 1.21) (writing took 0.18569907699975374 seconds)\n",
            "2024-01-07 20:04:18 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)\n",
            "2024-01-07 20:04:18 | INFO | train | epoch 145 | loss 1.188 | nll_loss 0.343 | ppl 1.27 | wps 913.7 | ups 0.26 | wpb 3472.2 | bsz 279.8 | num_updates 725 | lr 0.000725 | gnorm 0.417 | clip 0 | train_wall 17 | wall 2656\n",
            "2024-01-07 20:04:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 146:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:04:18 | INFO | fairseq.trainer | begin training epoch 146\n",
            "2024-01-07 20:04:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 146:  80% 4/5 [00:12<00:03,  3.19s/it]2024-01-07 20:04:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 146 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 146 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:04:35 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 1.206 | nll_loss 0.204 | ppl 1.15 | wps 2693.5 | wpb 1927.5 | bsz 150 | num_updates 730 | best_loss 1.204\n",
            "2024-01-07 20:04:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 730 updates\n",
            "2024-01-07 20:04:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 146 @ 730 updates, score 1.206) (writing took 0.19662096599995493 seconds)\n",
            "2024-01-07 20:04:35 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)\n",
            "2024-01-07 20:04:35 | INFO | train | epoch 146 | loss 1.183 | nll_loss 0.331 | ppl 1.26 | wps 967.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 730 | lr 0.00073 | gnorm 0.351 | clip 0 | train_wall 16 | wall 2674\n",
            "2024-01-07 20:04:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 147:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:04:36 | INFO | fairseq.trainer | begin training epoch 147\n",
            "2024-01-07 20:04:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 147:  80% 4/5 [00:13<00:02,  2.70s/it]2024-01-07 20:04:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 147 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 147 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:04:54 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 1.216 | nll_loss 0.21 | ppl 1.16 | wps 2786.6 | wpb 1927.5 | bsz 150 | num_updates 735 | best_loss 1.204\n",
            "2024-01-07 20:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 735 updates\n",
            "2024-01-07 20:04:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 147 @ 735 updates, score 1.216) (writing took 0.18698713699996006 seconds)\n",
            "2024-01-07 20:04:54 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)\n",
            "2024-01-07 20:04:54 | INFO | train | epoch 147 | loss 1.181 | nll_loss 0.326 | ppl 1.25 | wps 948.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 735 | lr 0.000735 | gnorm 0.438 | clip 0 | train_wall 17 | wall 2692\n",
            "2024-01-07 20:04:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 148:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:04:54 | INFO | fairseq.trainer | begin training epoch 148\n",
            "2024-01-07 20:04:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 148:  80% 4/5 [00:12<00:02,  2.98s/it]2024-01-07 20:05:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 148 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 148 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:05:11 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 1.211 | nll_loss 0.227 | ppl 1.17 | wps 2844.2 | wpb 1927.5 | bsz 150 | num_updates 740 | best_loss 1.204\n",
            "2024-01-07 20:05:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 740 updates\n",
            "2024-01-07 20:05:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 148 @ 740 updates, score 1.211) (writing took 0.1823468969996611 seconds)\n",
            "2024-01-07 20:05:12 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)\n",
            "2024-01-07 20:05:12 | INFO | train | epoch 148 | loss 1.181 | nll_loss 0.329 | ppl 1.26 | wps 978.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 740 | lr 0.00074 | gnorm 0.499 | clip 20 | train_wall 16 | wall 2710\n",
            "2024-01-07 20:05:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 149:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:05:12 | INFO | fairseq.trainer | begin training epoch 149\n",
            "2024-01-07 20:05:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 149:  80% 4/5 [00:16<00:04,  4.28s/it]2024-01-07 20:05:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 149 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 149 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:05:30 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 1.201 | nll_loss 0.215 | ppl 1.16 | wps 2837 | wpb 1927.5 | bsz 150 | num_updates 745 | best_loss 1.201\n",
            "2024-01-07 20:05:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 745 updates\n",
            "2024-01-07 20:05:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 149 @ 745 updates, score 1.201) (writing took 0.40577554099991175 seconds)\n",
            "2024-01-07 20:05:30 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)\n",
            "2024-01-07 20:05:30 | INFO | train | epoch 149 | loss 1.191 | nll_loss 0.347 | ppl 1.27 | wps 941 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 745 | lr 0.000745 | gnorm 0.575 | clip 20 | train_wall 16 | wall 2728\n",
            "2024-01-07 20:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 150:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:05:30 | INFO | fairseq.trainer | begin training epoch 150\n",
            "2024-01-07 20:05:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 150:  80% 4/5 [00:12<00:03,  3.71s/it]2024-01-07 20:05:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 150 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 150 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:05:48 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 1.201 | nll_loss 0.209 | ppl 1.16 | wps 2617.3 | wpb 1927.5 | bsz 150 | num_updates 750 | best_loss 1.201\n",
            "2024-01-07 20:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 750 updates\n",
            "2024-01-07 20:05:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:05:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:05:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 150 @ 750 updates, score 1.201) (writing took 0.3965163309999298 seconds)\n",
            "2024-01-07 20:05:48 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)\n",
            "2024-01-07 20:05:48 | INFO | train | epoch 150 | loss 1.187 | nll_loss 0.347 | ppl 1.27 | wps 957.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 750 | lr 0.00075 | gnorm 0.388 | clip 0 | train_wall 16 | wall 2747\n",
            "2024-01-07 20:05:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 151:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:05:48 | INFO | fairseq.trainer | begin training epoch 151\n",
            "2024-01-07 20:05:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 151:  80% 4/5 [00:16<00:03,  3.97s/it]2024-01-07 20:06:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 151 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 151 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:06:06 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 1.197 | nll_loss 0.203 | ppl 1.15 | wps 2745.8 | wpb 1927.5 | bsz 150 | num_updates 755 | best_loss 1.197\n",
            "2024-01-07 20:06:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 755 updates\n",
            "2024-01-07 20:06:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 151 @ 755 updates, score 1.197) (writing took 0.3894864620001499 seconds)\n",
            "2024-01-07 20:06:07 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)\n",
            "2024-01-07 20:06:07 | INFO | train | epoch 151 | loss 1.18 | nll_loss 0.332 | ppl 1.26 | wps 940.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 755 | lr 0.000755 | gnorm 0.611 | clip 20 | train_wall 16 | wall 2765\n",
            "2024-01-07 20:06:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 152:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:06:07 | INFO | fairseq.trainer | begin training epoch 152\n",
            "2024-01-07 20:06:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 152:  80% 4/5 [00:16<00:03,  3.89s/it]2024-01-07 20:06:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 152 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 152 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:06:24 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 1.211 | nll_loss 0.216 | ppl 1.16 | wps 2742.1 | wpb 1927.5 | bsz 150 | num_updates 760 | best_loss 1.197\n",
            "2024-01-07 20:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 760 updates\n",
            "2024-01-07 20:06:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:06:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 152 @ 760 updates, score 1.211) (writing took 0.20697102499980247 seconds)\n",
            "2024-01-07 20:06:25 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)\n",
            "2024-01-07 20:06:25 | INFO | train | epoch 152 | loss 1.192 | nll_loss 0.344 | ppl 1.27 | wps 958.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 760 | lr 0.00076 | gnorm 0.474 | clip 0 | train_wall 16 | wall 2783\n",
            "2024-01-07 20:06:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 153:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:06:25 | INFO | fairseq.trainer | begin training epoch 153\n",
            "2024-01-07 20:06:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 153:  80% 4/5 [00:12<00:02,  2.48s/it]2024-01-07 20:06:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 153 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 153 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:06:43 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 1.192 | nll_loss 0.203 | ppl 1.15 | wps 2715.8 | wpb 1927.5 | bsz 150 | num_updates 765 | best_loss 1.192\n",
            "2024-01-07 20:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 765 updates\n",
            "2024-01-07 20:06:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:06:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 153 @ 765 updates, score 1.192) (writing took 0.3486251059998722 seconds)\n",
            "2024-01-07 20:06:43 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)\n",
            "2024-01-07 20:06:43 | INFO | train | epoch 153 | loss 1.194 | nll_loss 0.348 | ppl 1.27 | wps 934 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 765 | lr 0.000765 | gnorm 0.498 | clip 0 | train_wall 17 | wall 2802\n",
            "2024-01-07 20:06:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 154:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:06:43 | INFO | fairseq.trainer | begin training epoch 154\n",
            "2024-01-07 20:06:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 154:  80% 4/5 [00:19<00:05,  5.11s/it]2024-01-07 20:07:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 154 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 154 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:07:04 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 1.215 | nll_loss 0.224 | ppl 1.17 | wps 2849.7 | wpb 1927.5 | bsz 150 | num_updates 770 | best_loss 1.192\n",
            "2024-01-07 20:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 770 updates\n",
            "2024-01-07 20:07:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 154 @ 770 updates, score 1.215) (writing took 0.3352393459999803 seconds)\n",
            "2024-01-07 20:07:05 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)\n",
            "2024-01-07 20:07:05 | INFO | train | epoch 154 | loss 1.177 | nll_loss 0.321 | ppl 1.25 | wps 806.5 | ups 0.23 | wpb 3472.2 | bsz 279.8 | num_updates 770 | lr 0.00077 | gnorm 0.525 | clip 20 | train_wall 20 | wall 2823\n",
            "2024-01-07 20:07:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 155:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:07:05 | INFO | fairseq.trainer | begin training epoch 155\n",
            "2024-01-07 20:07:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 155:  80% 4/5 [00:16<00:04,  4.09s/it]2024-01-07 20:07:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 155 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 155 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:07:23 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 1.235 | nll_loss 0.265 | ppl 1.2 | wps 2541.3 | wpb 1927.5 | bsz 150 | num_updates 775 | best_loss 1.192\n",
            "2024-01-07 20:07:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 775 updates\n",
            "2024-01-07 20:07:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 155 @ 775 updates, score 1.235) (writing took 0.1970745460002945 seconds)\n",
            "2024-01-07 20:07:23 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)\n",
            "2024-01-07 20:07:23 | INFO | train | epoch 155 | loss 1.197 | nll_loss 0.352 | ppl 1.28 | wps 959.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 775 | lr 0.000775 | gnorm 0.794 | clip 20 | train_wall 16 | wall 2841\n",
            "2024-01-07 20:07:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 156:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:07:23 | INFO | fairseq.trainer | begin training epoch 156\n",
            "2024-01-07 20:07:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 156:  80% 4/5 [00:12<00:03,  3.43s/it]2024-01-07 20:07:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 156 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 156 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:07:41 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 1.211 | nll_loss 0.218 | ppl 1.16 | wps 2782 | wpb 1927.5 | bsz 150 | num_updates 780 | best_loss 1.192\n",
            "2024-01-07 20:07:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 780 updates\n",
            "2024-01-07 20:07:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 156 @ 780 updates, score 1.211) (writing took 0.21610616399993887 seconds)\n",
            "2024-01-07 20:07:41 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
            "2024-01-07 20:07:41 | INFO | train | epoch 156 | loss 1.232 | nll_loss 0.409 | ppl 1.33 | wps 948.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 780 | lr 0.00078 | gnorm 0.773 | clip 40 | train_wall 17 | wall 2860\n",
            "2024-01-07 20:07:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 157:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:07:41 | INFO | fairseq.trainer | begin training epoch 157\n",
            "2024-01-07 20:07:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 157:  80% 4/5 [00:12<00:03,  3.51s/it]2024-01-07 20:07:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 157 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 157 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:07:59 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 1.204 | nll_loss 0.223 | ppl 1.17 | wps 1806.7 | wpb 1927.5 | bsz 150 | num_updates 785 | best_loss 1.192\n",
            "2024-01-07 20:07:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 785 updates\n",
            "2024-01-07 20:07:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 157 @ 785 updates, score 1.204) (writing took 0.28880290000006426 seconds)\n",
            "2024-01-07 20:07:59 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
            "2024-01-07 20:07:59 | INFO | train | epoch 157 | loss 1.233 | nll_loss 0.389 | ppl 1.31 | wps 958.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 785 | lr 0.000785 | gnorm 0.666 | clip 20 | train_wall 16 | wall 2878\n",
            "2024-01-07 20:07:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 158:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:07:59 | INFO | fairseq.trainer | begin training epoch 158\n",
            "2024-01-07 20:07:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 158:  80% 4/5 [00:13<00:02,  2.68s/it]2024-01-07 20:08:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 158 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 158 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:08:17 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 1.23 | nll_loss 0.232 | ppl 1.17 | wps 2718.8 | wpb 1927.5 | bsz 150 | num_updates 790 | best_loss 1.192\n",
            "2024-01-07 20:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 790 updates\n",
            "2024-01-07 20:08:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 158 @ 790 updates, score 1.23) (writing took 0.1933199360000799 seconds)\n",
            "2024-01-07 20:08:17 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
            "2024-01-07 20:08:17 | INFO | train | epoch 158 | loss 1.206 | nll_loss 0.358 | ppl 1.28 | wps 957.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 790 | lr 0.00079 | gnorm 0.625 | clip 20 | train_wall 16 | wall 2896\n",
            "2024-01-07 20:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 159:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:08:17 | INFO | fairseq.trainer | begin training epoch 159\n",
            "2024-01-07 20:08:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 159:  80% 4/5 [00:12<00:03,  3.20s/it]2024-01-07 20:08:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 159 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 159 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.82s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:08:36 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 1.207 | nll_loss 0.22 | ppl 1.16 | wps 2079.7 | wpb 1927.5 | bsz 150 | num_updates 795 | best_loss 1.192\n",
            "2024-01-07 20:08:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 795 updates\n",
            "2024-01-07 20:08:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 159 @ 795 updates, score 1.207) (writing took 0.19590339500018672 seconds)\n",
            "2024-01-07 20:08:36 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
            "2024-01-07 20:08:36 | INFO | train | epoch 159 | loss 1.206 | nll_loss 0.354 | ppl 1.28 | wps 941 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 795 | lr 0.000795 | gnorm 0.602 | clip 20 | train_wall 16 | wall 2914\n",
            "2024-01-07 20:08:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 160:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:08:36 | INFO | fairseq.trainer | begin training epoch 160\n",
            "2024-01-07 20:08:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 160:  80% 4/5 [00:12<00:02,  2.62s/it]2024-01-07 20:08:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 160 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 160 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:08:54 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 1.206 | nll_loss 0.223 | ppl 1.17 | wps 2432.1 | wpb 1927.5 | bsz 150 | num_updates 800 | best_loss 1.192\n",
            "2024-01-07 20:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 800 updates\n",
            "2024-01-07 20:08:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 160 @ 800 updates, score 1.206) (writing took 0.19872642600012114 seconds)\n",
            "2024-01-07 20:08:54 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
            "2024-01-07 20:08:54 | INFO | train | epoch 160 | loss 1.187 | nll_loss 0.337 | ppl 1.26 | wps 971 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 800 | lr 0.0008 | gnorm 0.636 | clip 20 | train_wall 16 | wall 2932\n",
            "2024-01-07 20:08:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 161:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:08:54 | INFO | fairseq.trainer | begin training epoch 161\n",
            "2024-01-07 20:08:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 161:  80% 4/5 [00:13<00:02,  2.73s/it]2024-01-07 20:09:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 161 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 161 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:09:12 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 1.204 | nll_loss 0.23 | ppl 1.17 | wps 2816.9 | wpb 1927.5 | bsz 150 | num_updates 805 | best_loss 1.192\n",
            "2024-01-07 20:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 805 updates\n",
            "2024-01-07 20:09:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 161 @ 805 updates, score 1.204) (writing took 0.1910080160000689 seconds)\n",
            "2024-01-07 20:09:12 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
            "2024-01-07 20:09:12 | INFO | train | epoch 161 | loss 1.199 | nll_loss 0.355 | ppl 1.28 | wps 943.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 805 | lr 0.000805 | gnorm 0.555 | clip 0 | train_wall 16 | wall 2951\n",
            "2024-01-07 20:09:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 162:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:09:12 | INFO | fairseq.trainer | begin training epoch 162\n",
            "2024-01-07 20:09:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 162:  80% 4/5 [00:12<00:03,  3.35s/it]2024-01-07 20:09:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 162 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 162 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:09:30 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 1.203 | nll_loss 0.218 | ppl 1.16 | wps 2727.8 | wpb 1927.5 | bsz 150 | num_updates 810 | best_loss 1.192\n",
            "2024-01-07 20:09:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 810 updates\n",
            "2024-01-07 20:09:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 162 @ 810 updates, score 1.203) (writing took 0.18214834700029314 seconds)\n",
            "2024-01-07 20:09:30 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
            "2024-01-07 20:09:30 | INFO | train | epoch 162 | loss 1.18 | nll_loss 0.339 | ppl 1.26 | wps 971.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 810 | lr 0.00081 | gnorm 0.492 | clip 0 | train_wall 16 | wall 2968\n",
            "2024-01-07 20:09:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 163:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:09:30 | INFO | fairseq.trainer | begin training epoch 163\n",
            "2024-01-07 20:09:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 163:  80% 4/5 [00:12<00:03,  3.48s/it]2024-01-07 20:09:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 163 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 163 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:09:48 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 1.196 | nll_loss 0.207 | ppl 1.15 | wps 2620.2 | wpb 1927.5 | bsz 150 | num_updates 815 | best_loss 1.192\n",
            "2024-01-07 20:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 815 updates\n",
            "2024-01-07 20:09:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 163 @ 815 updates, score 1.196) (writing took 0.179612797000118 seconds)\n",
            "2024-01-07 20:09:48 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
            "2024-01-07 20:09:48 | INFO | train | epoch 163 | loss 1.187 | nll_loss 0.34 | ppl 1.27 | wps 944.6 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 815 | lr 0.000815 | gnorm 0.536 | clip 0 | train_wall 17 | wall 2987\n",
            "2024-01-07 20:09:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 164:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:09:48 | INFO | fairseq.trainer | begin training epoch 164\n",
            "2024-01-07 20:09:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 164:  80% 4/5 [00:12<00:03,  3.17s/it]2024-01-07 20:10:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 164 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 164 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:10:06 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 1.191 | nll_loss 0.204 | ppl 1.15 | wps 2758.8 | wpb 1927.5 | bsz 150 | num_updates 820 | best_loss 1.191\n",
            "2024-01-07 20:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 820 updates\n",
            "2024-01-07 20:10:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:10:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:10:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 164 @ 820 updates, score 1.191) (writing took 0.3561729739999464 seconds)\n",
            "2024-01-07 20:10:06 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
            "2024-01-07 20:10:06 | INFO | train | epoch 164 | loss 1.182 | nll_loss 0.336 | ppl 1.26 | wps 960.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 820 | lr 0.00082 | gnorm 0.56 | clip 0 | train_wall 16 | wall 3005\n",
            "2024-01-07 20:10:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 165:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:10:07 | INFO | fairseq.trainer | begin training epoch 165\n",
            "2024-01-07 20:10:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 165:  80% 4/5 [00:12<00:02,  2.54s/it]2024-01-07 20:10:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 165 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 165 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:10:25 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 1.196 | nll_loss 0.22 | ppl 1.17 | wps 2303.7 | wpb 1927.5 | bsz 150 | num_updates 825 | best_loss 1.191\n",
            "2024-01-07 20:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 825 updates\n",
            "2024-01-07 20:10:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 165 @ 825 updates, score 1.196) (writing took 0.20111227500001405 seconds)\n",
            "2024-01-07 20:10:25 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
            "2024-01-07 20:10:25 | INFO | train | epoch 165 | loss 1.191 | nll_loss 0.346 | ppl 1.27 | wps 946.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 825 | lr 0.000825 | gnorm 0.693 | clip 20 | train_wall 17 | wall 3023\n",
            "2024-01-07 20:10:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 166:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:10:25 | INFO | fairseq.trainer | begin training epoch 166\n",
            "2024-01-07 20:10:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 166:  80% 4/5 [00:12<00:03,  3.14s/it]2024-01-07 20:10:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 166 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 166 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:10:43 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 1.198 | nll_loss 0.207 | ppl 1.15 | wps 2589 | wpb 1927.5 | bsz 150 | num_updates 830 | best_loss 1.191\n",
            "2024-01-07 20:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 830 updates\n",
            "2024-01-07 20:10:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:10:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 166 @ 830 updates, score 1.198) (writing took 0.18774907599981816 seconds)\n",
            "2024-01-07 20:10:43 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
            "2024-01-07 20:10:43 | INFO | train | epoch 166 | loss 1.198 | nll_loss 0.355 | ppl 1.28 | wps 961.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 830 | lr 0.00083 | gnorm 0.636 | clip 20 | train_wall 16 | wall 3041\n",
            "2024-01-07 20:10:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 167:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:10:43 | INFO | fairseq.trainer | begin training epoch 167\n",
            "2024-01-07 20:10:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 167:  80% 4/5 [00:12<00:02,  2.45s/it]2024-01-07 20:11:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 167 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 167 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:11:01 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 1.195 | nll_loss 0.222 | ppl 1.17 | wps 2783.4 | wpb 1927.5 | bsz 150 | num_updates 835 | best_loss 1.191\n",
            "2024-01-07 20:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 835 updates\n",
            "2024-01-07 20:11:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 167 @ 835 updates, score 1.195) (writing took 0.154377339000348 seconds)\n",
            "2024-01-07 20:11:01 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)\n",
            "2024-01-07 20:11:01 | INFO | train | epoch 167 | loss 1.184 | nll_loss 0.332 | ppl 1.26 | wps 948.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 835 | lr 0.000835 | gnorm 0.511 | clip 0 | train_wall 17 | wall 3060\n",
            "2024-01-07 20:11:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 168:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:11:01 | INFO | fairseq.trainer | begin training epoch 168\n",
            "2024-01-07 20:11:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 168:  80% 4/5 [00:12<00:03,  3.06s/it]2024-01-07 20:11:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 168 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 168 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.62s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:11:19 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 1.188 | nll_loss 0.212 | ppl 1.16 | wps 2192 | wpb 1927.5 | bsz 150 | num_updates 840 | best_loss 1.188\n",
            "2024-01-07 20:11:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 840 updates\n",
            "2024-01-07 20:11:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 168 @ 840 updates, score 1.188) (writing took 0.47157569699993473 seconds)\n",
            "2024-01-07 20:11:20 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)\n",
            "2024-01-07 20:11:20 | INFO | train | epoch 168 | loss 1.184 | nll_loss 0.349 | ppl 1.27 | wps 945.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 840 | lr 0.00084 | gnorm 0.68 | clip 20 | train_wall 16 | wall 3078\n",
            "2024-01-07 20:11:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 169:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:11:20 | INFO | fairseq.trainer | begin training epoch 169\n",
            "2024-01-07 20:11:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 169:  80% 4/5 [00:11<00:02,  2.99s/it]2024-01-07 20:11:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 169 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 169 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:11:37 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 1.182 | nll_loss 0.203 | ppl 1.15 | wps 2753.7 | wpb 1927.5 | bsz 150 | num_updates 845 | best_loss 1.182\n",
            "2024-01-07 20:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 845 updates\n",
            "2024-01-07 20:11:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:11:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 169 @ 845 updates, score 1.182) (writing took 0.44727084300029674 seconds)\n",
            "2024-01-07 20:11:38 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)\n",
            "2024-01-07 20:11:38 | INFO | train | epoch 169 | loss 1.179 | nll_loss 0.33 | ppl 1.26 | wps 947.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 845 | lr 0.000845 | gnorm 0.604 | clip 20 | train_wall 16 | wall 3096\n",
            "2024-01-07 20:11:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 170:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:11:38 | INFO | fairseq.trainer | begin training epoch 170\n",
            "2024-01-07 20:11:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 170:  80% 4/5 [00:12<00:03,  3.47s/it]2024-01-07 20:11:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 170 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 170 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:11:56 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 1.183 | nll_loss 0.209 | ppl 1.16 | wps 2875.2 | wpb 1927.5 | bsz 150 | num_updates 850 | best_loss 1.182\n",
            "2024-01-07 20:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 850 updates\n",
            "2024-01-07 20:11:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 170 @ 850 updates, score 1.183) (writing took 0.16996077599969794 seconds)\n",
            "2024-01-07 20:11:56 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)\n",
            "2024-01-07 20:11:56 | INFO | train | epoch 170 | loss 1.174 | nll_loss 0.33 | ppl 1.26 | wps 952.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 850 | lr 0.00085 | gnorm 0.802 | clip 20 | train_wall 16 | wall 3115\n",
            "2024-01-07 20:11:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 171:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:11:56 | INFO | fairseq.trainer | begin training epoch 171\n",
            "2024-01-07 20:11:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 171:  80% 4/5 [00:11<00:02,  2.91s/it]2024-01-07 20:12:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 171 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 171 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:12:14 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 1.171 | nll_loss 0.2 | ppl 1.15 | wps 2570.3 | wpb 1927.5 | bsz 150 | num_updates 855 | best_loss 1.171\n",
            "2024-01-07 20:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 855 updates\n",
            "2024-01-07 20:12:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:12:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 171 @ 855 updates, score 1.171) (writing took 0.3649212800000896 seconds)\n",
            "2024-01-07 20:12:14 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)\n",
            "2024-01-07 20:12:14 | INFO | train | epoch 171 | loss 1.178 | nll_loss 0.337 | ppl 1.26 | wps 967.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 855 | lr 0.000855 | gnorm 0.49 | clip 0 | train_wall 16 | wall 3132\n",
            "2024-01-07 20:12:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 172:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:12:14 | INFO | fairseq.trainer | begin training epoch 172\n",
            "2024-01-07 20:12:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 172:  80% 4/5 [00:16<00:03,  3.91s/it]2024-01-07 20:12:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 172 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 172 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:12:32 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 1.179 | nll_loss 0.194 | ppl 1.14 | wps 2734.8 | wpb 1927.5 | bsz 150 | num_updates 860 | best_loss 1.171\n",
            "2024-01-07 20:12:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 860 updates\n",
            "2024-01-07 20:12:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 172 @ 860 updates, score 1.179) (writing took 0.18306395499985229 seconds)\n",
            "2024-01-07 20:12:32 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)\n",
            "2024-01-07 20:12:32 | INFO | train | epoch 172 | loss 1.165 | nll_loss 0.322 | ppl 1.25 | wps 953.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 860 | lr 0.00086 | gnorm 0.42 | clip 0 | train_wall 16 | wall 3151\n",
            "2024-01-07 20:12:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 173:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:12:32 | INFO | fairseq.trainer | begin training epoch 173\n",
            "2024-01-07 20:12:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 173:  80% 4/5 [00:15<00:04,  4.08s/it]2024-01-07 20:12:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 173 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 173 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:12:50 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 1.165 | nll_loss 0.193 | ppl 1.14 | wps 2810.6 | wpb 1927.5 | bsz 150 | num_updates 865 | best_loss 1.165\n",
            "2024-01-07 20:12:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 865 updates\n",
            "2024-01-07 20:12:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 173 @ 865 updates, score 1.165) (writing took 0.4121190360001492 seconds)\n",
            "2024-01-07 20:12:50 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)\n",
            "2024-01-07 20:12:50 | INFO | train | epoch 173 | loss 1.147 | nll_loss 0.292 | ppl 1.22 | wps 959.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 865 | lr 0.000865 | gnorm 0.344 | clip 0 | train_wall 16 | wall 3169\n",
            "2024-01-07 20:12:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 174:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:12:50 | INFO | fairseq.trainer | begin training epoch 174\n",
            "2024-01-07 20:12:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 174:  80% 4/5 [00:16<00:04,  4.17s/it]2024-01-07 20:13:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 174 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 174 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:13:08 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 1.163 | nll_loss 0.19 | ppl 1.14 | wps 2844.3 | wpb 1927.5 | bsz 150 | num_updates 870 | best_loss 1.163\n",
            "2024-01-07 20:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 870 updates\n",
            "2024-01-07 20:13:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:13:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 174 @ 870 updates, score 1.163) (writing took 0.4171277660002488 seconds)\n",
            "2024-01-07 20:13:09 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)\n",
            "2024-01-07 20:13:09 | INFO | train | epoch 174 | loss 1.147 | nll_loss 0.303 | ppl 1.23 | wps 938.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 870 | lr 0.00087 | gnorm 0.451 | clip 0 | train_wall 17 | wall 3187\n",
            "2024-01-07 20:13:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 175:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:13:09 | INFO | fairseq.trainer | begin training epoch 175\n",
            "2024-01-07 20:13:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 175:  80% 4/5 [00:12<00:03,  3.24s/it]2024-01-07 20:13:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 175 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 175 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:13:26 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 1.165 | nll_loss 0.2 | ppl 1.15 | wps 2776.5 | wpb 1927.5 | bsz 150 | num_updates 875 | best_loss 1.163\n",
            "2024-01-07 20:13:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 875 updates\n",
            "2024-01-07 20:13:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 175 @ 875 updates, score 1.165) (writing took 0.16695350700001654 seconds)\n",
            "2024-01-07 20:13:27 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)\n",
            "2024-01-07 20:13:27 | INFO | train | epoch 175 | loss 1.135 | nll_loss 0.288 | ppl 1.22 | wps 978.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 875 | lr 0.000875 | gnorm 0.394 | clip 0 | train_wall 16 | wall 3205\n",
            "2024-01-07 20:13:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 176:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:13:27 | INFO | fairseq.trainer | begin training epoch 176\n",
            "2024-01-07 20:13:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 176:  80% 4/5 [00:12<00:03,  3.27s/it]2024-01-07 20:13:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 176 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 176 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:13:45 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 1.164 | nll_loss 0.188 | ppl 1.14 | wps 2847.6 | wpb 1927.5 | bsz 150 | num_updates 880 | best_loss 1.163\n",
            "2024-01-07 20:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 880 updates\n",
            "2024-01-07 20:13:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:13:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 176 @ 880 updates, score 1.164) (writing took 0.1760064759996567 seconds)\n",
            "2024-01-07 20:13:45 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)\n",
            "2024-01-07 20:13:45 | INFO | train | epoch 176 | loss 1.14 | nll_loss 0.298 | ppl 1.23 | wps 952.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 880 | lr 0.00088 | gnorm 0.43 | clip 0 | train_wall 16 | wall 3223\n",
            "2024-01-07 20:13:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 177:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:13:45 | INFO | fairseq.trainer | begin training epoch 177\n",
            "2024-01-07 20:13:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 177:  80% 4/5 [00:13<00:02,  2.63s/it]2024-01-07 20:14:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 177 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 177 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:14:03 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 1.159 | nll_loss 0.179 | ppl 1.13 | wps 2562.6 | wpb 1927.5 | bsz 150 | num_updates 885 | best_loss 1.159\n",
            "2024-01-07 20:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 885 updates\n",
            "2024-01-07 20:14:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:14:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:14:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 177 @ 885 updates, score 1.159) (writing took 0.47211633099959727 seconds)\n",
            "2024-01-07 20:14:03 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)\n",
            "2024-01-07 20:14:03 | INFO | train | epoch 177 | loss 1.132 | nll_loss 0.281 | ppl 1.22 | wps 950.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 885 | lr 0.000885 | gnorm 0.427 | clip 0 | train_wall 16 | wall 3242\n",
            "2024-01-07 20:14:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 178:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:14:03 | INFO | fairseq.trainer | begin training epoch 178\n",
            "2024-01-07 20:14:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 178:  80% 4/5 [00:12<00:03,  3.39s/it]2024-01-07 20:14:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 178 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 178 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:14:21 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 1.174 | nll_loss 0.193 | ppl 1.14 | wps 2775.4 | wpb 1927.5 | bsz 150 | num_updates 890 | best_loss 1.159\n",
            "2024-01-07 20:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 890 updates\n",
            "2024-01-07 20:14:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:14:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 178 @ 890 updates, score 1.174) (writing took 0.1811914049999359 seconds)\n",
            "2024-01-07 20:14:21 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)\n",
            "2024-01-07 20:14:21 | INFO | train | epoch 178 | loss 1.147 | nll_loss 0.294 | ppl 1.23 | wps 951.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 890 | lr 0.00089 | gnorm 0.48 | clip 0 | train_wall 16 | wall 3260\n",
            "2024-01-07 20:14:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 179:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:14:21 | INFO | fairseq.trainer | begin training epoch 179\n",
            "2024-01-07 20:14:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 179:  80% 4/5 [00:12<00:03,  3.03s/it]2024-01-07 20:14:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 179 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 179 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.70s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:14:39 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 1.159 | nll_loss 0.2 | ppl 1.15 | wps 2173 | wpb 1927.5 | bsz 150 | num_updates 895 | best_loss 1.159\n",
            "2024-01-07 20:14:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 895 updates\n",
            "2024-01-07 20:14:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:14:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:14:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 179 @ 895 updates, score 1.159) (writing took 0.4849933499999679 seconds)\n",
            "2024-01-07 20:14:40 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)\n",
            "2024-01-07 20:14:40 | INFO | train | epoch 179 | loss 1.141 | nll_loss 0.297 | ppl 1.23 | wps 942.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 895 | lr 0.000895 | gnorm 0.56 | clip 20 | train_wall 16 | wall 3278\n",
            "2024-01-07 20:14:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 180:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:14:40 | INFO | fairseq.trainer | begin training epoch 180\n",
            "2024-01-07 20:14:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 180:  80% 4/5 [00:12<00:02,  2.42s/it]2024-01-07 20:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 180 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 180 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:14:58 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 1.17 | nll_loss 0.202 | ppl 1.15 | wps 2858 | wpb 1927.5 | bsz 150 | num_updates 900 | best_loss 1.159\n",
            "2024-01-07 20:14:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 900 updates\n",
            "2024-01-07 20:14:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 180 @ 900 updates, score 1.17) (writing took 0.1887007939999421 seconds)\n",
            "2024-01-07 20:14:58 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)\n",
            "2024-01-07 20:14:58 | INFO | train | epoch 180 | loss 1.158 | nll_loss 0.318 | ppl 1.25 | wps 962.4 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 900 | lr 0.0009 | gnorm 0.586 | clip 20 | train_wall 16 | wall 3296\n",
            "2024-01-07 20:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 181:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:14:58 | INFO | fairseq.trainer | begin training epoch 181\n",
            "2024-01-07 20:14:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 181:  80% 4/5 [00:12<00:03,  3.52s/it]2024-01-07 20:15:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 181 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 181 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:15:16 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 1.173 | nll_loss 0.206 | ppl 1.15 | wps 2796.2 | wpb 1927.5 | bsz 150 | num_updates 905 | best_loss 1.159\n",
            "2024-01-07 20:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 905 updates\n",
            "2024-01-07 20:15:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 181 @ 905 updates, score 1.173) (writing took 0.19191803500007154 seconds)\n",
            "2024-01-07 20:15:16 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)\n",
            "2024-01-07 20:15:16 | INFO | train | epoch 181 | loss 1.163 | nll_loss 0.321 | ppl 1.25 | wps 942.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 905 | lr 0.000905 | gnorm 0.542 | clip 20 | train_wall 16 | wall 3315\n",
            "2024-01-07 20:15:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 182:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:15:16 | INFO | fairseq.trainer | begin training epoch 182\n",
            "2024-01-07 20:15:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 182:  80% 4/5 [00:15<00:03,  3.94s/it]2024-01-07 20:15:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 182 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 182 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:15:34 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 1.17 | nll_loss 0.2 | ppl 1.15 | wps 2621.2 | wpb 1927.5 | bsz 150 | num_updates 910 | best_loss 1.159\n",
            "2024-01-07 20:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 910 updates\n",
            "2024-01-07 20:15:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 182 @ 910 updates, score 1.17) (writing took 0.17332443599980252 seconds)\n",
            "2024-01-07 20:15:34 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)\n",
            "2024-01-07 20:15:34 | INFO | train | epoch 182 | loss 1.154 | nll_loss 0.31 | ppl 1.24 | wps 971.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 910 | lr 0.00091 | gnorm 0.516 | clip 20 | train_wall 16 | wall 3333\n",
            "2024-01-07 20:15:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 183:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:15:34 | INFO | fairseq.trainer | begin training epoch 183\n",
            "2024-01-07 20:15:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 183:  80% 4/5 [00:16<00:04,  4.04s/it]2024-01-07 20:15:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 183 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 183 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:15:52 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 1.183 | nll_loss 0.209 | ppl 1.16 | wps 2754.2 | wpb 1927.5 | bsz 150 | num_updates 915 | best_loss 1.159\n",
            "2024-01-07 20:15:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 915 updates\n",
            "2024-01-07 20:15:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 183 @ 915 updates, score 1.183) (writing took 0.19298973400009345 seconds)\n",
            "2024-01-07 20:15:52 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)\n",
            "2024-01-07 20:15:52 | INFO | train | epoch 183 | loss 1.145 | nll_loss 0.295 | ppl 1.23 | wps 949.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 915 | lr 0.000915 | gnorm 0.461 | clip 0 | train_wall 17 | wall 3351\n",
            "2024-01-07 20:15:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 184:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:15:52 | INFO | fairseq.trainer | begin training epoch 184\n",
            "2024-01-07 20:15:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 184:  80% 4/5 [00:12<00:03,  3.67s/it]2024-01-07 20:16:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 184 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 184 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:16:10 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 1.174 | nll_loss 0.207 | ppl 1.15 | wps 2737.1 | wpb 1927.5 | bsz 150 | num_updates 920 | best_loss 1.159\n",
            "2024-01-07 20:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 920 updates\n",
            "2024-01-07 20:16:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 184 @ 920 updates, score 1.174) (writing took 0.20963684200023636 seconds)\n",
            "2024-01-07 20:16:10 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)\n",
            "2024-01-07 20:16:10 | INFO | train | epoch 184 | loss 1.163 | nll_loss 0.308 | ppl 1.24 | wps 966.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 920 | lr 0.00092 | gnorm 0.578 | clip 20 | train_wall 16 | wall 3369\n",
            "2024-01-07 20:16:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 185:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:16:10 | INFO | fairseq.trainer | begin training epoch 185\n",
            "2024-01-07 20:16:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 185:  80% 4/5 [00:16<00:03,  3.96s/it]2024-01-07 20:16:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 185 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 185 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:16:29 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 1.166 | nll_loss 0.213 | ppl 1.16 | wps 2544 | wpb 1927.5 | bsz 150 | num_updates 925 | best_loss 1.159\n",
            "2024-01-07 20:16:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 925 updates\n",
            "2024-01-07 20:16:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 185 @ 925 updates, score 1.166) (writing took 0.21860230199990838 seconds)\n",
            "2024-01-07 20:16:29 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)\n",
            "2024-01-07 20:16:29 | INFO | train | epoch 185 | loss 1.155 | nll_loss 0.312 | ppl 1.24 | wps 944.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 925 | lr 0.000925 | gnorm 0.545 | clip 20 | train_wall 17 | wall 3387\n",
            "2024-01-07 20:16:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 186:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:16:29 | INFO | fairseq.trainer | begin training epoch 186\n",
            "2024-01-07 20:16:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 186:  80% 4/5 [00:13<00:03,  3.25s/it]2024-01-07 20:16:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 186 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 186 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:16:47 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 1.171 | nll_loss 0.204 | ppl 1.15 | wps 2841.5 | wpb 1927.5 | bsz 150 | num_updates 930 | best_loss 1.159\n",
            "2024-01-07 20:16:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 930 updates\n",
            "2024-01-07 20:16:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 186 @ 930 updates, score 1.171) (writing took 0.1937464339998769 seconds)\n",
            "2024-01-07 20:16:47 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
            "2024-01-07 20:16:47 | INFO | train | epoch 186 | loss 1.165 | nll_loss 0.333 | ppl 1.26 | wps 961 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 930 | lr 0.00093 | gnorm 0.504 | clip 0 | train_wall 16 | wall 3405\n",
            "2024-01-07 20:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 187:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:16:47 | INFO | fairseq.trainer | begin training epoch 187\n",
            "2024-01-07 20:16:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 187:  80% 4/5 [00:11<00:02,  2.39s/it]2024-01-07 20:17:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 187 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 187 | valid on 'valid' subset:  50% 1/2 [00:02<00:02,  2.88s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:17:07 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 1.169 | nll_loss 0.198 | ppl 1.15 | wps 1256.3 | wpb 1927.5 | bsz 150 | num_updates 935 | best_loss 1.159\n",
            "2024-01-07 20:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 935 updates\n",
            "2024-01-07 20:17:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 187 @ 935 updates, score 1.169) (writing took 0.45278799300012906 seconds)\n",
            "2024-01-07 20:17:08 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
            "2024-01-07 20:17:08 | INFO | train | epoch 187 | loss 1.151 | nll_loss 0.3 | ppl 1.23 | wps 827.4 | ups 0.24 | wpb 3472.2 | bsz 279.8 | num_updates 935 | lr 0.000935 | gnorm 0.628 | clip 20 | train_wall 17 | wall 3426\n",
            "2024-01-07 20:17:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 188:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:17:08 | INFO | fairseq.trainer | begin training epoch 188\n",
            "2024-01-07 20:17:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 188:  80% 4/5 [00:12<00:02,  3.00s/it]2024-01-07 20:17:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 188 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 188 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:17:26 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 1.178 | nll_loss 0.205 | ppl 1.15 | wps 2784.8 | wpb 1927.5 | bsz 150 | num_updates 940 | best_loss 1.159\n",
            "2024-01-07 20:17:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 940 updates\n",
            "2024-01-07 20:17:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 188 @ 940 updates, score 1.178) (writing took 0.3213399030000801 seconds)\n",
            "2024-01-07 20:17:27 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
            "2024-01-07 20:17:27 | INFO | train | epoch 188 | loss 1.144 | nll_loss 0.295 | ppl 1.23 | wps 922.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 940 | lr 0.00094 | gnorm 0.36 | clip 0 | train_wall 17 | wall 3445\n",
            "2024-01-07 20:17:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 189:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:17:27 | INFO | fairseq.trainer | begin training epoch 189\n",
            "2024-01-07 20:17:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 189:  80% 4/5 [00:16<00:03,  3.84s/it]2024-01-07 20:17:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 189 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 189 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:17:44 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 1.163 | nll_loss 0.198 | ppl 1.15 | wps 2751.7 | wpb 1927.5 | bsz 150 | num_updates 945 | best_loss 1.159\n",
            "2024-01-07 20:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 945 updates\n",
            "2024-01-07 20:17:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 189 @ 945 updates, score 1.163) (writing took 0.21455138300007093 seconds)\n",
            "2024-01-07 20:17:45 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
            "2024-01-07 20:17:45 | INFO | train | epoch 189 | loss 1.141 | nll_loss 0.29 | ppl 1.22 | wps 959.6 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 945 | lr 0.000945 | gnorm 0.546 | clip 20 | train_wall 16 | wall 3463\n",
            "2024-01-07 20:17:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 190:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:17:45 | INFO | fairseq.trainer | begin training epoch 190\n",
            "2024-01-07 20:17:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 190:  80% 4/5 [00:12<00:03,  3.33s/it]2024-01-07 20:18:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 190 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 190 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:18:03 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 1.173 | nll_loss 0.2 | ppl 1.15 | wps 2658.3 | wpb 1927.5 | bsz 150 | num_updates 950 | best_loss 1.159\n",
            "2024-01-07 20:18:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 950 updates\n",
            "2024-01-07 20:18:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 190 @ 950 updates, score 1.173) (writing took 0.24324781999985134 seconds)\n",
            "2024-01-07 20:18:03 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
            "2024-01-07 20:18:03 | INFO | train | epoch 190 | loss 1.153 | nll_loss 0.315 | ppl 1.24 | wps 941.4 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 950 | lr 0.00095 | gnorm 0.422 | clip 0 | train_wall 17 | wall 3482\n",
            "2024-01-07 20:18:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 191:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:18:03 | INFO | fairseq.trainer | begin training epoch 191\n",
            "2024-01-07 20:18:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 191:  80% 4/5 [00:11<00:03,  3.27s/it]2024-01-07 20:18:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 191 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 191 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:18:21 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 1.18 | nll_loss 0.207 | ppl 1.15 | wps 2834.1 | wpb 1927.5 | bsz 150 | num_updates 955 | best_loss 1.159\n",
            "2024-01-07 20:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 955 updates\n",
            "2024-01-07 20:18:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 191 @ 955 updates, score 1.18) (writing took 0.3158954740001718 seconds)\n",
            "2024-01-07 20:18:21 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
            "2024-01-07 20:18:21 | INFO | train | epoch 191 | loss 1.165 | nll_loss 0.31 | ppl 1.24 | wps 956.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 955 | lr 0.000955 | gnorm 0.814 | clip 20 | train_wall 16 | wall 3500\n",
            "2024-01-07 20:18:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 192:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:18:21 | INFO | fairseq.trainer | begin training epoch 192\n",
            "2024-01-07 20:18:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 192:  80% 4/5 [00:12<00:03,  3.48s/it]2024-01-07 20:18:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 192 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 192 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:18:40 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 1.163 | nll_loss 0.209 | ppl 1.16 | wps 2679.3 | wpb 1927.5 | bsz 150 | num_updates 960 | best_loss 1.159\n",
            "2024-01-07 20:18:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 960 updates\n",
            "2024-01-07 20:18:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 192 @ 960 updates, score 1.163) (writing took 0.18707283399999142 seconds)\n",
            "2024-01-07 20:18:40 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
            "2024-01-07 20:18:40 | INFO | train | epoch 192 | loss 1.168 | nll_loss 0.329 | ppl 1.26 | wps 929.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 960 | lr 0.00096 | gnorm 0.589 | clip 20 | train_wall 17 | wall 3518\n",
            "2024-01-07 20:18:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 193:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:18:40 | INFO | fairseq.trainer | begin training epoch 193\n",
            "2024-01-07 20:18:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 193:  80% 4/5 [00:12<00:03,  3.37s/it]2024-01-07 20:18:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 193 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 193 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.78s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:18:58 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 1.169 | nll_loss 0.207 | ppl 1.15 | wps 2011.1 | wpb 1927.5 | bsz 150 | num_updates 965 | best_loss 1.159\n",
            "2024-01-07 20:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 965 updates\n",
            "2024-01-07 20:18:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 193 @ 965 updates, score 1.169) (writing took 0.27256800800023484 seconds)\n",
            "2024-01-07 20:18:58 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
            "2024-01-07 20:18:58 | INFO | train | epoch 193 | loss 1.158 | nll_loss 0.322 | ppl 1.25 | wps 944.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 965 | lr 0.000965 | gnorm 0.482 | clip 0 | train_wall 16 | wall 3537\n",
            "2024-01-07 20:18:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 194:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:18:58 | INFO | fairseq.trainer | begin training epoch 194\n",
            "2024-01-07 20:18:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 194:  80% 4/5 [00:12<00:02,  2.86s/it]2024-01-07 20:19:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 194 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 194 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:19:16 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 1.16 | nll_loss 0.197 | ppl 1.15 | wps 2856.6 | wpb 1927.5 | bsz 150 | num_updates 970 | best_loss 1.159\n",
            "2024-01-07 20:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 970 updates\n",
            "2024-01-07 20:19:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 194 @ 970 updates, score 1.16) (writing took 0.18780582399995183 seconds)\n",
            "2024-01-07 20:19:16 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
            "2024-01-07 20:19:16 | INFO | train | epoch 194 | loss 1.167 | nll_loss 0.324 | ppl 1.25 | wps 968.6 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 970 | lr 0.00097 | gnorm 0.631 | clip 20 | train_wall 16 | wall 3555\n",
            "2024-01-07 20:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 195:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:19:16 | INFO | fairseq.trainer | begin training epoch 195\n",
            "2024-01-07 20:19:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 195:  80% 4/5 [00:16<00:03,  3.89s/it]2024-01-07 20:19:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 195 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 195 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:19:34 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 1.157 | nll_loss 0.2 | ppl 1.15 | wps 2841.7 | wpb 1927.5 | bsz 150 | num_updates 975 | best_loss 1.157\n",
            "2024-01-07 20:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 975 updates\n",
            "2024-01-07 20:19:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:19:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 195 @ 975 updates, score 1.157) (writing took 0.3801811389998875 seconds)\n",
            "2024-01-07 20:19:35 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)\n",
            "2024-01-07 20:19:35 | INFO | train | epoch 195 | loss 1.15 | nll_loss 0.305 | ppl 1.24 | wps 938.1 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 975 | lr 0.000975 | gnorm 0.529 | clip 0 | train_wall 16 | wall 3573\n",
            "2024-01-07 20:19:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 196:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:19:35 | INFO | fairseq.trainer | begin training epoch 196\n",
            "2024-01-07 20:19:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 196:  80% 4/5 [00:12<00:03,  3.47s/it]2024-01-07 20:19:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 196 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 196 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:19:53 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 1.188 | nll_loss 0.214 | ppl 1.16 | wps 2606.5 | wpb 1927.5 | bsz 150 | num_updates 980 | best_loss 1.157\n",
            "2024-01-07 20:19:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 980 updates\n",
            "2024-01-07 20:19:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:19:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:19:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 196 @ 980 updates, score 1.188) (writing took 0.2274535219999052 seconds)\n",
            "2024-01-07 20:19:53 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)\n",
            "2024-01-07 20:19:53 | INFO | train | epoch 196 | loss 1.162 | nll_loss 0.324 | ppl 1.25 | wps 957.9 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 980 | lr 0.00098 | gnorm 0.389 | clip 0 | train_wall 16 | wall 3591\n",
            "2024-01-07 20:19:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 197:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:19:53 | INFO | fairseq.trainer | begin training epoch 197\n",
            "2024-01-07 20:19:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 197:  80% 4/5 [00:12<00:03,  3.07s/it]2024-01-07 20:20:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 197 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 197 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:20:11 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 1.172 | nll_loss 0.2 | ppl 1.15 | wps 2786.5 | wpb 1927.5 | bsz 150 | num_updates 985 | best_loss 1.157\n",
            "2024-01-07 20:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 985 updates\n",
            "2024-01-07 20:20:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:20:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 197 @ 985 updates, score 1.172) (writing took 0.208907893000287 seconds)\n",
            "2024-01-07 20:20:11 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)\n",
            "2024-01-07 20:20:11 | INFO | train | epoch 197 | loss 1.152 | nll_loss 0.295 | ppl 1.23 | wps 945 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 985 | lr 0.000985 | gnorm 0.457 | clip 0 | train_wall 17 | wall 3610\n",
            "2024-01-07 20:20:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 198:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:20:11 | INFO | fairseq.trainer | begin training epoch 198\n",
            "2024-01-07 20:20:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 198:  80% 4/5 [00:15<00:04,  4.15s/it]2024-01-07 20:20:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 198 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 198 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:20:29 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 1.153 | nll_loss 0.192 | ppl 1.14 | wps 2853.1 | wpb 1927.5 | bsz 150 | num_updates 990 | best_loss 1.153\n",
            "2024-01-07 20:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 990 updates\n",
            "2024-01-07 20:20:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 198 @ 990 updates, score 1.153) (writing took 0.3844544489998043 seconds)\n",
            "2024-01-07 20:20:29 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)\n",
            "2024-01-07 20:20:29 | INFO | train | epoch 198 | loss 1.13 | nll_loss 0.278 | ppl 1.21 | wps 958.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 990 | lr 0.00099 | gnorm 0.606 | clip 20 | train_wall 16 | wall 3628\n",
            "2024-01-07 20:20:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 199:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:20:29 | INFO | fairseq.trainer | begin training epoch 199\n",
            "2024-01-07 20:20:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 199:  80% 4/5 [00:11<00:03,  3.14s/it]2024-01-07 20:20:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 199 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 199 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:20:47 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 1.157 | nll_loss 0.209 | ppl 1.16 | wps 2737.9 | wpb 1927.5 | bsz 150 | num_updates 995 | best_loss 1.153\n",
            "2024-01-07 20:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 995 updates\n",
            "2024-01-07 20:20:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:20:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 199 @ 995 updates, score 1.157) (writing took 0.163103215999854 seconds)\n",
            "2024-01-07 20:20:48 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)\n",
            "2024-01-07 20:20:48 | INFO | train | epoch 199 | loss 1.143 | nll_loss 0.302 | ppl 1.23 | wps 956.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 995 | lr 0.000995 | gnorm 0.496 | clip 0 | train_wall 16 | wall 3646\n",
            "2024-01-07 20:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 200:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:20:48 | INFO | fairseq.trainer | begin training epoch 200\n",
            "2024-01-07 20:20:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 200:  80% 4/5 [00:12<00:02,  2.96s/it]2024-01-07 20:21:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 200 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 200 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:21:05 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 1.155 | nll_loss 0.199 | ppl 1.15 | wps 2837.8 | wpb 1927.5 | bsz 150 | num_updates 1000 | best_loss 1.153\n",
            "2024-01-07 20:21:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 1000 updates\n",
            "2024-01-07 20:21:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:21:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:21:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 200 @ 1000 updates, score 1.155) (writing took 0.15533653700003924 seconds)\n",
            "2024-01-07 20:21:05 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)\n",
            "2024-01-07 20:21:05 | INFO | train | epoch 200 | loss 1.14 | nll_loss 0.299 | ppl 1.23 | wps 977.6 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1000 | lr 0.001 | gnorm 0.509 | clip 0 | train_wall 16 | wall 3664\n",
            "2024-01-07 20:21:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 201:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:21:05 | INFO | fairseq.trainer | begin training epoch 201\n",
            "2024-01-07 20:21:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 201:  80% 4/5 [00:13<00:02,  2.68s/it]2024-01-07 20:21:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 201 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 201 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:21:23 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 1.157 | nll_loss 0.198 | ppl 1.15 | wps 2874.2 | wpb 1927.5 | bsz 150 | num_updates 1005 | best_loss 1.153\n",
            "2024-01-07 20:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 1005 updates\n",
            "2024-01-07 20:21:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:21:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 201 @ 1005 updates, score 1.157) (writing took 0.17523692700024185 seconds)\n",
            "2024-01-07 20:21:24 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)\n",
            "2024-01-07 20:21:24 | INFO | train | epoch 201 | loss 1.14 | nll_loss 0.295 | ppl 1.23 | wps 949.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1005 | lr 0.000997509 | gnorm 0.525 | clip 20 | train_wall 17 | wall 3682\n",
            "2024-01-07 20:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 202:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:21:24 | INFO | fairseq.trainer | begin training epoch 202\n",
            "2024-01-07 20:21:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 202:  80% 4/5 [00:15<00:03,  3.99s/it]2024-01-07 20:21:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 202 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 202 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:21:41 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 1.14 | nll_loss 0.183 | ppl 1.14 | wps 2225.3 | wpb 1927.5 | bsz 150 | num_updates 1010 | best_loss 1.14\n",
            "2024-01-07 20:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 1010 updates\n",
            "2024-01-07 20:21:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 202 @ 1010 updates, score 1.14) (writing took 0.3913910589999432 seconds)\n",
            "2024-01-07 20:21:42 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)\n",
            "2024-01-07 20:21:42 | INFO | train | epoch 202 | loss 1.134 | nll_loss 0.29 | ppl 1.22 | wps 962.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1010 | lr 0.000995037 | gnorm 0.485 | clip 0 | train_wall 16 | wall 3700\n",
            "2024-01-07 20:21:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 203:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:21:42 | INFO | fairseq.trainer | begin training epoch 203\n",
            "2024-01-07 20:21:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 203:  80% 4/5 [00:12<00:02,  3.00s/it]2024-01-07 20:21:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 203 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 203 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:22:00 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 1.144 | nll_loss 0.189 | ppl 1.14 | wps 2738.2 | wpb 1927.5 | bsz 150 | num_updates 1015 | best_loss 1.14\n",
            "2024-01-07 20:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 1015 updates\n",
            "2024-01-07 20:22:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 203 @ 1015 updates, score 1.144) (writing took 0.15466100800040294 seconds)\n",
            "2024-01-07 20:22:00 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)\n",
            "2024-01-07 20:22:00 | INFO | train | epoch 203 | loss 1.133 | nll_loss 0.294 | ppl 1.23 | wps 954.5 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1015 | lr 0.000992583 | gnorm 0.534 | clip 20 | train_wall 16 | wall 3718\n",
            "2024-01-07 20:22:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 204:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:22:00 | INFO | fairseq.trainer | begin training epoch 204\n",
            "2024-01-07 20:22:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 204:  80% 4/5 [00:11<00:02,  2.78s/it]2024-01-07 20:22:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 204 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 204 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:22:18 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 1.162 | nll_loss 0.194 | ppl 1.14 | wps 2085.5 | wpb 1927.5 | bsz 150 | num_updates 1020 | best_loss 1.14\n",
            "2024-01-07 20:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 1020 updates\n",
            "2024-01-07 20:22:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 204 @ 1020 updates, score 1.162) (writing took 0.31246902500015494 seconds)\n",
            "2024-01-07 20:22:18 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)\n",
            "2024-01-07 20:22:18 | INFO | train | epoch 204 | loss 1.136 | nll_loss 0.29 | ppl 1.22 | wps 948.2 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1020 | lr 0.000990148 | gnorm 0.572 | clip 20 | train_wall 16 | wall 3737\n",
            "2024-01-07 20:22:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 205:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:22:18 | INFO | fairseq.trainer | begin training epoch 205\n",
            "2024-01-07 20:22:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 205:  80% 4/5 [00:11<00:03,  3.10s/it]2024-01-07 20:22:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 205 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 205 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:22:36 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 1.152 | nll_loss 0.192 | ppl 1.14 | wps 2905.8 | wpb 1927.5 | bsz 150 | num_updates 1025 | best_loss 1.14\n",
            "2024-01-07 20:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 1025 updates\n",
            "2024-01-07 20:22:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 205 @ 1025 updates, score 1.152) (writing took 0.16576126700010718 seconds)\n",
            "2024-01-07 20:22:36 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)\n",
            "2024-01-07 20:22:36 | INFO | train | epoch 205 | loss 1.139 | nll_loss 0.29 | ppl 1.22 | wps 972.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1025 | lr 0.00098773 | gnorm 0.473 | clip 0 | train_wall 16 | wall 3754\n",
            "2024-01-07 20:22:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 206:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:22:36 | INFO | fairseq.trainer | begin training epoch 206\n",
            "2024-01-07 20:22:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 206:  80% 4/5 [00:11<00:03,  3.22s/it]2024-01-07 20:22:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 206 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 206 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.71s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:22:54 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 1.162 | nll_loss 0.192 | ppl 1.14 | wps 2668.2 | wpb 1927.5 | bsz 150 | num_updates 1030 | best_loss 1.14\n",
            "2024-01-07 20:22:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 1030 updates\n",
            "2024-01-07 20:22:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:22:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 206 @ 1030 updates, score 1.162) (writing took 0.15656724700011182 seconds)\n",
            "2024-01-07 20:22:54 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)\n",
            "2024-01-07 20:22:54 | INFO | train | epoch 206 | loss 1.137 | nll_loss 0.291 | ppl 1.22 | wps 952 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1030 | lr 0.000985329 | gnorm 0.477 | clip 20 | train_wall 16 | wall 3773\n",
            "2024-01-07 20:22:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 207:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:22:54 | INFO | fairseq.trainer | begin training epoch 207\n",
            "2024-01-07 20:22:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 207:  80% 4/5 [00:16<00:03,  3.98s/it]2024-01-07 20:23:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 207 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 207 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:23:12 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 1.152 | nll_loss 0.179 | ppl 1.13 | wps 2888.6 | wpb 1927.5 | bsz 150 | num_updates 1035 | best_loss 1.14\n",
            "2024-01-07 20:23:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 1035 updates\n",
            "2024-01-07 20:23:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 207 @ 1035 updates, score 1.152) (writing took 0.15549517799991008 seconds)\n",
            "2024-01-07 20:23:12 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)\n",
            "2024-01-07 20:23:12 | INFO | train | epoch 207 | loss 1.141 | nll_loss 0.285 | ppl 1.22 | wps 969.2 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1035 | lr 0.000982946 | gnorm 0.568 | clip 20 | train_wall 16 | wall 3791\n",
            "2024-01-07 20:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 208:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:23:12 | INFO | fairseq.trainer | begin training epoch 208\n",
            "2024-01-07 20:23:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 208:  80% 4/5 [00:12<00:03,  3.58s/it]2024-01-07 20:23:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 208 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 208 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:23:30 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 1.146 | nll_loss 0.186 | ppl 1.14 | wps 2909.6 | wpb 1927.5 | bsz 150 | num_updates 1040 | best_loss 1.14\n",
            "2024-01-07 20:23:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 1040 updates\n",
            "2024-01-07 20:23:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 208 @ 1040 updates, score 1.146) (writing took 0.17094611700031237 seconds)\n",
            "2024-01-07 20:23:30 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)\n",
            "2024-01-07 20:23:30 | INFO | train | epoch 208 | loss 1.15 | nll_loss 0.307 | ppl 1.24 | wps 954 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1040 | lr 0.000980581 | gnorm 0.519 | clip 0 | train_wall 16 | wall 3809\n",
            "2024-01-07 20:23:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 209:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:23:30 | INFO | fairseq.trainer | begin training epoch 209\n",
            "2024-01-07 20:23:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 209:  80% 4/5 [00:11<00:03,  3.28s/it]2024-01-07 20:23:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 209 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 209 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:23:48 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 1.154 | nll_loss 0.192 | ppl 1.14 | wps 2834.4 | wpb 1927.5 | bsz 150 | num_updates 1045 | best_loss 1.14\n",
            "2024-01-07 20:23:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 1045 updates\n",
            "2024-01-07 20:23:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:23:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 209 @ 1045 updates, score 1.154) (writing took 0.18161744500002897 seconds)\n",
            "2024-01-07 20:23:48 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)\n",
            "2024-01-07 20:23:48 | INFO | train | epoch 209 | loss 1.148 | nll_loss 0.309 | ppl 1.24 | wps 969.5 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1045 | lr 0.000978232 | gnorm 0.75 | clip 20 | train_wall 16 | wall 3827\n",
            "2024-01-07 20:23:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 210:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:23:48 | INFO | fairseq.trainer | begin training epoch 210\n",
            "2024-01-07 20:23:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 210:  80% 4/5 [00:16<00:04,  4.18s/it]2024-01-07 20:24:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 210 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 210 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:24:06 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 1.151 | nll_loss 0.185 | ppl 1.14 | wps 2773.5 | wpb 1927.5 | bsz 150 | num_updates 1050 | best_loss 1.14\n",
            "2024-01-07 20:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 1050 updates\n",
            "2024-01-07 20:24:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 210 @ 1050 updates, score 1.151) (writing took 0.16270808699982808 seconds)\n",
            "2024-01-07 20:24:07 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)\n",
            "2024-01-07 20:24:07 | INFO | train | epoch 210 | loss 1.131 | nll_loss 0.282 | ppl 1.22 | wps 948.7 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1050 | lr 0.0009759 | gnorm 0.43 | clip 0 | train_wall 17 | wall 3845\n",
            "2024-01-07 20:24:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 211:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:24:07 | INFO | fairseq.trainer | begin training epoch 211\n",
            "2024-01-07 20:24:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 211:  80% 4/5 [00:12<00:03,  3.49s/it]2024-01-07 20:24:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 211 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 211 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:24:24 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 1.146 | nll_loss 0.184 | ppl 1.14 | wps 2877.6 | wpb 1927.5 | bsz 150 | num_updates 1055 | best_loss 1.14\n",
            "2024-01-07 20:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 1055 updates\n",
            "2024-01-07 20:24:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 211 @ 1055 updates, score 1.146) (writing took 0.1572099570003047 seconds)\n",
            "2024-01-07 20:24:24 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)\n",
            "2024-01-07 20:24:24 | INFO | train | epoch 211 | loss 1.137 | nll_loss 0.294 | ppl 1.23 | wps 967.3 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1055 | lr 0.000973585 | gnorm 0.387 | clip 0 | train_wall 16 | wall 3863\n",
            "2024-01-07 20:24:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 212:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:24:25 | INFO | fairseq.trainer | begin training epoch 212\n",
            "2024-01-07 20:24:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 212:  80% 4/5 [00:13<00:03,  3.02s/it]2024-01-07 20:24:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 212 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 212 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:24:43 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 1.145 | nll_loss 0.18 | ppl 1.13 | wps 2803.8 | wpb 1927.5 | bsz 150 | num_updates 1060 | best_loss 1.14\n",
            "2024-01-07 20:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 1060 updates\n",
            "2024-01-07 20:24:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 212 @ 1060 updates, score 1.145) (writing took 0.17428323500007536 seconds)\n",
            "2024-01-07 20:24:43 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)\n",
            "2024-01-07 20:24:43 | INFO | train | epoch 212 | loss 1.124 | nll_loss 0.272 | ppl 1.21 | wps 947.3 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1060 | lr 0.000971286 | gnorm 0.35 | clip 0 | train_wall 17 | wall 3881\n",
            "2024-01-07 20:24:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 213:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:24:43 | INFO | fairseq.trainer | begin training epoch 213\n",
            "2024-01-07 20:24:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 213:  80% 4/5 [00:12<00:02,  2.55s/it]2024-01-07 20:24:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 213 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 213 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:25:01 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 1.144 | nll_loss 0.192 | ppl 1.14 | wps 2854.3 | wpb 1927.5 | bsz 150 | num_updates 1065 | best_loss 1.14\n",
            "2024-01-07 20:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 1065 updates\n",
            "2024-01-07 20:25:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 213 @ 1065 updates, score 1.144) (writing took 0.17713132500011852 seconds)\n",
            "2024-01-07 20:25:01 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)\n",
            "2024-01-07 20:25:01 | INFO | train | epoch 213 | loss 1.118 | nll_loss 0.266 | ppl 1.2 | wps 970.8 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1065 | lr 0.000969003 | gnorm 0.508 | clip 20 | train_wall 16 | wall 3899\n",
            "2024-01-07 20:25:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 214:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:25:01 | INFO | fairseq.trainer | begin training epoch 214\n",
            "2024-01-07 20:25:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 214:  80% 4/5 [00:11<00:02,  2.82s/it]2024-01-07 20:25:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 214 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 214 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:25:19 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 1.135 | nll_loss 0.184 | ppl 1.14 | wps 2844.5 | wpb 1927.5 | bsz 150 | num_updates 1070 | best_loss 1.135\n",
            "2024-01-07 20:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 1070 updates\n",
            "2024-01-07 20:25:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-07 20:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 214 @ 1070 updates, score 1.135) (writing took 0.4064363979996415 seconds)\n",
            "2024-01-07 20:25:19 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)\n",
            "2024-01-07 20:25:19 | INFO | train | epoch 214 | loss 1.123 | nll_loss 0.286 | ppl 1.22 | wps 930.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1070 | lr 0.000966736 | gnorm 0.53 | clip 20 | train_wall 17 | wall 3918\n",
            "2024-01-07 20:25:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 215:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:25:19 | INFO | fairseq.trainer | begin training epoch 215\n",
            "2024-01-07 20:25:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 215:  80% 4/5 [00:12<00:03,  3.66s/it]2024-01-07 20:25:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 215 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 215 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.71s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:25:37 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 1.137 | nll_loss 0.182 | ppl 1.13 | wps 1866.4 | wpb 1927.5 | bsz 150 | num_updates 1075 | best_loss 1.135\n",
            "2024-01-07 20:25:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 1075 updates\n",
            "2024-01-07 20:25:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 215 @ 1075 updates, score 1.137) (writing took 0.2859676070002024 seconds)\n",
            "2024-01-07 20:25:38 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)\n",
            "2024-01-07 20:25:38 | INFO | train | epoch 215 | loss 1.121 | nll_loss 0.275 | ppl 1.21 | wps 950.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1075 | lr 0.000964486 | gnorm 0.372 | clip 0 | train_wall 16 | wall 3936\n",
            "2024-01-07 20:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 216:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:25:38 | INFO | fairseq.trainer | begin training epoch 216\n",
            "2024-01-07 20:25:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 216:  80% 4/5 [00:11<00:03,  3.39s/it]2024-01-07 20:25:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 216 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 216 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:25:56 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 1.144 | nll_loss 0.191 | ppl 1.14 | wps 2637.7 | wpb 1927.5 | bsz 150 | num_updates 1080 | best_loss 1.135\n",
            "2024-01-07 20:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 1080 updates\n",
            "2024-01-07 20:25:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 216 @ 1080 updates, score 1.144) (writing took 0.20602162399973167 seconds)\n",
            "2024-01-07 20:25:56 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)\n",
            "2024-01-07 20:25:56 | INFO | train | epoch 216 | loss 1.117 | nll_loss 0.272 | ppl 1.21 | wps 951.8 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1080 | lr 0.00096225 | gnorm 0.626 | clip 20 | train_wall 16 | wall 3954\n",
            "2024-01-07 20:25:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 217:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:25:56 | INFO | fairseq.trainer | begin training epoch 217\n",
            "2024-01-07 20:25:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 217:  80% 4/5 [00:12<00:03,  3.34s/it]2024-01-07 20:26:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 217 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 217 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.84s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:26:14 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 1.145 | nll_loss 0.193 | ppl 1.14 | wps 2676.3 | wpb 1927.5 | bsz 150 | num_updates 1085 | best_loss 1.135\n",
            "2024-01-07 20:26:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 1085 updates\n",
            "2024-01-07 20:26:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 217 @ 1085 updates, score 1.145) (writing took 0.19071919500038348 seconds)\n",
            "2024-01-07 20:26:14 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)\n",
            "2024-01-07 20:26:14 | INFO | train | epoch 217 | loss 1.121 | nll_loss 0.279 | ppl 1.21 | wps 952.9 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1085 | lr 0.000960031 | gnorm 0.539 | clip 20 | train_wall 16 | wall 3972\n",
            "2024-01-07 20:26:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 218:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:26:14 | INFO | fairseq.trainer | begin training epoch 218\n",
            "2024-01-07 20:26:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 218:  80% 4/5 [00:12<00:03,  3.02s/it]2024-01-07 20:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 218 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 218 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:26:32 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 1.145 | nll_loss 0.188 | ppl 1.14 | wps 2701.8 | wpb 1927.5 | bsz 150 | num_updates 1090 | best_loss 1.135\n",
            "2024-01-07 20:26:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 1090 updates\n",
            "2024-01-07 20:26:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 218 @ 1090 updates, score 1.145) (writing took 0.17456728600063798 seconds)\n",
            "2024-01-07 20:26:32 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)\n",
            "2024-01-07 20:26:32 | INFO | train | epoch 218 | loss 1.113 | nll_loss 0.269 | ppl 1.2 | wps 964.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1090 | lr 0.000957826 | gnorm 0.589 | clip 20 | train_wall 16 | wall 3990\n",
            "2024-01-07 20:26:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 219:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:26:32 | INFO | fairseq.trainer | begin training epoch 219\n",
            "2024-01-07 20:26:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 219:  80% 4/5 [00:12<00:03,  3.03s/it]2024-01-07 20:26:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 219 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 219 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.57s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:26:50 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 1.138 | nll_loss 0.179 | ppl 1.13 | wps 2627.3 | wpb 1927.5 | bsz 150 | num_updates 1095 | best_loss 1.135\n",
            "2024-01-07 20:26:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 1095 updates\n",
            "2024-01-07 20:26:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:26:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 219 @ 1095 updates, score 1.138) (writing took 0.180018734999976 seconds)\n",
            "2024-01-07 20:26:50 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)\n",
            "2024-01-07 20:26:50 | INFO | train | epoch 219 | loss 1.121 | nll_loss 0.274 | ppl 1.21 | wps 950 | ups 0.27 | wpb 3472.2 | bsz 279.8 | num_updates 1095 | lr 0.000955637 | gnorm 0.378 | clip 0 | train_wall 16 | wall 4009\n",
            "2024-01-07 20:26:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 220:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:26:50 | INFO | fairseq.trainer | begin training epoch 220\n",
            "2024-01-07 20:26:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 220:  80% 4/5 [00:16<00:04,  4.10s/it]2024-01-07 20:27:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 220 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 220 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-07 20:27:08 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 1.145 | nll_loss 0.189 | ppl 1.14 | wps 2466.1 | wpb 1927.5 | bsz 150 | num_updates 1100 | best_loss 1.135\n",
            "2024-01-07 20:27:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 1100 updates\n",
            "2024-01-07 20:27:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-07 20:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 220 @ 1100 updates, score 1.145) (writing took 0.18094872600067902 seconds)\n",
            "2024-01-07 20:27:08 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)\n",
            "2024-01-07 20:27:08 | INFO | train | epoch 220 | loss 1.113 | nll_loss 0.264 | ppl 1.2 | wps 967.7 | ups 0.28 | wpb 3472.2 | bsz 279.8 | num_updates 1100 | lr 0.000953463 | gnorm 0.357 | clip 0 | train_wall 16 | wall 4027\n",
            "2024-01-07 20:27:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5\n",
            "epoch 221:   0% 0/5 [00:00<?, ?it/s]2024-01-07 20:27:08 | INFO | fairseq.trainer | begin training epoch 221\n",
            "2024-01-07 20:27:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 824, in train_step\n",
            "    loss, sample_size_i, logging_output = self.task.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 519, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate predictions on test data\n",
        "\n",
        "Generate predictions on test data - read in all the inputs from tst.esp.input and generate outputs to the file tst.esp.output (this is slow and takes about a minute)"
      ],
      "metadata": {
        "id": "C70_-Z3hGSXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoints/dan-models/checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxe16BNGRzh",
        "outputId": "4f4d4bbb-1b02-448d-b9aa-104e98a27b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 22:54:42.220435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 22:54:42.220512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 22:54:42.222690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 22:54:42.232371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 22:54:44.180374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-06 22:54:48 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/dan-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-06 22:54:48 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 48 types\n",
            "2024-01-06 22:54:48 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-06 22:54:48 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 22:54:49 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-06 22:54:49 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-06 22:55:03 | INFO | fairseq_cli.interactive | Total time: 14.336 seconds; translation time: 13.288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the generated outputs and inputs and display the first 20 side-by-side\n",
        "linesinput = [l.strip() for l in open(\"tst.dan.input\")]\n",
        "linesoutput = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "tuple(zip(linesinput, linesoutput))[:20] # Look at 20 first test inputs and predicted outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vGn473BGQUU",
        "outputId": "5fa6a6a3-7a3a-4700-dcc4-aa317b8cf3a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('< k r a v e b e n > N INDF NOM SG', '< k r a v e b e n >'),\n",
              " ('< e k s t r a h e r e > V.PTCP ACT PRS', '< e k s t r a h e r e n d e >'),\n",
              " ('< p r o v i n s > N INDF NOM PL', '< p r o v i n s e r >'),\n",
              " ('< a r g u m e n t e r e > V ACT IND PST', '< a r g u m e n t e r e d e >'),\n",
              " ('< t i d > DEF N NOM SG', '< t i d e n s >'),\n",
              " ('< t r o n e > N DEF NOM SG', '< t r o n e n s >'),\n",
              " ('< r o r > N INDF NOM PL', '< r o r >'),\n",
              " ('< v i k i n g > INDF N NOM PL', '< v i k i n g e r >'),\n",
              " ('< p æ d e r a s t > INDF N NOM SG', '< p æ d e r a s t >'),\n",
              " ('< k v o t i e n t > DEF N NOM PL', '< k v o t i e n t e r n e s >'),\n",
              " ('< k y n i s m e > DEF N NOM SG', '< k y n i s m e n s >'),\n",
              " ('< c i g a r e t s æ l g e r > GEN INDF N SG',\n",
              "  '< c i g a r e t s æ l g e r s >'),\n",
              " ('< m e t r o p o l > INDF N NOM SG', '< m e t r o p o l >'),\n",
              " ('< k a m m e r > DEF N NOM PL', '< k a m r e n e s >'),\n",
              " ('< k r a v e b e n > INDF N NOM SG', '< k r a v e b e n >'),\n",
              " ('< v æ r d i m æ n g d e > N INDF NOM SG', '< v æ r d i m æ n g d e >'),\n",
              " ('< r y g s ø j l e > INDF N NOM SG', '< r y g s ø j l e >'),\n",
              " ('< d a t a m a t > INDF N NOM SG', '< d a t a m a t >'),\n",
              " ('< s u d a n e r > N INDF GEN SG', '< s u d a n e r s >'),\n",
              " ('< j u n g l e > GEN INDF N SG', '< j u n g l e s >'))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate test accuracy and Levensthein distance"
      ],
      "metadata": {
        "id": "fmRuBQm-7CgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the predictions from the checkpoint\n",
        "!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t9HAmFd7CPJ",
        "outputId": "3401c3c7-29fc-4ac1-8263-5353e0b39da3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-07 20:27:39.465058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-07 20:27:39.465114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-07 20:27:39.466064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-07 20:27:39.471468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-07 20:27:40.378330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-07 20:27:44 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-07 20:27:44 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 56 types\n",
            "2024-01-07 20:27:44 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-07 20:27:44 | INFO | fairseq_cli.interactive | loading model(s) from checkpoint_best.pt\n",
            "2024-01-07 20:27:44 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-07 20:27:44 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-07 20:28:14 | INFO | fairseq_cli.interactive | Total time: 30.256 seconds; translation time: 28.745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy between the ground truth and the predictions\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "hits = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "  if pred == ground:\n",
        "    hits += 1\n",
        "  lines += 1\n",
        "\n",
        "print(\"Accuracy: \" + str(hits/lines))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqHpNu-S7gIJ",
        "outputId": "5f1b6c34-0c0b-40f3-c34e-e4bb73358557"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7266666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Levenshtein distance between the ground truth and the predictions\n",
        "\n",
        "# Function from ChatGPT\n",
        "def levenshtein_distance(str1, str2):\n",
        "    len_str1 = len(str1) + 1\n",
        "    len_str2 = len(str2) + 1\n",
        "\n",
        "    # Create a matrix to store the distances\n",
        "    matrix = [[0 for _ in range(len_str2)] for _ in range(len_str1)]\n",
        "\n",
        "    # Initialize the matrix\n",
        "    for i in range(len_str1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len_str2):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    # Fill in the matrix\n",
        "    for i in range(1, len_str1):\n",
        "        for j in range(1, len_str2):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i - 1][j] + 1,        # Deletion\n",
        "                matrix[i][j - 1] + 1,        # Insertion\n",
        "                matrix[i - 1][j - 1] + cost  # Substitution\n",
        "            )\n",
        "\n",
        "    # The bottom-right cell contains the Levenshtein distance\n",
        "    return matrix[-1][-1]\n",
        "\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "distances = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "\n",
        "  distances += levenshtein_distance(pred, ground)\n",
        "  lines += 1\n",
        "\n",
        "print(\"Levenshtein distance: \" + str(distances/lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOwSI5Mw702e",
        "outputId": "03f93c5a-60c5-40b3-85cb-bb5f3db010be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levenshtein distance: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance = 0: The strings are identical. No edits are needed.\n",
        "\n",
        "Distance = 1: The strings are very similar. Typically, this means either a single insertion, deletion, or substitution is required to make them identical. For example, \"cat\" and \"cot\" have a Levenshtein distance of 1 because you can transform one into the other by changing a single character.\n",
        "\n",
        "Distance > 1: As the distance increases, the dissimilarity between the strings also increases. A distance of 2 or more indicates a greater degree of dissimilarity, involving multiple edits."
      ],
      "metadata": {
        "id": "Aeqp6-_YDHuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/checkpoints.zip /content/checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_5ZluciL2Fm",
        "outputId": "95202da7-a63a-4a86-96ee-8038bc475689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/checkpoints/ (stored 0%)\n",
            "  adding: content/checkpoints/dan-models/ (stored 0%)\n",
            "  adding: content/checkpoints/dan-models/checkpoint_best.pt (deflated 20%)\n",
            "  adding: content/checkpoints/dan-models/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/checkpoints/dan-models/checkpoint_last.pt (deflated 20%)\n"
          ]
        }
      ]
    }
  ]
}