{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training an inflectional system\n",
        "\n",
        "## Inputs & Installs"
      ],
      "metadata": {
        "id": "AnmSkrVyFnc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L44JJRSjdF8S",
        "outputId": "52dfddc4-1033-432d-9260-a0892ca457c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login aa0b9ecff47af231f410704977e504d7928ffb05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAAylMkWdHSj",
        "outputId": "747e1928-e863-4c42-959a-42388beda8cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"danish-inflection-beta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "1W8fAQJEdaFW",
        "outputId": "e127376b-a511-429f-b749-74b1bed487d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mignacioct_\u001b[0m (\u001b[33mignacio_at_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240106_230236-lpmsczfn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ignacio_at_ai/danish-inflection-beta/runs/lpmsczfn' target=\"_blank\">generous-disco-2</a></strong> to <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-beta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-beta' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection-beta</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection-beta/runs/lpmsczfn' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection-beta/runs/lpmsczfn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ignacio_at_ai/danish-inflection-beta/runs/lpmsczfn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc4cb3ebac0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBSGWunsEwRs",
        "outputId": "e395025c-bebb-43c3-bf87-1ac33df2d2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.7)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OafxnvaWYdsS",
        "outputId": "063de330-d64e-4a39-d00e-115bf7747e92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data"
      ],
      "metadata": {
        "id": "OR7TeZKoF_Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh dan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0d-GDOTGBlD",
        "outputId": "4b6a5614-eba9-4f55-dd74-c717c6fee2ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 23:02:52.734264: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 23:02:52.734314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 23:02:52.735307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 23:02:52.740641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 23:02:53.789517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='dan.input', target_lang='dan.output', trainpref='train', validpref='dev', testpref=None, align_suffix=None, destdir='data-bin/dan', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 48 types\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.input] train.dan.input: 700 sents, 10187 tokens, 0.187% replaced (by <unk>)\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 48 types\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.input] dev.dan.input: 150 sents, 2167 tokens, 0.277% replaced (by <unk>)\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.output] train.dan.output: 700 sents, 8785 tokens, 0.159% replaced (by <unk>)\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | [dan.output] dev.dan.output: 150 sents, 1842 tokens, 0.163% replaced (by <unk>)\n",
            "2024-01-06 23:02:57 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/dan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Train with default parameters, roughly the baseline in SIGMORPHON 2020 shared task\n",
        "Let this run until the loss on the validation (dev) test no longer improves. (Maybe 10 minutes with a GPU)."
      ],
      "metadata": {
        "id": "6i4VwjnlGHG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./train.sh dan --patience 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1iVwFdGIPz",
        "outputId": "d3437a21-628f-4bdc-a092-93893b2fed32"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 23:03:00.919268: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 23:03:00.919326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 23:03:00.920239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 23:03:00.925497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 23:03:02.461805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-06 23:03:04 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/dan-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/dan-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/dan', source_lang='dan.input', target_lang='dan.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/dan', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-06 23:03:07 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 48 types\n",
            "2024-01-06 23:03:07 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(48, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | num. shared model params: 10,551,296 (num. trained: 10,551,296)\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-06 23:03:07 | INFO | fairseq.data.data_utils | loaded 150 examples from: data-bin/dan/valid.dan.input-dan.output.dan.input\n",
            "2024-01-06 23:03:07 | INFO | fairseq.data.data_utils | loaded 150 examples from: data-bin/dan/valid.dan.input-dan.output.dan.output\n",
            "2024-01-06 23:03:07 | INFO | fairseq.tasks.translation | data-bin/dan valid dan.input-dan.output 150 examples\n",
            "2024-01-06 23:03:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-06 23:03:07 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-06 23:03:07 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:03:07 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:03:07 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-06 23:03:07 | INFO | fairseq.data.data_utils | loaded 700 examples from: data-bin/dan/train.dan.input-dan.output.dan.input\n",
            "2024-01-06 23:03:07 | INFO | fairseq.data.data_utils | loaded 700 examples from: data-bin/dan/train.dan.input-dan.output.dan.output\n",
            "2024-01-06 23:03:07 | INFO | fairseq.tasks.translation | data-bin/dan train dan.input-dan.output 700 examples\n",
            "2024-01-06 23:03:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 001:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:03:08 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-06 23:03:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:  67% 2/3 [00:05<00:03,  3.47s/it]2024-01-06 23:03:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.49it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:03:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.43 | nll_loss 11.45 | ppl 2796.83 | wps 2605.9 | wpb 921 | bsz 75 | num_updates 3\n",
            "2024-01-06 23:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3 updates\n",
            "2024-01-06 23:03:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 1 @ 3 updates, score 11.43) (writing took 0.2542233199999373 seconds)\n",
            "2024-01-06 23:03:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-06 23:03:19 | INFO | train | epoch 001 | loss 10.145 | nll_loss 10.15 | ppl 1136.56 | wps 796.6 | ups 0.18 | wpb 2928.3 | bsz 233.3 | num_updates 3 | lr 3e-06 | gnorm 7.604 | clip 100 | train_wall 10 | wall 11\n",
            "2024-01-06 23:03:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 002:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:03:19 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-06 23:03:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  67% 2/3 [00:10<00:05,  5.24s/it]2024-01-06 23:03:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:03:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.139 | nll_loss 11.155 | ppl 2279.82 | wps 2871 | wpb 921 | bsz 75 | num_updates 6 | best_loss 11.139\n",
            "2024-01-06 23:03:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 6 updates\n",
            "2024-01-06 23:03:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 2 @ 6 updates, score 11.139) (writing took 0.378053791000184 seconds)\n",
            "2024-01-06 23:03:31 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-06 23:03:31 | INFO | train | epoch 002 | loss 10.097 | nll_loss 10.102 | ppl 1098.65 | wps 722.1 | ups 0.25 | wpb 2928.3 | bsz 233.3 | num_updates 6 | lr 6e-06 | gnorm 7.518 | clip 100 | train_wall 11 | wall 24\n",
            "2024-01-06 23:03:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 003:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:03:31 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-06 23:03:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:03:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:03:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.66 | nll_loss 10.671 | ppl 1630.66 | wps 2713.6 | wpb 921 | bsz 75 | num_updates 9 | best_loss 10.66\n",
            "2024-01-06 23:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 9 updates\n",
            "2024-01-06 23:03:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 3 @ 9 updates, score 10.66) (writing took 0.39955800999996427 seconds)\n",
            "2024-01-06 23:03:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-06 23:03:42 | INFO | train | epoch 003 | loss 9.963 | nll_loss 9.967 | ppl 1000.64 | wps 805.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 9 | lr 9e-06 | gnorm 7.529 | clip 100 | train_wall 10 | wall 35\n",
            "2024-01-06 23:03:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 004:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:03:42 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-06 23:03:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  67% 2/3 [00:13<00:07,  7.11s/it]2024-01-06 23:03:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:03:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.041 | nll_loss 10.046 | ppl 1057.36 | wps 2820.2 | wpb 921 | bsz 75 | num_updates 12 | best_loss 10.041\n",
            "2024-01-06 23:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 12 updates\n",
            "2024-01-06 23:03:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:03:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 4 @ 12 updates, score 10.041) (writing took 0.6041906240000117 seconds)\n",
            "2024-01-06 23:03:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-06 23:03:57 | INFO | train | epoch 004 | loss 9.699 | nll_loss 9.7 | ppl 831.92 | wps 593.8 | ups 0.2 | wpb 2928.3 | bsz 233.3 | num_updates 12 | lr 1.2e-05 | gnorm 7.375 | clip 100 | train_wall 13 | wall 49\n",
            "2024-01-06 23:03:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 005:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:03:57 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-06 23:03:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  67% 2/3 [00:09<00:04,  4.99s/it]2024-01-06 23:04:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:04:07 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.347 | nll_loss 9.345 | ppl 650.39 | wps 2882.1 | wpb 921 | bsz 75 | num_updates 15 | best_loss 9.347\n",
            "2024-01-06 23:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 15 updates\n",
            "2024-01-06 23:04:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 5 @ 15 updates, score 9.347) (writing took 0.3800524639998457 seconds)\n",
            "2024-01-06 23:04:08 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-06 23:04:08 | INFO | train | epoch 005 | loss 9.344 | nll_loss 9.34 | ppl 648.12 | wps 798.7 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 15 | lr 1.5e-05 | gnorm 7.161 | clip 100 | train_wall 10 | wall 60\n",
            "2024-01-06 23:04:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 006:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:04:08 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-06 23:04:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  67% 2/3 [00:05<00:03,  3.02s/it]2024-01-06 23:04:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:04:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.659 | nll_loss 8.65 | ppl 401.72 | wps 2880.4 | wpb 921 | bsz 75 | num_updates 18 | best_loss 8.659\n",
            "2024-01-06 23:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 18 updates\n",
            "2024-01-06 23:04:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 6 @ 18 updates, score 8.659) (writing took 0.4357446419999178 seconds)\n",
            "2024-01-06 23:04:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-06 23:04:19 | INFO | train | epoch 006 | loss 8.806 | nll_loss 8.797 | ppl 444.93 | wps 811.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 18 | lr 1.8e-05 | gnorm 6.629 | clip 100 | train_wall 10 | wall 71\n",
            "2024-01-06 23:04:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 007:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:04:19 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-01-06 23:04:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  67% 2/3 [00:05<00:03,  3.04s/it]2024-01-06 23:04:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:04:29 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.016 | nll_loss 8 | ppl 256.01 | wps 2966.3 | wpb 921 | bsz 75 | num_updates 21 | best_loss 8.016\n",
            "2024-01-06 23:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 21 updates\n",
            "2024-01-06 23:04:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 7 @ 21 updates, score 8.016) (writing took 0.772241638999958 seconds)\n",
            "2024-01-06 23:04:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-01-06 23:04:30 | INFO | train | epoch 007 | loss 8.312 | nll_loss 8.296 | ppl 314.33 | wps 780.2 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 21 | lr 2.1e-05 | gnorm 6.005 | clip 100 | train_wall 10 | wall 82\n",
            "2024-01-06 23:04:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 008:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:04:30 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-01-06 23:04:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  67% 2/3 [00:09<00:04,  4.69s/it]2024-01-06 23:04:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:04:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.445 | nll_loss 7.42 | ppl 171.26 | wps 2731.6 | wpb 921 | bsz 75 | num_updates 24 | best_loss 7.445\n",
            "2024-01-06 23:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 24 updates\n",
            "2024-01-06 23:04:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 8 @ 24 updates, score 7.445) (writing took 0.3143189579998307 seconds)\n",
            "2024-01-06 23:04:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-01-06 23:04:40 | INFO | train | epoch 008 | loss 8.002 | nll_loss 7.984 | ppl 253.13 | wps 826.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 24 | lr 2.4e-05 | gnorm 5.157 | clip 100 | train_wall 10 | wall 93\n",
            "2024-01-06 23:04:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 009:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:04:40 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-01-06 23:04:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  67% 2/3 [00:04<00:01,  1.80s/it]2024-01-06 23:04:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.61it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:04:51 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.963 | nll_loss 6.928 | ppl 121.79 | wps 2824.1 | wpb 921 | bsz 75 | num_updates 27 | best_loss 6.963\n",
            "2024-01-06 23:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 27 updates\n",
            "2024-01-06 23:04:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 9 @ 27 updates, score 6.963) (writing took 0.40997746300013205 seconds)\n",
            "2024-01-06 23:04:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-01-06 23:04:51 | INFO | train | epoch 009 | loss 7.446 | nll_loss 7.419 | ppl 171.12 | wps 819.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 27 | lr 2.7e-05 | gnorm 4.465 | clip 100 | train_wall 10 | wall 104\n",
            "2024-01-06 23:04:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 010:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:04:51 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-01-06 23:04:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  67% 2/3 [00:04<00:01,  1.76s/it]2024-01-06 23:05:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:01 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.548 | nll_loss 6.502 | ppl 90.66 | wps 2069.1 | wpb 921 | bsz 75 | num_updates 30 | best_loss 6.548\n",
            "2024-01-06 23:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 30 updates\n",
            "2024-01-06 23:05:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 10 @ 30 updates, score 6.548) (writing took 0.47974554999996144 seconds)\n",
            "2024-01-06 23:05:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-01-06 23:05:02 | INFO | train | epoch 010 | loss 7.004 | nll_loss 6.967 | ppl 125.07 | wps 834.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 30 | lr 3e-05 | gnorm 3.836 | clip 100 | train_wall 9 | wall 114\n",
            "2024-01-06 23:05:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 011:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:02 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-01-06 23:05:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  67% 2/3 [00:09<00:04,  4.69s/it]2024-01-06 23:05:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.75it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:12 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.183 | nll_loss 6.126 | ppl 69.82 | wps 3016.8 | wpb 921 | bsz 75 | num_updates 33 | best_loss 6.183\n",
            "2024-01-06 23:05:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 33 updates\n",
            "2024-01-06 23:05:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 11 @ 33 updates, score 6.183) (writing took 0.33000225600017075 seconds)\n",
            "2024-01-06 23:05:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-01-06 23:05:12 | INFO | train | epoch 011 | loss 6.692 | nll_loss 6.648 | ppl 100.31 | wps 844.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 33 | lr 3.3e-05 | gnorm 3.314 | clip 100 | train_wall 9 | wall 125\n",
            "2024-01-06 23:05:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 012:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:12 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-01-06 23:05:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  67% 2/3 [00:09<00:04,  4.66s/it]2024-01-06 23:05:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:22 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.868 | nll_loss 5.797 | ppl 55.6 | wps 2791 | wpb 921 | bsz 75 | num_updates 36 | best_loss 5.868\n",
            "2024-01-06 23:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 36 updates\n",
            "2024-01-06 23:05:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 12 @ 36 updates, score 5.868) (writing took 0.3457604459999857 seconds)\n",
            "2024-01-06 23:05:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-01-06 23:05:23 | INFO | train | epoch 012 | loss 6.324 | nll_loss 6.266 | ppl 76.98 | wps 820.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 36 | lr 3.6e-05 | gnorm 2.878 | clip 100 | train_wall 10 | wall 135\n",
            "2024-01-06 23:05:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 013:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:23 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-01-06 23:05:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  67% 2/3 [00:05<00:02,  2.31s/it]2024-01-06 23:05:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.604 | nll_loss 5.519 | ppl 45.84 | wps 2840.9 | wpb 921 | bsz 75 | num_updates 39 | best_loss 5.604\n",
            "2024-01-06 23:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 39 updates\n",
            "2024-01-06 23:05:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 13 @ 39 updates, score 5.604) (writing took 0.3584579959999701 seconds)\n",
            "2024-01-06 23:05:33 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-01-06 23:05:33 | INFO | train | epoch 013 | loss 5.987 | nll_loss 5.916 | ppl 60.39 | wps 821.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 39 | lr 3.9e-05 | gnorm 2.496 | clip 100 | train_wall 10 | wall 146\n",
            "2024-01-06 23:05:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 014:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:33 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-01-06 23:05:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  67% 2/3 [00:05<00:03,  3.18s/it]2024-01-06 23:05:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:44 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.386 | nll_loss 5.286 | ppl 39.03 | wps 2680.3 | wpb 921 | bsz 75 | num_updates 42 | best_loss 5.386\n",
            "2024-01-06 23:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 42 updates\n",
            "2024-01-06 23:05:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 14 @ 42 updates, score 5.386) (writing took 0.3595979450001323 seconds)\n",
            "2024-01-06 23:05:44 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-01-06 23:05:44 | INFO | train | epoch 014 | loss 5.685 | nll_loss 5.6 | ppl 48.51 | wps 838.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 42 | lr 4.2e-05 | gnorm 2.158 | clip 100 | train_wall 9 | wall 157\n",
            "2024-01-06 23:05:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 015:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:44 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-01-06 23:05:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  67% 2/3 [00:04<00:02,  2.65s/it]2024-01-06 23:05:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:05:54 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.207 | nll_loss 5.094 | ppl 34.15 | wps 2803.7 | wpb 921 | bsz 75 | num_updates 45 | best_loss 5.207\n",
            "2024-01-06 23:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 45 updates\n",
            "2024-01-06 23:05:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 15 @ 45 updates, score 5.207) (writing took 0.3291527159999532 seconds)\n",
            "2024-01-06 23:05:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-01-06 23:05:54 | INFO | train | epoch 015 | loss 5.465 | nll_loss 5.365 | ppl 41.21 | wps 830.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 45 | lr 4.5e-05 | gnorm 1.89 | clip 100 | train_wall 10 | wall 167\n",
            "2024-01-06 23:05:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 016:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:05:55 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-01-06 23:05:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  67% 2/3 [00:04<00:01,  1.77s/it]2024-01-06 23:06:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:05 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.059 | nll_loss 4.932 | ppl 30.53 | wps 2845.1 | wpb 921 | bsz 75 | num_updates 48 | best_loss 5.059\n",
            "2024-01-06 23:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 48 updates\n",
            "2024-01-06 23:06:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 16 @ 48 updates, score 5.059) (writing took 0.3639792250000937 seconds)\n",
            "2024-01-06 23:06:05 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-01-06 23:06:05 | INFO | train | epoch 016 | loss 5.326 | nll_loss 5.216 | ppl 37.16 | wps 832.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 48 | lr 4.8e-05 | gnorm 1.641 | clip 100 | train_wall 9 | wall 178\n",
            "2024-01-06 23:06:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 017:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:05 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-01-06 23:06:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  67% 2/3 [00:09<00:04,  4.85s/it]2024-01-06 23:06:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:15 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.939 | nll_loss 4.797 | ppl 27.81 | wps 2864.2 | wpb 921 | bsz 75 | num_updates 51 | best_loss 4.939\n",
            "2024-01-06 23:06:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 51 updates\n",
            "2024-01-06 23:06:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 17 @ 51 updates, score 4.939) (writing took 0.31019856699981574 seconds)\n",
            "2024-01-06 23:06:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-01-06 23:06:16 | INFO | train | epoch 017 | loss 5.19 | nll_loss 5.069 | ppl 33.56 | wps 826.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 51 | lr 5.1e-05 | gnorm 1.455 | clip 100 | train_wall 10 | wall 188\n",
            "2024-01-06 23:06:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 018:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:16 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-01-06 23:06:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  67% 2/3 [00:04<00:02,  2.49s/it]2024-01-06 23:06:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:26 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.849 | nll_loss 4.695 | ppl 25.91 | wps 2836.3 | wpb 921 | bsz 75 | num_updates 54 | best_loss 4.849\n",
            "2024-01-06 23:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 54 updates\n",
            "2024-01-06 23:06:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 18 @ 54 updates, score 4.849) (writing took 0.40937114300004396 seconds)\n",
            "2024-01-06 23:06:26 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-01-06 23:06:26 | INFO | train | epoch 018 | loss 5.032 | nll_loss 4.893 | ppl 29.71 | wps 824.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 54 | lr 5.4e-05 | gnorm 1.281 | clip 66.7 | train_wall 10 | wall 199\n",
            "2024-01-06 23:06:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 019:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:26 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-01-06 23:06:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  67% 2/3 [00:09<00:04,  4.86s/it]2024-01-06 23:06:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.769 | nll_loss 4.604 | ppl 24.31 | wps 2838.7 | wpb 921 | bsz 75 | num_updates 57 | best_loss 4.769\n",
            "2024-01-06 23:06:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 57 updates\n",
            "2024-01-06 23:06:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 19 @ 57 updates, score 4.769) (writing took 0.3277870469999016 seconds)\n",
            "2024-01-06 23:06:37 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-01-06 23:06:37 | INFO | train | epoch 019 | loss 4.959 | nll_loss 4.811 | ppl 28.06 | wps 824.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 57 | lr 5.7e-05 | gnorm 1.192 | clip 66.7 | train_wall 10 | wall 210\n",
            "2024-01-06 23:06:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 020:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:37 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-01-06 23:06:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  67% 2/3 [00:04<00:01,  1.81s/it]2024-01-06 23:06:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:47 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.708 | nll_loss 4.531 | ppl 23.12 | wps 2116.4 | wpb 921 | bsz 75 | num_updates 60 | best_loss 4.708\n",
            "2024-01-06 23:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 60 updates\n",
            "2024-01-06 23:06:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 20 @ 60 updates, score 4.708) (writing took 0.47672722100014653 seconds)\n",
            "2024-01-06 23:06:48 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-01-06 23:06:48 | INFO | train | epoch 020 | loss 4.863 | nll_loss 4.702 | ppl 26.03 | wps 818 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 60 | lr 6e-05 | gnorm 1.128 | clip 33.3 | train_wall 9 | wall 220\n",
            "2024-01-06 23:06:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 021:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:48 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-01-06 23:06:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  67% 2/3 [00:04<00:02,  2.51s/it]2024-01-06 23:06:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:06:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.659 | nll_loss 4.475 | ppl 22.23 | wps 2812.9 | wpb 921 | bsz 75 | num_updates 63 | best_loss 4.659\n",
            "2024-01-06 23:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 63 updates\n",
            "2024-01-06 23:06:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:06:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 21 @ 63 updates, score 4.659) (writing took 0.4610572809999667 seconds)\n",
            "2024-01-06 23:06:58 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-01-06 23:06:58 | INFO | train | epoch 021 | loss 4.793 | nll_loss 4.622 | ppl 24.62 | wps 853.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 63 | lr 6.3e-05 | gnorm 1.091 | clip 33.3 | train_wall 9 | wall 231\n",
            "2024-01-06 23:06:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 022:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:06:58 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-01-06 23:06:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  67% 2/3 [00:09<00:04,  4.55s/it]2024-01-06 23:07:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:07:08 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.611 | nll_loss 4.419 | ppl 21.39 | wps 2969.3 | wpb 921 | bsz 75 | num_updates 66 | best_loss 4.611\n",
            "2024-01-06 23:07:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 66 updates\n",
            "2024-01-06 23:07:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 22 @ 66 updates, score 4.611) (writing took 0.3142124169999079 seconds)\n",
            "2024-01-06 23:07:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-01-06 23:07:08 | INFO | train | epoch 022 | loss 4.746 | nll_loss 4.568 | ppl 23.71 | wps 840.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 66 | lr 6.6e-05 | gnorm 1.055 | clip 33.3 | train_wall 9 | wall 241\n",
            "2024-01-06 23:07:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 023:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:07:08 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-01-06 23:07:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  67% 2/3 [00:09<00:04,  4.71s/it]2024-01-06 23:07:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:07:19 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.57 | nll_loss 4.37 | ppl 20.68 | wps 2785.4 | wpb 921 | bsz 75 | num_updates 69 | best_loss 4.57\n",
            "2024-01-06 23:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 69 updates\n",
            "2024-01-06 23:07:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 23 @ 69 updates, score 4.57) (writing took 0.39706007399990995 seconds)\n",
            "2024-01-06 23:07:19 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-01-06 23:07:19 | INFO | train | epoch 023 | loss 4.705 | nll_loss 4.52 | ppl 22.95 | wps 831.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 69 | lr 6.9e-05 | gnorm 1.061 | clip 33.3 | train_wall 9 | wall 252\n",
            "2024-01-06 23:07:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 024:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:07:19 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-01-06 23:07:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  67% 2/3 [00:05<00:02,  2.32s/it]2024-01-06 23:07:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:07:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.539 | nll_loss 4.333 | ppl 20.15 | wps 2891.1 | wpb 921 | bsz 75 | num_updates 72 | best_loss 4.539\n",
            "2024-01-06 23:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 72 updates\n",
            "2024-01-06 23:07:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 24 @ 72 updates, score 4.539) (writing took 0.32318252799996117 seconds)\n",
            "2024-01-06 23:07:30 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-01-06 23:07:30 | INFO | train | epoch 024 | loss 4.65 | nll_loss 4.457 | ppl 21.96 | wps 820.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 72 | lr 7.2e-05 | gnorm 0.996 | clip 33.3 | train_wall 10 | wall 262\n",
            "2024-01-06 23:07:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 025:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:07:30 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-01-06 23:07:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  67% 2/3 [00:04<00:02,  2.64s/it]2024-01-06 23:07:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:07:40 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.51 | nll_loss 4.299 | ppl 19.68 | wps 2599.2 | wpb 921 | bsz 75 | num_updates 75 | best_loss 4.51\n",
            "2024-01-06 23:07:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 75 updates\n",
            "2024-01-06 23:07:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 25 @ 75 updates, score 4.51) (writing took 0.35322306600005504 seconds)\n",
            "2024-01-06 23:07:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-01-06 23:07:40 | INFO | train | epoch 025 | loss 4.619 | nll_loss 4.42 | ppl 21.41 | wps 831 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 75 | lr 7.5e-05 | gnorm 1.001 | clip 33.3 | train_wall 10 | wall 273\n",
            "2024-01-06 23:07:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 026:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:07:40 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-01-06 23:07:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  67% 2/3 [00:05<00:02,  2.25s/it]2024-01-06 23:07:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:07:51 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.478 | nll_loss 4.262 | ppl 19.18 | wps 2684 | wpb 921 | bsz 75 | num_updates 78 | best_loss 4.478\n",
            "2024-01-06 23:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 78 updates\n",
            "2024-01-06 23:07:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 26 @ 78 updates, score 4.478) (writing took 0.3913115439995636 seconds)\n",
            "2024-01-06 23:07:51 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-01-06 23:07:51 | INFO | train | epoch 026 | loss 4.596 | nll_loss 4.393 | ppl 21.01 | wps 826.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 78 | lr 7.8e-05 | gnorm 1.006 | clip 33.3 | train_wall 9 | wall 284\n",
            "2024-01-06 23:07:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 027:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:07:51 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-01-06 23:07:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  67% 2/3 [00:04<00:01,  1.88s/it]2024-01-06 23:08:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:01 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.444 | nll_loss 4.221 | ppl 18.65 | wps 2791.4 | wpb 921 | bsz 75 | num_updates 81 | best_loss 4.444\n",
            "2024-01-06 23:08:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 81 updates\n",
            "2024-01-06 23:08:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 27 @ 81 updates, score 4.444) (writing took 0.3613928559998385 seconds)\n",
            "2024-01-06 23:08:02 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-01-06 23:08:02 | INFO | train | epoch 027 | loss 4.564 | nll_loss 4.355 | ppl 20.46 | wps 807.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 81 | lr 8.1e-05 | gnorm 0.981 | clip 33.3 | train_wall 10 | wall 294\n",
            "2024-01-06 23:08:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 028:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:02 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-01-06 23:08:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  67% 2/3 [00:04<00:01,  1.83s/it]2024-01-06 23:08:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.409 | nll_loss 4.181 | ppl 18.13 | wps 2981.6 | wpb 921 | bsz 75 | num_updates 84 | best_loss 4.409\n",
            "2024-01-06 23:08:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 84 updates\n",
            "2024-01-06 23:08:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 28 @ 84 updates, score 4.409) (writing took 0.3590102160001152 seconds)\n",
            "2024-01-06 23:08:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-01-06 23:08:13 | INFO | train | epoch 028 | loss 4.528 | nll_loss 4.313 | ppl 19.88 | wps 816.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 84 | lr 8.4e-05 | gnorm 0.981 | clip 33.3 | train_wall 10 | wall 305\n",
            "2024-01-06 23:08:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 029:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:13 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-01-06 23:08:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  67% 2/3 [00:04<00:01,  1.76s/it]2024-01-06 23:08:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:23 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.374 | nll_loss 4.14 | ppl 17.63 | wps 2665.8 | wpb 921 | bsz 75 | num_updates 87 | best_loss 4.374\n",
            "2024-01-06 23:08:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 87 updates\n",
            "2024-01-06 23:08:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 29 @ 87 updates, score 4.374) (writing took 0.33808759600015037 seconds)\n",
            "2024-01-06 23:08:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-01-06 23:08:23 | INFO | train | epoch 029 | loss 4.493 | nll_loss 4.273 | ppl 19.34 | wps 843.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 87 | lr 8.7e-05 | gnorm 0.994 | clip 33.3 | train_wall 9 | wall 316\n",
            "2024-01-06 23:08:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 030:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:23 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-01-06 23:08:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  67% 2/3 [00:09<00:04,  4.69s/it]2024-01-06 23:08:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:33 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.333 | nll_loss 4.093 | ppl 17.06 | wps 2178.2 | wpb 921 | bsz 75 | num_updates 90 | best_loss 4.333\n",
            "2024-01-06 23:08:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 90 updates\n",
            "2024-01-06 23:08:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 30 @ 90 updates, score 4.333) (writing took 0.6963923820003401 seconds)\n",
            "2024-01-06 23:08:34 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-01-06 23:08:34 | INFO | train | epoch 030 | loss 4.464 | nll_loss 4.239 | ppl 18.89 | wps 798.5 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 90 | lr 9e-05 | gnorm 0.973 | clip 33.3 | train_wall 9 | wall 327\n",
            "2024-01-06 23:08:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 031:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:34 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-01-06 23:08:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  67% 2/3 [00:04<00:02,  2.47s/it]2024-01-06 23:08:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.39it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:44 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.296 | nll_loss 4.05 | ppl 16.57 | wps 2072.9 | wpb 921 | bsz 75 | num_updates 93 | best_loss 4.296\n",
            "2024-01-06 23:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 93 updates\n",
            "2024-01-06 23:08:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 31 @ 93 updates, score 4.296) (writing took 0.5486428980002529 seconds)\n",
            "2024-01-06 23:08:44 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-01-06 23:08:44 | INFO | train | epoch 031 | loss 4.425 | nll_loss 4.195 | ppl 18.31 | wps 837.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 93 | lr 9.3e-05 | gnorm 1.015 | clip 33.3 | train_wall 9 | wall 337\n",
            "2024-01-06 23:08:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 032:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:45 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-01-06 23:08:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  67% 2/3 [00:05<00:03,  3.08s/it]2024-01-06 23:08:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:08:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.252 | nll_loss 4 | ppl 16 | wps 2782.9 | wpb 921 | bsz 75 | num_updates 96 | best_loss 4.252\n",
            "2024-01-06 23:08:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 96 updates\n",
            "2024-01-06 23:08:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 32 @ 96 updates, score 4.252) (writing took 0.34887237600014487 seconds)\n",
            "2024-01-06 23:08:55 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-01-06 23:08:55 | INFO | train | epoch 032 | loss 4.387 | nll_loss 4.152 | ppl 17.78 | wps 844.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 96 | lr 9.6e-05 | gnorm 1.012 | clip 33.3 | train_wall 9 | wall 348\n",
            "2024-01-06 23:08:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 033:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:08:55 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-01-06 23:08:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  67% 2/3 [00:04<00:02,  2.74s/it]2024-01-06 23:09:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:05 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.192 | nll_loss 3.93 | ppl 15.24 | wps 3007.9 | wpb 921 | bsz 75 | num_updates 99 | best_loss 4.192\n",
            "2024-01-06 23:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 99 updates\n",
            "2024-01-06 23:09:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 33 @ 99 updates, score 4.192) (writing took 0.3494064859996797 seconds)\n",
            "2024-01-06 23:09:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-01-06 23:09:06 | INFO | train | epoch 033 | loss 4.341 | nll_loss 4.1 | ppl 17.15 | wps 824.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 99 | lr 9.9e-05 | gnorm 0.998 | clip 33.3 | train_wall 10 | wall 358\n",
            "2024-01-06 23:09:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 034:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:06 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-01-06 23:09:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  67% 2/3 [00:04<00:01,  1.96s/it, loss=6.143, nll_loss=6.025, ppl=65.13, wps=811.8, ups=0.27, wpb=2941.3, bsz=235, num_updates=100, lr=0.0001, gnorm=2.885, clip=69, train_wall=324, wall=363]2024-01-06 23:09:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:16 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.121 | nll_loss 3.846 | ppl 14.38 | wps 2928.9 | wpb 921 | bsz 75 | num_updates 102 | best_loss 4.121\n",
            "2024-01-06 23:09:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 102 updates\n",
            "2024-01-06 23:09:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 34 @ 102 updates, score 4.121) (writing took 0.3605580559997179 seconds)\n",
            "2024-01-06 23:09:16 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-01-06 23:09:16 | INFO | train | epoch 034 | loss 4.296 | nll_loss 4.046 | ppl 16.52 | wps 825.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 102 | lr 0.000102 | gnorm 1.016 | clip 33.3 | train_wall 10 | wall 369\n",
            "2024-01-06 23:09:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 035:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:16 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-01-06 23:09:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  67% 2/3 [00:04<00:02,  2.74s/it]2024-01-06 23:09:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:27 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.056 | nll_loss 3.766 | ppl 13.61 | wps 2891.2 | wpb 921 | bsz 75 | num_updates 105 | best_loss 4.056\n",
            "2024-01-06 23:09:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 105 updates\n",
            "2024-01-06 23:09:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 35 @ 105 updates, score 4.056) (writing took 0.34001351500000965 seconds)\n",
            "2024-01-06 23:09:27 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-01-06 23:09:27 | INFO | train | epoch 035 | loss 4.216 | nll_loss 3.952 | ppl 15.48 | wps 823 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 105 | lr 0.000105 | gnorm 0.973 | clip 33.3 | train_wall 10 | wall 380\n",
            "2024-01-06 23:09:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 036:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:27 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-01-06 23:09:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  67% 2/3 [00:05<00:03,  3.17s/it]2024-01-06 23:09:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:37 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.995 | nll_loss 3.688 | ppl 12.89 | wps 2814.5 | wpb 921 | bsz 75 | num_updates 108 | best_loss 3.995\n",
            "2024-01-06 23:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 108 updates\n",
            "2024-01-06 23:09:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 36 @ 108 updates, score 3.995) (writing took 0.34144990400000097 seconds)\n",
            "2024-01-06 23:09:37 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-01-06 23:09:37 | INFO | train | epoch 036 | loss 4.155 | nll_loss 3.88 | ppl 14.72 | wps 835.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 108 | lr 0.000108 | gnorm 1.012 | clip 33.3 | train_wall 9 | wall 390\n",
            "2024-01-06 23:09:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 037:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:37 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-01-06 23:09:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  67% 2/3 [00:04<00:01,  1.77s/it]2024-01-06 23:09:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.72it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.934 | nll_loss 3.604 | ppl 12.16 | wps 2954.5 | wpb 921 | bsz 75 | num_updates 111 | best_loss 3.934\n",
            "2024-01-06 23:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 111 updates\n",
            "2024-01-06 23:09:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 37 @ 111 updates, score 3.934) (writing took 0.3441332639999928 seconds)\n",
            "2024-01-06 23:09:48 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-01-06 23:09:48 | INFO | train | epoch 037 | loss 4.107 | nll_loss 3.82 | ppl 14.13 | wps 837.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 111 | lr 0.000111 | gnorm 1.006 | clip 33.3 | train_wall 9 | wall 401\n",
            "2024-01-06 23:09:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 038:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:48 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-01-06 23:09:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  67% 2/3 [00:04<00:01,  1.84s/it]2024-01-06 23:09:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:09:58 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.888 | nll_loss 3.533 | ppl 11.58 | wps 2756.3 | wpb 921 | bsz 75 | num_updates 114 | best_loss 3.888\n",
            "2024-01-06 23:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 114 updates\n",
            "2024-01-06 23:09:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 38 @ 114 updates, score 3.888) (writing took 0.33570539499987717 seconds)\n",
            "2024-01-06 23:09:59 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-01-06 23:09:59 | INFO | train | epoch 038 | loss 4.053 | nll_loss 3.75 | ppl 13.45 | wps 819.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 114 | lr 0.000114 | gnorm 0.965 | clip 33.3 | train_wall 10 | wall 411\n",
            "2024-01-06 23:09:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 039:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:09:59 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-01-06 23:09:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:10:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:10:09 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.847 | nll_loss 3.47 | ppl 11.08 | wps 2998.5 | wpb 921 | bsz 75 | num_updates 117 | best_loss 3.847\n",
            "2024-01-06 23:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 117 updates\n",
            "2024-01-06 23:10:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 39 @ 117 updates, score 3.847) (writing took 0.3145261270001356 seconds)\n",
            "2024-01-06 23:10:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-01-06 23:10:09 | INFO | train | epoch 039 | loss 3.996 | nll_loss 3.676 | ppl 12.78 | wps 837.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 117 | lr 0.000117 | gnorm 0.939 | clip 33.3 | train_wall 9 | wall 422\n",
            "2024-01-06 23:10:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 040:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:10:09 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-01-06 23:10:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  67% 2/3 [00:05<00:02,  2.97s/it]2024-01-06 23:10:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.44it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:10:19 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.82 | nll_loss 3.425 | ppl 10.74 | wps 2920.6 | wpb 921 | bsz 75 | num_updates 120 | best_loss 3.82\n",
            "2024-01-06 23:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 120 updates\n",
            "2024-01-06 23:10:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 40 @ 120 updates, score 3.82) (writing took 0.3351446950000536 seconds)\n",
            "2024-01-06 23:10:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-01-06 23:10:20 | INFO | train | epoch 040 | loss 3.944 | nll_loss 3.604 | ppl 12.16 | wps 835.2 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 120 | lr 0.00012 | gnorm 0.995 | clip 33.3 | train_wall 9 | wall 432\n",
            "2024-01-06 23:10:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 041:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:10:20 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-01-06 23:10:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  67% 2/3 [00:09<00:04,  4.99s/it]2024-01-06 23:10:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:10:31 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.777 | nll_loss 3.362 | ppl 10.29 | wps 2215.6 | wpb 921 | bsz 75 | num_updates 123 | best_loss 3.777\n",
            "2024-01-06 23:10:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 123 updates\n",
            "2024-01-06 23:10:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 41 @ 123 updates, score 3.777) (writing took 0.4558350600000267 seconds)\n",
            "2024-01-06 23:10:31 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-01-06 23:10:31 | INFO | train | epoch 041 | loss 3.921 | nll_loss 3.573 | ppl 11.9 | wps 766.1 | ups 0.26 | wpb 2928.3 | bsz 233.3 | num_updates 123 | lr 0.000123 | gnorm 0.973 | clip 33.3 | train_wall 10 | wall 444\n",
            "2024-01-06 23:10:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 042:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:10:31 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-01-06 23:10:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  67% 2/3 [00:05<00:02,  2.14s/it]2024-01-06 23:10:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.53it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:10:41 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.737 | nll_loss 3.314 | ppl 9.94 | wps 1909.8 | wpb 921 | bsz 75 | num_updates 126 | best_loss 3.737\n",
            "2024-01-06 23:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 126 updates\n",
            "2024-01-06 23:10:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 42 @ 126 updates, score 3.737) (writing took 0.42531902099972285 seconds)\n",
            "2024-01-06 23:10:41 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-01-06 23:10:41 | INFO | train | epoch 042 | loss 3.849 | nll_loss 3.485 | ppl 11.19 | wps 843.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 126 | lr 0.000126 | gnorm 0.875 | clip 33.3 | train_wall 9 | wall 454\n",
            "2024-01-06 23:10:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 043:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:10:42 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-01-06 23:10:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  67% 2/3 [00:05<00:03,  3.13s/it]2024-01-06 23:10:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:10:52 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.708 | nll_loss 3.275 | ppl 9.68 | wps 2740.8 | wpb 921 | bsz 75 | num_updates 129 | best_loss 3.708\n",
            "2024-01-06 23:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 129 updates\n",
            "2024-01-06 23:10:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 43 @ 129 updates, score 3.708) (writing took 0.33299336599975504 seconds)\n",
            "2024-01-06 23:10:52 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-01-06 23:10:52 | INFO | train | epoch 043 | loss 3.813 | nll_loss 3.442 | ppl 10.87 | wps 840.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 129 | lr 0.000129 | gnorm 0.933 | clip 33.3 | train_wall 9 | wall 465\n",
            "2024-01-06 23:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 044:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:10:52 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-01-06 23:10:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  67% 2/3 [00:09<00:04,  4.80s/it]2024-01-06 23:11:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:02 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.679 | nll_loss 3.231 | ppl 9.39 | wps 2967.8 | wpb 921 | bsz 75 | num_updates 132 | best_loss 3.679\n",
            "2024-01-06 23:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 132 updates\n",
            "2024-01-06 23:11:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 44 @ 132 updates, score 3.679) (writing took 0.3703152430002774 seconds)\n",
            "2024-01-06 23:11:03 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-01-06 23:11:03 | INFO | train | epoch 044 | loss 3.783 | nll_loss 3.405 | ppl 10.59 | wps 819.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 132 | lr 0.000132 | gnorm 0.863 | clip 33.3 | train_wall 10 | wall 475\n",
            "2024-01-06 23:11:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 045:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:03 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-01-06 23:11:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  67% 2/3 [00:04<00:01,  1.97s/it]2024-01-06 23:11:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:13 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.649 | nll_loss 3.192 | ppl 9.14 | wps 2700.7 | wpb 921 | bsz 75 | num_updates 135 | best_loss 3.649\n",
            "2024-01-06 23:11:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 135 updates\n",
            "2024-01-06 23:11:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 45 @ 135 updates, score 3.649) (writing took 0.31567039599985947 seconds)\n",
            "2024-01-06 23:11:13 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-01-06 23:11:13 | INFO | train | epoch 045 | loss 3.732 | nll_loss 3.34 | ppl 10.13 | wps 830.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 135 | lr 0.000135 | gnorm 0.737 | clip 33.3 | train_wall 10 | wall 486\n",
            "2024-01-06 23:11:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 046:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:13 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-01-06 23:11:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  67% 2/3 [00:09<00:04,  4.78s/it]2024-01-06 23:11:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:24 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.62 | nll_loss 3.156 | ppl 8.92 | wps 2861.3 | wpb 921 | bsz 75 | num_updates 138 | best_loss 3.62\n",
            "2024-01-06 23:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 138 updates\n",
            "2024-01-06 23:11:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 46 @ 138 updates, score 3.62) (writing took 0.34010457500016855 seconds)\n",
            "2024-01-06 23:11:24 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-01-06 23:11:24 | INFO | train | epoch 046 | loss 3.705 | nll_loss 3.306 | ppl 9.89 | wps 825.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 138 | lr 0.000138 | gnorm 0.685 | clip 33.3 | train_wall 10 | wall 497\n",
            "2024-01-06 23:11:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 047:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:24 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-01-06 23:11:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  67% 2/3 [00:05<00:03,  3.17s/it]2024-01-06 23:11:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:34 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.607 | nll_loss 3.126 | ppl 8.73 | wps 2652.1 | wpb 921 | bsz 75 | num_updates 141 | best_loss 3.607\n",
            "2024-01-06 23:11:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 141 updates\n",
            "2024-01-06 23:11:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 47 @ 141 updates, score 3.607) (writing took 0.3132834160001039 seconds)\n",
            "2024-01-06 23:11:34 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-01-06 23:11:34 | INFO | train | epoch 047 | loss 3.668 | nll_loss 3.256 | ppl 9.55 | wps 832.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 141 | lr 0.000141 | gnorm 0.751 | clip 33.3 | train_wall 10 | wall 507\n",
            "2024-01-06 23:11:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 048:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:34 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-01-06 23:11:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  67% 2/3 [00:09<00:04,  4.81s/it]2024-01-06 23:11:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.577 | nll_loss 3.098 | ppl 8.56 | wps 2641.4 | wpb 921 | bsz 75 | num_updates 144 | best_loss 3.577\n",
            "2024-01-06 23:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 144 updates\n",
            "2024-01-06 23:11:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 48 @ 144 updates, score 3.577) (writing took 0.333684754999922 seconds)\n",
            "2024-01-06 23:11:45 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-01-06 23:11:45 | INFO | train | epoch 048 | loss 3.66 | nll_loss 3.242 | ppl 9.46 | wps 829.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 144 | lr 0.000144 | gnorm 0.752 | clip 33.3 | train_wall 10 | wall 518\n",
            "2024-01-06 23:11:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 049:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:45 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-01-06 23:11:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  67% 2/3 [00:05<00:02,  2.13s/it]2024-01-06 23:11:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:11:55 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.553 | nll_loss 3.07 | ppl 8.4 | wps 2808 | wpb 921 | bsz 75 | num_updates 147 | best_loss 3.553\n",
            "2024-01-06 23:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 147 updates\n",
            "2024-01-06 23:11:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 49 @ 147 updates, score 3.553) (writing took 0.3856438729999354 seconds)\n",
            "2024-01-06 23:11:56 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-01-06 23:11:56 | INFO | train | epoch 049 | loss 3.616 | nll_loss 3.2 | ppl 9.19 | wps 823.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 147 | lr 0.000147 | gnorm 0.729 | clip 33.3 | train_wall 10 | wall 528\n",
            "2024-01-06 23:11:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 050:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:11:56 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-01-06 23:11:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  67% 2/3 [00:09<00:04,  4.85s/it]2024-01-06 23:12:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:06 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.547 | nll_loss 3.051 | ppl 8.29 | wps 2897.9 | wpb 921 | bsz 75 | num_updates 150 | best_loss 3.547\n",
            "2024-01-06 23:12:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 150 updates\n",
            "2024-01-06 23:12:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 50 @ 150 updates, score 3.547) (writing took 0.38809808299993165 seconds)\n",
            "2024-01-06 23:12:06 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-01-06 23:12:06 | INFO | train | epoch 050 | loss 3.591 | nll_loss 3.167 | ppl 8.98 | wps 819.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 150 | lr 0.00015 | gnorm 0.7 | clip 33.3 | train_wall 10 | wall 539\n",
            "2024-01-06 23:12:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 051:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:06 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-01-06 23:12:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  67% 2/3 [00:04<00:02,  2.51s/it]2024-01-06 23:12:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.51it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:17 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.535 | nll_loss 3.031 | ppl 8.17 | wps 2647.4 | wpb 921 | bsz 75 | num_updates 153 | best_loss 3.535\n",
            "2024-01-06 23:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 153 updates\n",
            "2024-01-06 23:12:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 51 @ 153 updates, score 3.535) (writing took 0.3540499240002646 seconds)\n",
            "2024-01-06 23:12:17 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-01-06 23:12:17 | INFO | train | epoch 051 | loss 3.559 | nll_loss 3.121 | ppl 8.7 | wps 823.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 153 | lr 0.000153 | gnorm 0.745 | clip 33.3 | train_wall 10 | wall 550\n",
            "2024-01-06 23:12:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 052:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:17 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-01-06 23:12:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  67% 2/3 [00:04<00:01,  1.81s/it]2024-01-06 23:12:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:27 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.509 | nll_loss 3.011 | ppl 8.06 | wps 2048.9 | wpb 921 | bsz 75 | num_updates 156 | best_loss 3.509\n",
            "2024-01-06 23:12:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 156 updates\n",
            "2024-01-06 23:12:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 52 @ 156 updates, score 3.509) (writing took 0.49228995800012854 seconds)\n",
            "2024-01-06 23:12:28 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-01-06 23:12:28 | INFO | train | epoch 052 | loss 3.54 | nll_loss 3.101 | ppl 8.58 | wps 824.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 156 | lr 0.000156 | gnorm 0.667 | clip 33.3 | train_wall 9 | wall 560\n",
            "2024-01-06 23:12:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 053:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:28 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-01-06 23:12:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  67% 2/3 [00:04<00:02,  2.56s/it]2024-01-06 23:12:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:38 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.459 | nll_loss 2.959 | ppl 7.78 | wps 2828 | wpb 921 | bsz 75 | num_updates 159 | best_loss 3.459\n",
            "2024-01-06 23:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 159 updates\n",
            "2024-01-06 23:12:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 53 @ 159 updates, score 3.459) (writing took 0.31477246600024955 seconds)\n",
            "2024-01-06 23:12:38 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-01-06 23:12:38 | INFO | train | epoch 053 | loss 3.513 | nll_loss 3.078 | ppl 8.44 | wps 845.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 159 | lr 0.000159 | gnorm 0.769 | clip 33.3 | train_wall 9 | wall 571\n",
            "2024-01-06 23:12:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 054:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:38 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-01-06 23:12:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  67% 2/3 [00:04<00:02,  2.73s/it]2024-01-06 23:12:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:48 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.436 | nll_loss 2.929 | ppl 7.62 | wps 2845.6 | wpb 921 | bsz 75 | num_updates 162 | best_loss 3.436\n",
            "2024-01-06 23:12:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 162 updates\n",
            "2024-01-06 23:12:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 54 @ 162 updates, score 3.436) (writing took 0.3234019259998604 seconds)\n",
            "2024-01-06 23:12:49 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-01-06 23:12:49 | INFO | train | epoch 054 | loss 3.488 | nll_loss 3.049 | ppl 8.28 | wps 824.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 162 | lr 0.000162 | gnorm 0.807 | clip 33.3 | train_wall 10 | wall 581\n",
            "2024-01-06 23:12:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 055:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:49 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-01-06 23:12:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  67% 2/3 [00:04<00:02,  2.69s/it]2024-01-06 23:12:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:12:59 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.416 | nll_loss 2.905 | ppl 7.49 | wps 2724 | wpb 921 | bsz 75 | num_updates 165 | best_loss 3.416\n",
            "2024-01-06 23:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 165 updates\n",
            "2024-01-06 23:12:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 55 @ 165 updates, score 3.416) (writing took 0.4094937520003441 seconds)\n",
            "2024-01-06 23:12:59 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-01-06 23:12:59 | INFO | train | epoch 055 | loss 3.463 | nll_loss 3.018 | ppl 8.1 | wps 832.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 165 | lr 0.000165 | gnorm 0.792 | clip 33.3 | train_wall 9 | wall 592\n",
            "2024-01-06 23:12:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 056:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:12:59 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-01-06 23:12:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:13:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:13:10 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.362 | nll_loss 2.857 | ppl 7.24 | wps 2719.4 | wpb 921 | bsz 75 | num_updates 168 | best_loss 3.362\n",
            "2024-01-06 23:13:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 168 updates\n",
            "2024-01-06 23:13:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 56 @ 168 updates, score 3.362) (writing took 0.3919004720000885 seconds)\n",
            "2024-01-06 23:13:10 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-01-06 23:13:10 | INFO | train | epoch 056 | loss 3.427 | nll_loss 2.981 | ppl 7.89 | wps 820.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 168 | lr 0.000168 | gnorm 0.841 | clip 33.3 | train_wall 10 | wall 603\n",
            "2024-01-06 23:13:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 057:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:13:10 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-01-06 23:13:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  67% 2/3 [00:05<00:03,  3.21s/it]2024-01-06 23:13:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:13:20 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.35 | nll_loss 2.827 | ppl 7.09 | wps 2490.2 | wpb 921 | bsz 75 | num_updates 171 | best_loss 3.35\n",
            "2024-01-06 23:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 171 updates\n",
            "2024-01-06 23:13:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 57 @ 171 updates, score 3.35) (writing took 0.33001985500004594 seconds)\n",
            "2024-01-06 23:13:21 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-01-06 23:13:21 | INFO | train | epoch 057 | loss 3.415 | nll_loss 2.967 | ppl 7.82 | wps 820.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 171 | lr 0.000171 | gnorm 0.962 | clip 33.3 | train_wall 10 | wall 613\n",
            "2024-01-06 23:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 058:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:13:21 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-01-06 23:13:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  67% 2/3 [00:04<00:02,  2.52s/it]2024-01-06 23:13:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:13:31 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.34 | nll_loss 2.805 | ppl 6.99 | wps 2916.1 | wpb 921 | bsz 75 | num_updates 174 | best_loss 3.34\n",
            "2024-01-06 23:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 174 updates\n",
            "2024-01-06 23:13:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 58 @ 174 updates, score 3.34) (writing took 0.40068988199982414 seconds)\n",
            "2024-01-06 23:13:31 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-01-06 23:13:31 | INFO | train | epoch 058 | loss 3.382 | nll_loss 2.919 | ppl 7.56 | wps 823.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 174 | lr 0.000174 | gnorm 0.859 | clip 33.3 | train_wall 10 | wall 624\n",
            "2024-01-06 23:13:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 059:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:13:31 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-01-06 23:13:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  67% 2/3 [00:04<00:01,  1.77s/it]2024-01-06 23:13:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:13:42 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.29 | nll_loss 2.763 | ppl 6.79 | wps 2796.9 | wpb 921 | bsz 75 | num_updates 177 | best_loss 3.29\n",
            "2024-01-06 23:13:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 177 updates\n",
            "2024-01-06 23:13:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 59 @ 177 updates, score 3.29) (writing took 0.33145637200004785 seconds)\n",
            "2024-01-06 23:13:42 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-01-06 23:13:42 | INFO | train | epoch 059 | loss 3.348 | nll_loss 2.886 | ppl 7.39 | wps 839.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 177 | lr 0.000177 | gnorm 0.926 | clip 33.3 | train_wall 9 | wall 635\n",
            "2024-01-06 23:13:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 060:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:13:42 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-01-06 23:13:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  67% 2/3 [00:09<00:04,  4.62s/it]2024-01-06 23:13:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:13:52 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.256 | nll_loss 2.727 | ppl 6.62 | wps 2040.8 | wpb 921 | bsz 75 | num_updates 180 | best_loss 3.256\n",
            "2024-01-06 23:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 180 updates\n",
            "2024-01-06 23:13:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 60 @ 180 updates, score 3.256) (writing took 0.39606301999992866 seconds)\n",
            "2024-01-06 23:13:52 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-01-06 23:13:52 | INFO | train | epoch 060 | loss 3.342 | nll_loss 2.887 | ppl 7.4 | wps 834.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 180 | lr 0.00018 | gnorm 1.066 | clip 33.3 | train_wall 9 | wall 645\n",
            "2024-01-06 23:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 061:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:13:52 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-01-06 23:13:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  67% 2/3 [00:12<00:06,  6.58s/it]2024-01-06 23:14:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:14:06 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.248 | nll_loss 2.684 | ppl 6.43 | wps 2788.4 | wpb 921 | bsz 75 | num_updates 183 | best_loss 3.248\n",
            "2024-01-06 23:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 183 updates\n",
            "2024-01-06 23:14:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 61 @ 183 updates, score 3.248) (writing took 0.5224474770002416 seconds)\n",
            "2024-01-06 23:14:06 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-01-06 23:14:06 | INFO | train | epoch 061 | loss 3.303 | nll_loss 2.843 | ppl 7.18 | wps 626.1 | ups 0.21 | wpb 2928.3 | bsz 233.3 | num_updates 183 | lr 0.000183 | gnorm 0.959 | clip 33.3 | train_wall 13 | wall 659\n",
            "2024-01-06 23:14:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 062:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:14:06 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-01-06 23:14:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  67% 2/3 [00:05<00:02,  2.24s/it]2024-01-06 23:14:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:14:17 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.226 | nll_loss 2.65 | ppl 6.28 | wps 2873.6 | wpb 921 | bsz 75 | num_updates 186 | best_loss 3.226\n",
            "2024-01-06 23:14:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 186 updates\n",
            "2024-01-06 23:14:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 62 @ 186 updates, score 3.226) (writing took 0.4986361879996366 seconds)\n",
            "2024-01-06 23:14:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-01-06 23:14:18 | INFO | train | epoch 062 | loss 3.27 | nll_loss 2.783 | ppl 6.88 | wps 792.2 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 186 | lr 0.000186 | gnorm 0.924 | clip 33.3 | train_wall 10 | wall 670\n",
            "2024-01-06 23:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 063:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:14:18 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-01-06 23:14:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  67% 2/3 [00:05<00:02,  2.17s/it]2024-01-06 23:14:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:14:28 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.221 | nll_loss 2.649 | ppl 6.27 | wps 2137.5 | wpb 921 | bsz 75 | num_updates 189 | best_loss 3.221\n",
            "2024-01-06 23:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 189 updates\n",
            "2024-01-06 23:14:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 63 @ 189 updates, score 3.221) (writing took 0.4816317480003818 seconds)\n",
            "2024-01-06 23:14:28 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-01-06 23:14:28 | INFO | train | epoch 063 | loss 3.255 | nll_loss 2.764 | ppl 6.79 | wps 812.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 189 | lr 0.000189 | gnorm 1.151 | clip 33.3 | train_wall 9 | wall 681\n",
            "2024-01-06 23:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 064:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:14:28 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-01-06 23:14:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:14:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.61it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:14:38 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.149 | nll_loss 2.583 | ppl 5.99 | wps 2257 | wpb 921 | bsz 75 | num_updates 192 | best_loss 3.149\n",
            "2024-01-06 23:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 192 updates\n",
            "2024-01-06 23:14:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 64 @ 192 updates, score 3.149) (writing took 0.480345558999943 seconds)\n",
            "2024-01-06 23:14:39 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-01-06 23:14:39 | INFO | train | epoch 064 | loss 3.227 | nll_loss 2.734 | ppl 6.65 | wps 832.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 192 | lr 0.000192 | gnorm 1.498 | clip 33.3 | train_wall 9 | wall 692\n",
            "2024-01-06 23:14:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 065:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:14:39 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-01-06 23:14:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  67% 2/3 [00:05<00:02,  2.27s/it]2024-01-06 23:14:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:14:49 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.114 | nll_loss 2.559 | ppl 5.89 | wps 2551.4 | wpb 921 | bsz 75 | num_updates 195 | best_loss 3.114\n",
            "2024-01-06 23:14:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 195 updates\n",
            "2024-01-06 23:14:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 65 @ 195 updates, score 3.114) (writing took 0.31668352299993785 seconds)\n",
            "2024-01-06 23:14:49 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-01-06 23:14:49 | INFO | train | epoch 065 | loss 3.209 | nll_loss 2.73 | ppl 6.63 | wps 832.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 195 | lr 0.000195 | gnorm 1.303 | clip 33.3 | train_wall 10 | wall 702\n",
            "2024-01-06 23:14:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 066:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:14:49 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-01-06 23:14:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  67% 2/3 [00:04<00:02,  2.74s/it]2024-01-06 23:14:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:00 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.109 | nll_loss 2.507 | ppl 5.69 | wps 2842.7 | wpb 921 | bsz 75 | num_updates 198 | best_loss 3.109\n",
            "2024-01-06 23:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 198 updates\n",
            "2024-01-06 23:15:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 66 @ 198 updates, score 3.109) (writing took 0.5288033580000047 seconds)\n",
            "2024-01-06 23:15:00 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-01-06 23:15:00 | INFO | train | epoch 066 | loss 3.169 | nll_loss 2.67 | ppl 6.37 | wps 808.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 198 | lr 0.000198 | gnorm 1.333 | clip 33.3 | train_wall 10 | wall 713\n",
            "2024-01-06 23:15:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 067:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:00 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-01-06 23:15:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  67% 2/3 [00:09<00:04,  4.71s/it, loss=3.614, nll_loss=3.204, ppl=9.22, wps=818.7, ups=0.28, wpb=2943.6, bsz=234, num_updates=200, lr=0.0002, gnorm=0.92, clip=33, train_wall=322, wall=722]2024-01-06 23:15:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:10 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.072 | nll_loss 2.484 | ppl 5.59 | wps 2198.8 | wpb 921 | bsz 75 | num_updates 201 | best_loss 3.072\n",
            "2024-01-06 23:15:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 201 updates\n",
            "2024-01-06 23:15:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 67 @ 201 updates, score 3.072) (writing took 0.31380942300029346 seconds)\n",
            "2024-01-06 23:15:11 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-01-06 23:15:11 | INFO | train | epoch 067 | loss 3.146 | nll_loss 2.631 | ppl 6.19 | wps 835.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 201 | lr 0.000201 | gnorm 1.136 | clip 33.3 | train_wall 9 | wall 723\n",
            "2024-01-06 23:15:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 068:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:11 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-01-06 23:15:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:15:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:21 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.009 | nll_loss 2.408 | ppl 5.31 | wps 2670.8 | wpb 921 | bsz 75 | num_updates 204 | best_loss 3.009\n",
            "2024-01-06 23:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 204 updates\n",
            "2024-01-06 23:15:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 68 @ 204 updates, score 3.009) (writing took 0.37721708100025353 seconds)\n",
            "2024-01-06 23:15:22 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-01-06 23:15:22 | INFO | train | epoch 068 | loss 3.116 | nll_loss 2.608 | ppl 6.1 | wps 820.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 204 | lr 0.000204 | gnorm 1.327 | clip 33.3 | train_wall 10 | wall 734\n",
            "2024-01-06 23:15:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 069:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:22 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-01-06 23:15:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  67% 2/3 [00:09<00:04,  4.70s/it]2024-01-06 23:15:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.72it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:32 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 2.973 | nll_loss 2.373 | ppl 5.18 | wps 2660 | wpb 921 | bsz 75 | num_updates 207 | best_loss 2.973\n",
            "2024-01-06 23:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 207 updates\n",
            "2024-01-06 23:15:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 69 @ 207 updates, score 2.973) (writing took 0.3324306129998149 seconds)\n",
            "2024-01-06 23:15:32 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-01-06 23:15:32 | INFO | train | epoch 069 | loss 3.081 | nll_loss 2.569 | ppl 5.93 | wps 817.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 207 | lr 0.000207 | gnorm 1.035 | clip 33.3 | train_wall 10 | wall 745\n",
            "2024-01-06 23:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 070:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:32 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-01-06 23:15:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  67% 2/3 [00:04<00:01,  1.78s/it]2024-01-06 23:15:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:43 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 2.946 | nll_loss 2.314 | ppl 4.97 | wps 2788.5 | wpb 921 | bsz 75 | num_updates 210 | best_loss 2.946\n",
            "2024-01-06 23:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 210 updates\n",
            "2024-01-06 23:15:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 70 @ 210 updates, score 2.946) (writing took 0.3383809410001959 seconds)\n",
            "2024-01-06 23:15:43 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-01-06 23:15:43 | INFO | train | epoch 070 | loss 3.066 | nll_loss 2.543 | ppl 5.83 | wps 821.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 210 | lr 0.00021 | gnorm 1.343 | clip 66.7 | train_wall 10 | wall 756\n",
            "2024-01-06 23:15:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 071:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:43 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-01-06 23:15:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  67% 2/3 [00:05<00:02,  2.99s/it]2024-01-06 23:15:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:15:53 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 2.903 | nll_loss 2.284 | ppl 4.87 | wps 2533.7 | wpb 921 | bsz 75 | num_updates 213 | best_loss 2.903\n",
            "2024-01-06 23:15:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 213 updates\n",
            "2024-01-06 23:15:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 71 @ 213 updates, score 2.903) (writing took 0.3231960420002906 seconds)\n",
            "2024-01-06 23:15:54 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-01-06 23:15:54 | INFO | train | epoch 071 | loss 3.032 | nll_loss 2.492 | ppl 5.63 | wps 830.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 213 | lr 0.000213 | gnorm 1.345 | clip 66.7 | train_wall 10 | wall 766\n",
            "2024-01-06 23:15:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 072:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:15:54 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-01-06 23:15:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  67% 2/3 [00:09<00:04,  4.76s/it]2024-01-06 23:16:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:04 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 2.892 | nll_loss 2.269 | ppl 4.82 | wps 2727 | wpb 921 | bsz 75 | num_updates 216 | best_loss 2.892\n",
            "2024-01-06 23:16:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 216 updates\n",
            "2024-01-06 23:16:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 72 @ 216 updates, score 2.892) (writing took 0.37431198099966423 seconds)\n",
            "2024-01-06 23:16:04 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-01-06 23:16:04 | INFO | train | epoch 072 | loss 3.012 | nll_loss 2.488 | ppl 5.61 | wps 813.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 216 | lr 0.000216 | gnorm 1.332 | clip 33.3 | train_wall 10 | wall 777\n",
            "2024-01-06 23:16:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 073:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:04 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-01-06 23:16:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  67% 2/3 [00:09<00:04,  4.84s/it]2024-01-06 23:16:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:15 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 2.848 | nll_loss 2.201 | ppl 4.6 | wps 2828.2 | wpb 921 | bsz 75 | num_updates 219 | best_loss 2.848\n",
            "2024-01-06 23:16:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 219 updates\n",
            "2024-01-06 23:16:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 73 @ 219 updates, score 2.848) (writing took 0.3911150600001747 seconds)\n",
            "2024-01-06 23:16:15 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-01-06 23:16:15 | INFO | train | epoch 073 | loss 2.982 | nll_loss 2.449 | ppl 5.46 | wps 823.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 219 | lr 0.000219 | gnorm 1.261 | clip 33.3 | train_wall 10 | wall 788\n",
            "2024-01-06 23:16:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 074:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:15 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-01-06 23:16:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  67% 2/3 [00:05<00:02,  2.13s/it]2024-01-06 23:16:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:25 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 2.818 | nll_loss 2.154 | ppl 4.45 | wps 2179 | wpb 921 | bsz 75 | num_updates 222 | best_loss 2.818\n",
            "2024-01-06 23:16:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 222 updates\n",
            "2024-01-06 23:16:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 74 @ 222 updates, score 2.818) (writing took 0.4082983499997681 seconds)\n",
            "2024-01-06 23:16:26 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-01-06 23:16:26 | INFO | train | epoch 074 | loss 2.94 | nll_loss 2.389 | ppl 5.24 | wps 827.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 222 | lr 0.000222 | gnorm 1.184 | clip 33.3 | train_wall 9 | wall 798\n",
            "2024-01-06 23:16:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 075:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:26 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-01-06 23:16:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  67% 2/3 [00:04<00:01,  1.82s/it]2024-01-06 23:16:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.60it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:36 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 2.776 | nll_loss 2.107 | ppl 4.31 | wps 2135.7 | wpb 921 | bsz 75 | num_updates 225 | best_loss 2.776\n",
            "2024-01-06 23:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 225 updates\n",
            "2024-01-06 23:16:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 75 @ 225 updates, score 2.776) (writing took 0.5013503189998119 seconds)\n",
            "2024-01-06 23:16:36 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-01-06 23:16:36 | INFO | train | epoch 075 | loss 2.904 | nll_loss 2.339 | ppl 5.06 | wps 836.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 225 | lr 0.000225 | gnorm 1.181 | clip 33.3 | train_wall 9 | wall 809\n",
            "2024-01-06 23:16:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 076:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:36 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-01-06 23:16:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  67% 2/3 [00:05<00:02,  2.24s/it]2024-01-06 23:16:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:46 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 2.714 | nll_loss 2.068 | ppl 4.19 | wps 2846.3 | wpb 921 | bsz 75 | num_updates 228 | best_loss 2.714\n",
            "2024-01-06 23:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 228 updates\n",
            "2024-01-06 23:16:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 76 @ 228 updates, score 2.714) (writing took 0.33396680199984985 seconds)\n",
            "2024-01-06 23:16:47 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-01-06 23:16:47 | INFO | train | epoch 076 | loss 2.886 | nll_loss 2.331 | ppl 5.03 | wps 838.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 228 | lr 0.000228 | gnorm 1.425 | clip 66.7 | train_wall 9 | wall 819\n",
            "2024-01-06 23:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 077:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:47 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-01-06 23:16:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  67% 2/3 [00:04<00:02,  2.68s/it]2024-01-06 23:16:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:16:57 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 2.7 | nll_loss 2.027 | ppl 4.07 | wps 2805.3 | wpb 921 | bsz 75 | num_updates 231 | best_loss 2.7\n",
            "2024-01-06 23:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 231 updates\n",
            "2024-01-06 23:16:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 77 @ 231 updates, score 2.7) (writing took 0.34583803099985744 seconds)\n",
            "2024-01-06 23:16:57 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-01-06 23:16:57 | INFO | train | epoch 077 | loss 2.866 | nll_loss 2.314 | ppl 4.97 | wps 836.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 231 | lr 0.000231 | gnorm 1.36 | clip 33.3 | train_wall 9 | wall 830\n",
            "2024-01-06 23:16:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 078:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:16:57 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-01-06 23:16:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  67% 2/3 [00:04<00:02,  2.72s/it]2024-01-06 23:17:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:17:07 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 2.676 | nll_loss 1.973 | ppl 3.92 | wps 2869.4 | wpb 921 | bsz 75 | num_updates 234 | best_loss 2.676\n",
            "2024-01-06 23:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 234 updates\n",
            "2024-01-06 23:17:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 78 @ 234 updates, score 2.676) (writing took 0.34697776200027874 seconds)\n",
            "2024-01-06 23:17:08 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-01-06 23:17:08 | INFO | train | epoch 078 | loss 2.838 | nll_loss 2.267 | ppl 4.81 | wps 834 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 234 | lr 0.000234 | gnorm 1.52 | clip 66.7 | train_wall 9 | wall 840\n",
            "2024-01-06 23:17:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 079:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:17:08 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-01-06 23:17:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  67% 2/3 [00:05<00:02,  2.31s/it]2024-01-06 23:17:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:17:18 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 2.681 | nll_loss 1.979 | ppl 3.94 | wps 2770.3 | wpb 921 | bsz 75 | num_updates 237 | best_loss 2.676\n",
            "2024-01-06 23:17:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 237 updates\n",
            "2024-01-06 23:17:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 79 @ 237 updates, score 2.681) (writing took 0.17771453600016685 seconds)\n",
            "2024-01-06 23:17:18 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-01-06 23:17:18 | INFO | train | epoch 079 | loss 2.815 | nll_loss 2.231 | ppl 4.7 | wps 840.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 237 | lr 0.000237 | gnorm 1.756 | clip 66.7 | train_wall 10 | wall 851\n",
            "2024-01-06 23:17:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 080:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:17:18 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-01-06 23:17:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  67% 2/3 [00:04<00:02,  2.51s/it]2024-01-06 23:17:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.73it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:17:28 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 2.589 | nll_loss 1.873 | ppl 3.66 | wps 2993.9 | wpb 921 | bsz 75 | num_updates 240 | best_loss 2.589\n",
            "2024-01-06 23:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 240 updates\n",
            "2024-01-06 23:17:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 80 @ 240 updates, score 2.589) (writing took 0.3478225129997554 seconds)\n",
            "2024-01-06 23:17:29 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-01-06 23:17:29 | INFO | train | epoch 080 | loss 2.775 | nll_loss 2.187 | ppl 4.55 | wps 825.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 240 | lr 0.00024 | gnorm 1.511 | clip 33.3 | train_wall 10 | wall 861\n",
            "2024-01-06 23:17:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 081:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:17:29 | INFO | fairseq.trainer | begin training epoch 81\n",
            "2024-01-06 23:17:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 081:  67% 2/3 [00:09<00:04,  4.89s/it]2024-01-06 23:17:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 081 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:17:39 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 2.52 | nll_loss 1.814 | ppl 3.52 | wps 2779 | wpb 921 | bsz 75 | num_updates 243 | best_loss 2.52\n",
            "2024-01-06 23:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 243 updates\n",
            "2024-01-06 23:17:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 81 @ 243 updates, score 2.52) (writing took 0.38430520099973364 seconds)\n",
            "2024-01-06 23:17:39 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
            "2024-01-06 23:17:39 | INFO | train | epoch 081 | loss 2.737 | nll_loss 2.144 | ppl 4.42 | wps 814.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 243 | lr 0.000243 | gnorm 1.369 | clip 33.3 | train_wall 10 | wall 872\n",
            "2024-01-06 23:17:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 082:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:17:40 | INFO | fairseq.trainer | begin training epoch 82\n",
            "2024-01-06 23:17:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 082:  67% 2/3 [00:09<00:04,  4.83s/it]2024-01-06 23:17:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 082 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:17:50 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 2.5 | nll_loss 1.771 | ppl 3.41 | wps 2914.2 | wpb 921 | bsz 75 | num_updates 246 | best_loss 2.5\n",
            "2024-01-06 23:17:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 246 updates\n",
            "2024-01-06 23:17:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:17:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 82 @ 246 updates, score 2.5) (writing took 0.3753995409997515 seconds)\n",
            "2024-01-06 23:17:50 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
            "2024-01-06 23:17:50 | INFO | train | epoch 082 | loss 2.715 | nll_loss 2.131 | ppl 4.38 | wps 824.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 246 | lr 0.000246 | gnorm 1.452 | clip 33.3 | train_wall 10 | wall 883\n",
            "2024-01-06 23:17:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 083:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:17:50 | INFO | fairseq.trainer | begin training epoch 83\n",
            "2024-01-06 23:17:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 083:  67% 2/3 [00:09<00:04,  4.80s/it]2024-01-06 23:18:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 083 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:00 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 2.433 | nll_loss 1.705 | ppl 3.26 | wps 2855.8 | wpb 921 | bsz 75 | num_updates 249 | best_loss 2.433\n",
            "2024-01-06 23:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 249 updates\n",
            "2024-01-06 23:18:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 83 @ 249 updates, score 2.433) (writing took 0.3738354860001891 seconds)\n",
            "2024-01-06 23:18:01 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
            "2024-01-06 23:18:01 | INFO | train | epoch 083 | loss 2.67 | nll_loss 2.072 | ppl 4.21 | wps 831.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 249 | lr 0.000249 | gnorm 1.113 | clip 33.3 | train_wall 9 | wall 893\n",
            "2024-01-06 23:18:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 084:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:01 | INFO | fairseq.trainer | begin training epoch 84\n",
            "2024-01-06 23:18:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 084:  67% 2/3 [00:09<00:04,  4.60s/it]2024-01-06 23:18:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 084 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:11 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 2.488 | nll_loss 1.731 | ppl 3.32 | wps 2853.3 | wpb 921 | bsz 75 | num_updates 252 | best_loss 2.433\n",
            "2024-01-06 23:18:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 252 updates\n",
            "2024-01-06 23:18:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 84 @ 252 updates, score 2.488) (writing took 0.18214004300034503 seconds)\n",
            "2024-01-06 23:18:11 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
            "2024-01-06 23:18:11 | INFO | train | epoch 084 | loss 2.635 | nll_loss 2.037 | ppl 4.1 | wps 832.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 252 | lr 0.000252 | gnorm 1.491 | clip 33.3 | train_wall 9 | wall 904\n",
            "2024-01-06 23:18:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 085:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:11 | INFO | fairseq.trainer | begin training epoch 85\n",
            "2024-01-06 23:18:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 085:  67% 2/3 [00:05<00:02,  2.14s/it]2024-01-06 23:18:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 085 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.56it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:21 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 2.424 | nll_loss 1.648 | ppl 3.13 | wps 2204.6 | wpb 921 | bsz 75 | num_updates 255 | best_loss 2.424\n",
            "2024-01-06 23:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 255 updates\n",
            "2024-01-06 23:18:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 85 @ 255 updates, score 2.424) (writing took 0.4831270310000946 seconds)\n",
            "2024-01-06 23:18:22 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
            "2024-01-06 23:18:22 | INFO | train | epoch 085 | loss 2.605 | nll_loss 1.988 | ppl 3.97 | wps 841.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 255 | lr 0.000255 | gnorm 1.259 | clip 66.7 | train_wall 9 | wall 914\n",
            "2024-01-06 23:18:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 086:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:22 | INFO | fairseq.trainer | begin training epoch 86\n",
            "2024-01-06 23:18:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 086:  67% 2/3 [00:09<00:04,  4.76s/it]2024-01-06 23:18:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 086 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:32 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 2.456 | nll_loss 1.687 | ppl 3.22 | wps 2654.2 | wpb 921 | bsz 75 | num_updates 258 | best_loss 2.424\n",
            "2024-01-06 23:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 258 updates\n",
            "2024-01-06 23:18:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 86 @ 258 updates, score 2.456) (writing took 0.1881802930001868 seconds)\n",
            "2024-01-06 23:18:32 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
            "2024-01-06 23:18:32 | INFO | train | epoch 086 | loss 2.575 | nll_loss 1.951 | ppl 3.87 | wps 840 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 258 | lr 0.000258 | gnorm 1.16 | clip 33.3 | train_wall 9 | wall 925\n",
            "2024-01-06 23:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 087:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:32 | INFO | fairseq.trainer | begin training epoch 87\n",
            "2024-01-06 23:18:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 087:  67% 2/3 [00:05<00:02,  2.28s/it]2024-01-06 23:18:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 087 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:42 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 2.301 | nll_loss 1.529 | ppl 2.89 | wps 2775.4 | wpb 921 | bsz 75 | num_updates 261 | best_loss 2.301\n",
            "2024-01-06 23:18:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 261 updates\n",
            "2024-01-06 23:18:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:18:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 87 @ 261 updates, score 2.301) (writing took 0.3315826270004436 seconds)\n",
            "2024-01-06 23:18:43 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
            "2024-01-06 23:18:43 | INFO | train | epoch 087 | loss 2.559 | nll_loss 1.937 | ppl 3.83 | wps 829.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 261 | lr 0.000261 | gnorm 1.56 | clip 66.7 | train_wall 10 | wall 935\n",
            "2024-01-06 23:18:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 088:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:43 | INFO | fairseq.trainer | begin training epoch 88\n",
            "2024-01-06 23:18:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 088:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:18:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 088 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:18:53 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 2.382 | nll_loss 1.578 | ppl 2.99 | wps 2881.7 | wpb 921 | bsz 75 | num_updates 264 | best_loss 2.301\n",
            "2024-01-06 23:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 264 updates\n",
            "2024-01-06 23:18:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 88 @ 264 updates, score 2.382) (writing took 0.2080371520000881 seconds)\n",
            "2024-01-06 23:18:53 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
            "2024-01-06 23:18:53 | INFO | train | epoch 088 | loss 2.514 | nll_loss 1.891 | ppl 3.71 | wps 828.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 264 | lr 0.000264 | gnorm 1.609 | clip 66.7 | train_wall 10 | wall 946\n",
            "2024-01-06 23:18:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 089:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:18:53 | INFO | fairseq.trainer | begin training epoch 89\n",
            "2024-01-06 23:18:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 089:  67% 2/3 [00:04<00:02,  2.72s/it]2024-01-06 23:19:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 089 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:04 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 2.266 | nll_loss 1.454 | ppl 2.74 | wps 2770.9 | wpb 921 | bsz 75 | num_updates 267 | best_loss 2.266\n",
            "2024-01-06 23:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 267 updates\n",
            "2024-01-06 23:19:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 89 @ 267 updates, score 2.266) (writing took 0.4150023039996995 seconds)\n",
            "2024-01-06 23:19:04 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
            "2024-01-06 23:19:04 | INFO | train | epoch 089 | loss 2.52 | nll_loss 1.872 | ppl 3.66 | wps 821 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 267 | lr 0.000267 | gnorm 1.551 | clip 100 | train_wall 10 | wall 957\n",
            "2024-01-06 23:19:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 090:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:04 | INFO | fairseq.trainer | begin training epoch 90\n",
            "2024-01-06 23:19:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 090:  67% 2/3 [00:09<00:04,  4.54s/it]2024-01-06 23:19:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 090 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:14 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 2.183 | nll_loss 1.371 | ppl 2.59 | wps 2124.4 | wpb 921 | bsz 75 | num_updates 270 | best_loss 2.183\n",
            "2024-01-06 23:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 270 updates\n",
            "2024-01-06 23:19:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 90 @ 270 updates, score 2.183) (writing took 0.3817184750000706 seconds)\n",
            "2024-01-06 23:19:15 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
            "2024-01-06 23:19:15 | INFO | train | epoch 090 | loss 2.474 | nll_loss 1.837 | ppl 3.57 | wps 839 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 270 | lr 0.00027 | gnorm 1.592 | clip 66.7 | train_wall 9 | wall 967\n",
            "2024-01-06 23:19:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 091:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:15 | INFO | fairseq.trainer | begin training epoch 91\n",
            "2024-01-06 23:19:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 091:  67% 2/3 [00:09<00:04,  4.78s/it]2024-01-06 23:19:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 091 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:25 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 2.311 | nll_loss 1.48 | ppl 2.79 | wps 2957.4 | wpb 921 | bsz 75 | num_updates 273 | best_loss 2.183\n",
            "2024-01-06 23:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 273 updates\n",
            "2024-01-06 23:19:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 91 @ 273 updates, score 2.311) (writing took 0.19105213199964055 seconds)\n",
            "2024-01-06 23:19:25 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
            "2024-01-06 23:19:25 | INFO | train | epoch 091 | loss 2.441 | nll_loss 1.8 | ppl 3.48 | wps 826.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 273 | lr 0.000273 | gnorm 1.507 | clip 33.3 | train_wall 10 | wall 978\n",
            "2024-01-06 23:19:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 092:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:25 | INFO | fairseq.trainer | begin training epoch 92\n",
            "2024-01-06 23:19:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 092:  67% 2/3 [00:04<00:01,  1.81s/it]2024-01-06 23:19:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 092 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:35 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 2.172 | nll_loss 1.323 | ppl 2.5 | wps 2826.9 | wpb 921 | bsz 75 | num_updates 276 | best_loss 2.172\n",
            "2024-01-06 23:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 276 updates\n",
            "2024-01-06 23:19:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 92 @ 276 updates, score 2.172) (writing took 0.41808592400002453 seconds)\n",
            "2024-01-06 23:19:36 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
            "2024-01-06 23:19:36 | INFO | train | epoch 092 | loss 2.425 | nll_loss 1.764 | ppl 3.4 | wps 818.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 276 | lr 0.000276 | gnorm 1.455 | clip 66.7 | train_wall 10 | wall 989\n",
            "2024-01-06 23:19:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 093:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:36 | INFO | fairseq.trainer | begin training epoch 93\n",
            "2024-01-06 23:19:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 093:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:19:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 093 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:46 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 2.327 | nll_loss 1.486 | ppl 2.8 | wps 2929.3 | wpb 921 | bsz 75 | num_updates 279 | best_loss 2.172\n",
            "2024-01-06 23:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 279 updates\n",
            "2024-01-06 23:19:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 93 @ 279 updates, score 2.327) (writing took 0.1845442029998594 seconds)\n",
            "2024-01-06 23:19:46 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
            "2024-01-06 23:19:46 | INFO | train | epoch 093 | loss 2.404 | nll_loss 1.747 | ppl 3.36 | wps 840.2 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 279 | lr 0.000279 | gnorm 1.639 | clip 66.7 | train_wall 10 | wall 999\n",
            "2024-01-06 23:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 094:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:46 | INFO | fairseq.trainer | begin training epoch 94\n",
            "2024-01-06 23:19:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 094:  67% 2/3 [00:09<00:04,  4.64s/it]2024-01-06 23:19:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 094 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:19:57 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 2.112 | nll_loss 1.272 | ppl 2.42 | wps 1983.7 | wpb 921 | bsz 75 | num_updates 282 | best_loss 2.112\n",
            "2024-01-06 23:19:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 282 updates\n",
            "2024-01-06 23:19:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 94 @ 282 updates, score 2.112) (writing took 0.503808281000147 seconds)\n",
            "2024-01-06 23:19:57 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
            "2024-01-06 23:19:57 | INFO | train | epoch 094 | loss 2.396 | nll_loss 1.732 | ppl 3.32 | wps 819.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 282 | lr 0.000282 | gnorm 1.58 | clip 100 | train_wall 9 | wall 1010\n",
            "2024-01-06 23:19:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 095:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:19:57 | INFO | fairseq.trainer | begin training epoch 95\n",
            "2024-01-06 23:19:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 095:  67% 2/3 [00:05<00:02,  2.13s/it]2024-01-06 23:20:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 095 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:07 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 2.07 | nll_loss 1.224 | ppl 2.34 | wps 2791.4 | wpb 921 | bsz 75 | num_updates 285 | best_loss 2.07\n",
            "2024-01-06 23:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 285 updates\n",
            "2024-01-06 23:20:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 95 @ 285 updates, score 2.07) (writing took 0.45560245199976634 seconds)\n",
            "2024-01-06 23:20:07 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
            "2024-01-06 23:20:07 | INFO | train | epoch 095 | loss 2.39 | nll_loss 1.742 | ppl 3.35 | wps 854.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 285 | lr 0.000285 | gnorm 1.576 | clip 100 | train_wall 9 | wall 1020\n",
            "2024-01-06 23:20:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 096:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:20:07 | INFO | fairseq.trainer | begin training epoch 96\n",
            "2024-01-06 23:20:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 096:  67% 2/3 [00:09<00:04,  4.71s/it]2024-01-06 23:20:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 096 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:18 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 2.236 | nll_loss 1.375 | ppl 2.59 | wps 2922.7 | wpb 921 | bsz 75 | num_updates 288 | best_loss 2.07\n",
            "2024-01-06 23:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 288 updates\n",
            "2024-01-06 23:20:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 96 @ 288 updates, score 2.236) (writing took 0.18388956299986603 seconds)\n",
            "2024-01-06 23:20:18 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
            "2024-01-06 23:20:18 | INFO | train | epoch 096 | loss 2.359 | nll_loss 1.7 | ppl 3.25 | wps 848 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 288 | lr 0.000288 | gnorm 2.214 | clip 100 | train_wall 9 | wall 1030\n",
            "2024-01-06 23:20:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 097:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:20:18 | INFO | fairseq.trainer | begin training epoch 97\n",
            "2024-01-06 23:20:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 097:  67% 2/3 [00:09<00:04,  4.56s/it]2024-01-06 23:20:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 097 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:28 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 2.095 | nll_loss 1.216 | ppl 2.32 | wps 2878 | wpb 921 | bsz 75 | num_updates 291 | best_loss 2.07\n",
            "2024-01-06 23:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 291 updates\n",
            "2024-01-06 23:20:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 97 @ 291 updates, score 2.095) (writing took 0.18146802299997944 seconds)\n",
            "2024-01-06 23:20:28 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
            "2024-01-06 23:20:28 | INFO | train | epoch 097 | loss 2.37 | nll_loss 1.697 | ppl 3.24 | wps 850.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 291 | lr 0.000291 | gnorm 1.925 | clip 100 | train_wall 9 | wall 1041\n",
            "2024-01-06 23:20:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 098:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:20:28 | INFO | fairseq.trainer | begin training epoch 98\n",
            "2024-01-06 23:20:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 098:  67% 2/3 [00:04<00:01,  1.92s/it]2024-01-06 23:20:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 098 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:38 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 2.137 | nll_loss 1.239 | ppl 2.36 | wps 2903 | wpb 921 | bsz 75 | num_updates 294 | best_loss 2.07\n",
            "2024-01-06 23:20:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 294 updates\n",
            "2024-01-06 23:20:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:20:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 98 @ 294 updates, score 2.137) (writing took 0.2029665620002561 seconds)\n",
            "2024-01-06 23:20:38 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
            "2024-01-06 23:20:38 | INFO | train | epoch 098 | loss 2.314 | nll_loss 1.633 | ppl 3.1 | wps 847.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 294 | lr 0.000294 | gnorm 1.525 | clip 100 | train_wall 9 | wall 1051\n",
            "2024-01-06 23:20:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 099:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:20:38 | INFO | fairseq.trainer | begin training epoch 99\n",
            "2024-01-06 23:20:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 099:  67% 2/3 [00:05<00:02,  2.28s/it]2024-01-06 23:20:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 099 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:49 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 2.049 | nll_loss 1.162 | ppl 2.24 | wps 2770 | wpb 921 | bsz 75 | num_updates 297 | best_loss 2.049\n",
            "2024-01-06 23:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 297 updates\n",
            "2024-01-06 23:20:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 99 @ 297 updates, score 2.049) (writing took 0.3974343339996267 seconds)\n",
            "2024-01-06 23:20:49 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
            "2024-01-06 23:20:49 | INFO | train | epoch 099 | loss 2.243 | nll_loss 1.551 | ppl 2.93 | wps 827.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 297 | lr 0.000297 | gnorm 1.549 | clip 66.7 | train_wall 9 | wall 1062\n",
            "2024-01-06 23:20:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 100:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:20:49 | INFO | fairseq.trainer | begin training epoch 100\n",
            "2024-01-06 23:20:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 100:  67% 2/3 [00:04<00:02,  2.48s/it]2024-01-06 23:20:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 100 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:20:59 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 1.97 | nll_loss 1.077 | ppl 2.11 | wps 2533.4 | wpb 921 | bsz 75 | num_updates 300 | best_loss 1.97\n",
            "2024-01-06 23:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 300 updates\n",
            "2024-01-06 23:20:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:20:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 100 @ 300 updates, score 1.97) (writing took 0.33324901800006046 seconds)\n",
            "2024-01-06 23:21:00 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
            "2024-01-06 23:21:00 | INFO | train | epoch 100 | loss 2.261 | nll_loss 1.588 | ppl 3.01 | wps 830.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 300 | lr 0.0003 | gnorm 1.331 | clip 100 | train_wall 10 | wall 1072\n",
            "2024-01-06 23:21:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 101:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:00 | INFO | fairseq.trainer | begin training epoch 101\n",
            "2024-01-06 23:21:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 101:  67% 2/3 [00:05<00:02,  2.94s/it]2024-01-06 23:21:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 101 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:21:10 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 2.037 | nll_loss 1.121 | ppl 2.18 | wps 2892 | wpb 921 | bsz 75 | num_updates 303 | best_loss 1.97\n",
            "2024-01-06 23:21:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 303 updates\n",
            "2024-01-06 23:21:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 101 @ 303 updates, score 2.037) (writing took 0.18340888299962899 seconds)\n",
            "2024-01-06 23:21:10 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
            "2024-01-06 23:21:10 | INFO | train | epoch 101 | loss 2.214 | nll_loss 1.521 | ppl 2.87 | wps 848.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 303 | lr 0.000303 | gnorm 1.319 | clip 33.3 | train_wall 9 | wall 1083\n",
            "2024-01-06 23:21:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 102:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:10 | INFO | fairseq.trainer | begin training epoch 102\n",
            "2024-01-06 23:21:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 102:  67% 2/3 [00:04<00:01,  1.82s/it]2024-01-06 23:21:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 102 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:21:20 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 1.982 | nll_loss 1.061 | ppl 2.09 | wps 2847.4 | wpb 921 | bsz 75 | num_updates 306 | best_loss 1.97\n",
            "2024-01-06 23:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 306 updates\n",
            "2024-01-06 23:21:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 102 @ 306 updates, score 1.982) (writing took 0.1957952530001421 seconds)\n",
            "2024-01-06 23:21:21 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
            "2024-01-06 23:21:21 | INFO | train | epoch 102 | loss 2.188 | nll_loss 1.487 | ppl 2.8 | wps 829.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 306 | lr 0.000306 | gnorm 1.501 | clip 33.3 | train_wall 10 | wall 1093\n",
            "2024-01-06 23:21:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 103:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:21 | INFO | fairseq.trainer | begin training epoch 103\n",
            "2024-01-06 23:21:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 103:  67% 2/3 [00:04<00:01,  1.84s/it]2024-01-06 23:21:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 103 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:21:31 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 1.959 | nll_loss 1.031 | ppl 2.04 | wps 2081.2 | wpb 921 | bsz 75 | num_updates 309 | best_loss 1.959\n",
            "2024-01-06 23:21:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 309 updates\n",
            "2024-01-06 23:21:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:21:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 103 @ 309 updates, score 1.959) (writing took 0.9244265540000924 seconds)\n",
            "2024-01-06 23:21:32 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
            "2024-01-06 23:21:32 | INFO | train | epoch 103 | loss 2.16 | nll_loss 1.453 | ppl 2.74 | wps 768.5 | ups 0.26 | wpb 2928.3 | bsz 233.3 | num_updates 309 | lr 0.000309 | gnorm 1.21 | clip 66.7 | train_wall 10 | wall 1105\n",
            "2024-01-06 23:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 104:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:32 | INFO | fairseq.trainer | begin training epoch 104\n",
            "2024-01-06 23:21:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 104:  67% 2/3 [00:05<00:03,  3.03s/it]2024-01-06 23:21:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 104 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:21:42 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 1.901 | nll_loss 0.959 | ppl 1.94 | wps 2018 | wpb 921 | bsz 75 | num_updates 312 | best_loss 1.901\n",
            "2024-01-06 23:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 312 updates\n",
            "2024-01-06 23:21:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:21:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 104 @ 312 updates, score 1.901) (writing took 0.44128410299981624 seconds)\n",
            "2024-01-06 23:21:43 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
            "2024-01-06 23:21:43 | INFO | train | epoch 104 | loss 2.138 | nll_loss 1.428 | ppl 2.69 | wps 811 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 312 | lr 0.000312 | gnorm 1.178 | clip 33.3 | train_wall 9 | wall 1115\n",
            "2024-01-06 23:21:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 105:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:43 | INFO | fairseq.trainer | begin training epoch 105\n",
            "2024-01-06 23:21:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 105:  67% 2/3 [00:09<00:04,  4.52s/it]2024-01-06 23:21:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 105 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.56it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:21:53 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 1.907 | nll_loss 0.951 | ppl 1.93 | wps 2095.2 | wpb 921 | bsz 75 | num_updates 315 | best_loss 1.901\n",
            "2024-01-06 23:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 315 updates\n",
            "2024-01-06 23:21:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:21:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 105 @ 315 updates, score 1.907) (writing took 0.2667206900000565 seconds)\n",
            "2024-01-06 23:21:53 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
            "2024-01-06 23:21:53 | INFO | train | epoch 105 | loss 2.105 | nll_loss 1.389 | ppl 2.62 | wps 850.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 315 | lr 0.000315 | gnorm 1.033 | clip 33.3 | train_wall 9 | wall 1126\n",
            "2024-01-06 23:21:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 106:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:21:53 | INFO | fairseq.trainer | begin training epoch 106\n",
            "2024-01-06 23:21:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 106:  67% 2/3 [00:09<00:04,  4.58s/it]2024-01-06 23:22:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 106 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:03 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 1.87 | nll_loss 0.909 | ppl 1.88 | wps 2899.7 | wpb 921 | bsz 75 | num_updates 318 | best_loss 1.87\n",
            "2024-01-06 23:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 318 updates\n",
            "2024-01-06 23:22:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 106 @ 318 updates, score 1.87) (writing took 0.34777948700002526 seconds)\n",
            "2024-01-06 23:22:04 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
            "2024-01-06 23:22:04 | INFO | train | epoch 106 | loss 2.088 | nll_loss 1.365 | ppl 2.58 | wps 831.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 318 | lr 0.000318 | gnorm 1.123 | clip 33.3 | train_wall 9 | wall 1136\n",
            "2024-01-06 23:22:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 107:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:04 | INFO | fairseq.trainer | begin training epoch 107\n",
            "2024-01-06 23:22:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 107:  67% 2/3 [00:04<00:01,  1.96s/it]2024-01-06 23:22:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 107 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:14 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 1.869 | nll_loss 0.898 | ppl 1.86 | wps 2936.8 | wpb 921 | bsz 75 | num_updates 321 | best_loss 1.869\n",
            "2024-01-06 23:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 321 updates\n",
            "2024-01-06 23:22:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 107 @ 321 updates, score 1.869) (writing took 0.33751683699983914 seconds)\n",
            "2024-01-06 23:22:14 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
            "2024-01-06 23:22:14 | INFO | train | epoch 107 | loss 2.055 | nll_loss 1.329 | ppl 2.51 | wps 823.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 321 | lr 0.000321 | gnorm 1.211 | clip 33.3 | train_wall 10 | wall 1147\n",
            "2024-01-06 23:22:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 108:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:14 | INFO | fairseq.trainer | begin training epoch 108\n",
            "2024-01-06 23:22:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 108:  67% 2/3 [00:09<00:04,  4.71s/it]2024-01-06 23:22:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 108 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:25 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 1.822 | nll_loss 0.857 | ppl 1.81 | wps 2142.3 | wpb 921 | bsz 75 | num_updates 324 | best_loss 1.822\n",
            "2024-01-06 23:22:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 324 updates\n",
            "2024-01-06 23:22:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 108 @ 324 updates, score 1.822) (writing took 0.3658308659996692 seconds)\n",
            "2024-01-06 23:22:25 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
            "2024-01-06 23:22:25 | INFO | train | epoch 108 | loss 2.041 | nll_loss 1.313 | ppl 2.48 | wps 833.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 324 | lr 0.000324 | gnorm 1.041 | clip 33.3 | train_wall 9 | wall 1158\n",
            "2024-01-06 23:22:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 109:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:25 | INFO | fairseq.trainer | begin training epoch 109\n",
            "2024-01-06 23:22:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 109:  67% 2/3 [00:05<00:03,  3.23s/it]2024-01-06 23:22:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 109 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:35 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 1.787 | nll_loss 0.807 | ppl 1.75 | wps 2850.8 | wpb 921 | bsz 75 | num_updates 327 | best_loss 1.787\n",
            "2024-01-06 23:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 327 updates\n",
            "2024-01-06 23:22:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 109 @ 327 updates, score 1.787) (writing took 0.3169840179998573 seconds)\n",
            "2024-01-06 23:22:36 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
            "2024-01-06 23:22:36 | INFO | train | epoch 109 | loss 2.046 | nll_loss 1.32 | ppl 2.5 | wps 822.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 327 | lr 0.000327 | gnorm 1.304 | clip 66.7 | train_wall 10 | wall 1168\n",
            "2024-01-06 23:22:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 110:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:36 | INFO | fairseq.trainer | begin training epoch 110\n",
            "2024-01-06 23:22:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 110:  67% 2/3 [00:05<00:02,  2.31s/it]2024-01-06 23:22:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 110 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:46 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 1.791 | nll_loss 0.794 | ppl 1.73 | wps 2914.7 | wpb 921 | bsz 75 | num_updates 330 | best_loss 1.787\n",
            "2024-01-06 23:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 330 updates\n",
            "2024-01-06 23:22:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 110 @ 330 updates, score 1.791) (writing took 0.1771790630000396 seconds)\n",
            "2024-01-06 23:22:46 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
            "2024-01-06 23:22:46 | INFO | train | epoch 110 | loss 2.001 | nll_loss 1.264 | ppl 2.4 | wps 833.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 330 | lr 0.00033 | gnorm 1.3 | clip 33.3 | train_wall 10 | wall 1179\n",
            "2024-01-06 23:22:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 111:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:46 | INFO | fairseq.trainer | begin training epoch 111\n",
            "2024-01-06 23:22:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 111:  67% 2/3 [00:05<00:03,  3.06s/it]2024-01-06 23:22:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 111 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:22:57 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 1.768 | nll_loss 0.764 | ppl 1.7 | wps 2652.4 | wpb 921 | bsz 75 | num_updates 333 | best_loss 1.768\n",
            "2024-01-06 23:22:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 333 updates\n",
            "2024-01-06 23:22:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:22:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 111 @ 333 updates, score 1.768) (writing took 0.3230221779999738 seconds)\n",
            "2024-01-06 23:22:57 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
            "2024-01-06 23:22:57 | INFO | train | epoch 111 | loss 1.99 | nll_loss 1.241 | ppl 2.36 | wps 793.5 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 333 | lr 0.000333 | gnorm 1.215 | clip 33.3 | train_wall 10 | wall 1190\n",
            "2024-01-06 23:22:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 112:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:22:57 | INFO | fairseq.trainer | begin training epoch 112\n",
            "2024-01-06 23:22:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 112:  67% 2/3 [00:04<00:02,  2.54s/it]2024-01-06 23:23:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 112 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:23:08 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 1.777 | nll_loss 0.774 | ppl 1.71 | wps 2967 | wpb 921 | bsz 75 | num_updates 336 | best_loss 1.768\n",
            "2024-01-06 23:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 336 updates\n",
            "2024-01-06 23:23:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 112 @ 336 updates, score 1.777) (writing took 0.1924683720003486 seconds)\n",
            "2024-01-06 23:23:08 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
            "2024-01-06 23:23:08 | INFO | train | epoch 112 | loss 1.95 | nll_loss 1.199 | ppl 2.3 | wps 834.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 336 | lr 0.000336 | gnorm 1.495 | clip 33.3 | train_wall 10 | wall 1200\n",
            "2024-01-06 23:23:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 113:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:23:08 | INFO | fairseq.trainer | begin training epoch 113\n",
            "2024-01-06 23:23:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 113:  67% 2/3 [00:09<00:04,  4.76s/it]2024-01-06 23:23:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 113 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:23:18 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 1.735 | nll_loss 0.731 | ppl 1.66 | wps 1969.7 | wpb 921 | bsz 75 | num_updates 339 | best_loss 1.735\n",
            "2024-01-06 23:23:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 339 updates\n",
            "2024-01-06 23:23:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:23:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:23:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 113 @ 339 updates, score 1.735) (writing took 0.37871555499987153 seconds)\n",
            "2024-01-06 23:23:18 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
            "2024-01-06 23:23:18 | INFO | train | epoch 113 | loss 1.941 | nll_loss 1.192 | ppl 2.29 | wps 833 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 339 | lr 0.000339 | gnorm 1.12 | clip 33.3 | train_wall 9 | wall 1211\n",
            "2024-01-06 23:23:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 114:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:23:18 | INFO | fairseq.trainer | begin training epoch 114\n",
            "2024-01-06 23:23:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 114:  67% 2/3 [00:09<00:04,  4.63s/it]2024-01-06 23:23:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 114 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.43it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:23:29 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 1.832 | nll_loss 0.827 | ppl 1.77 | wps 2448.3 | wpb 921 | bsz 75 | num_updates 342 | best_loss 1.735\n",
            "2024-01-06 23:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 342 updates\n",
            "2024-01-06 23:23:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 114 @ 342 updates, score 1.832) (writing took 0.1770391430000018 seconds)\n",
            "2024-01-06 23:23:29 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
            "2024-01-06 23:23:29 | INFO | train | epoch 114 | loss 1.914 | nll_loss 1.165 | ppl 2.24 | wps 834 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 342 | lr 0.000342 | gnorm 1.125 | clip 33.3 | train_wall 10 | wall 1221\n",
            "2024-01-06 23:23:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 115:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:23:29 | INFO | fairseq.trainer | begin training epoch 115\n",
            "2024-01-06 23:23:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 115:  67% 2/3 [00:04<00:01,  1.82s/it]2024-01-06 23:23:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 115 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:23:39 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 1.7 | nll_loss 0.681 | ppl 1.6 | wps 2282.6 | wpb 921 | bsz 75 | num_updates 345 | best_loss 1.7\n",
            "2024-01-06 23:23:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 345 updates\n",
            "2024-01-06 23:23:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:23:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:23:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 115 @ 345 updates, score 1.7) (writing took 0.47173414200005936 seconds)\n",
            "2024-01-06 23:23:39 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
            "2024-01-06 23:23:39 | INFO | train | epoch 115 | loss 1.894 | nll_loss 1.135 | ppl 2.2 | wps 831 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 345 | lr 0.000345 | gnorm 1.133 | clip 33.3 | train_wall 9 | wall 1232\n",
            "2024-01-06 23:23:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 116:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:23:39 | INFO | fairseq.trainer | begin training epoch 116\n",
            "2024-01-06 23:23:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 116:  67% 2/3 [00:09<00:04,  4.56s/it]2024-01-06 23:23:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 116 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:23:50 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 1.779 | nll_loss 0.755 | ppl 1.69 | wps 2828.9 | wpb 921 | bsz 75 | num_updates 348 | best_loss 1.7\n",
            "2024-01-06 23:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 348 updates\n",
            "2024-01-06 23:23:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 116 @ 348 updates, score 1.779) (writing took 0.16967801300006613 seconds)\n",
            "2024-01-06 23:23:50 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
            "2024-01-06 23:23:50 | INFO | train | epoch 116 | loss 1.883 | nll_loss 1.127 | ppl 2.18 | wps 850.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 348 | lr 0.000348 | gnorm 1.329 | clip 33.3 | train_wall 9 | wall 1242\n",
            "2024-01-06 23:23:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 117:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:23:50 | INFO | fairseq.trainer | begin training epoch 117\n",
            "2024-01-06 23:23:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 117:  67% 2/3 [00:05<00:03,  3.22s/it]2024-01-06 23:23:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 117 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:00 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 1.663 | nll_loss 0.653 | ppl 1.57 | wps 2809.5 | wpb 921 | bsz 75 | num_updates 351 | best_loss 1.663\n",
            "2024-01-06 23:24:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 351 updates\n",
            "2024-01-06 23:24:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:24:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 117 @ 351 updates, score 1.663) (writing took 0.32744792699986647 seconds)\n",
            "2024-01-06 23:24:00 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
            "2024-01-06 23:24:00 | INFO | train | epoch 117 | loss 1.93 | nll_loss 1.165 | ppl 2.24 | wps 823.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 351 | lr 0.000351 | gnorm 1.589 | clip 100 | train_wall 10 | wall 1253\n",
            "2024-01-06 23:24:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 118:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:00 | INFO | fairseq.trainer | begin training epoch 118\n",
            "2024-01-06 23:24:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 118:  67% 2/3 [00:10<00:05,  5.41s/it]2024-01-06 23:24:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 118 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  50% 1/2 [00:01<00:01,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:13 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 1.725 | nll_loss 0.697 | ppl 1.62 | wps 1112.9 | wpb 921 | bsz 75 | num_updates 354 | best_loss 1.663\n",
            "2024-01-06 23:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 354 updates\n",
            "2024-01-06 23:24:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 118 @ 354 updates, score 1.725) (writing took 0.5840540780000083 seconds)\n",
            "2024-01-06 23:24:14 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
            "2024-01-06 23:24:14 | INFO | train | epoch 118 | loss 1.899 | nll_loss 1.147 | ppl 2.21 | wps 649.8 | ups 0.22 | wpb 2928.3 | bsz 233.3 | num_updates 354 | lr 0.000354 | gnorm 1.59 | clip 100 | train_wall 11 | wall 1267\n",
            "2024-01-06 23:24:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 119:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:14 | INFO | fairseq.trainer | begin training epoch 119\n",
            "2024-01-06 23:24:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 119:  67% 2/3 [00:06<00:02,  2.51s/it]2024-01-06 23:24:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 119 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:25 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 1.71 | nll_loss 0.686 | ppl 1.61 | wps 2785.6 | wpb 921 | bsz 75 | num_updates 357 | best_loss 1.663\n",
            "2024-01-06 23:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 357 updates\n",
            "2024-01-06 23:24:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 119 @ 357 updates, score 1.71) (writing took 0.2936913989997265 seconds)\n",
            "2024-01-06 23:24:25 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
            "2024-01-06 23:24:25 | INFO | train | epoch 119 | loss 1.867 | nll_loss 1.099 | ppl 2.14 | wps 789.8 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 357 | lr 0.000357 | gnorm 1.233 | clip 66.7 | train_wall 10 | wall 1278\n",
            "2024-01-06 23:24:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 120:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:25 | INFO | fairseq.trainer | begin training epoch 120\n",
            "2024-01-06 23:24:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 120:  67% 2/3 [00:09<00:04,  4.83s/it]2024-01-06 23:24:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 120 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:35 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 1.675 | nll_loss 0.64 | ppl 1.56 | wps 2938.2 | wpb 921 | bsz 75 | num_updates 360 | best_loss 1.663\n",
            "2024-01-06 23:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 360 updates\n",
            "2024-01-06 23:24:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 120 @ 360 updates, score 1.675) (writing took 0.19093534300009196 seconds)\n",
            "2024-01-06 23:24:36 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
            "2024-01-06 23:24:36 | INFO | train | epoch 120 | loss 1.841 | nll_loss 1.073 | ppl 2.1 | wps 827.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 360 | lr 0.00036 | gnorm 1.397 | clip 66.7 | train_wall 10 | wall 1288\n",
            "2024-01-06 23:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 121:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:36 | INFO | fairseq.trainer | begin training epoch 121\n",
            "2024-01-06 23:24:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 121:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:24:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 121 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:46 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 1.734 | nll_loss 0.689 | ppl 1.61 | wps 2816.6 | wpb 921 | bsz 75 | num_updates 363 | best_loss 1.663\n",
            "2024-01-06 23:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 363 updates\n",
            "2024-01-06 23:24:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:24:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 121 @ 363 updates, score 1.734) (writing took 0.17552597300027628 seconds)\n",
            "2024-01-06 23:24:46 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
            "2024-01-06 23:24:46 | INFO | train | epoch 121 | loss 1.853 | nll_loss 1.083 | ppl 2.12 | wps 832.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 363 | lr 0.000363 | gnorm 1.527 | clip 100 | train_wall 10 | wall 1299\n",
            "2024-01-06 23:24:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 122:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:46 | INFO | fairseq.trainer | begin training epoch 122\n",
            "2024-01-06 23:24:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 122:  67% 2/3 [00:09<00:04,  4.88s/it]2024-01-06 23:24:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 122 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:24:57 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 1.64 | nll_loss 0.593 | ppl 1.51 | wps 2842.5 | wpb 921 | bsz 75 | num_updates 366 | best_loss 1.64\n",
            "2024-01-06 23:24:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 366 updates\n",
            "2024-01-06 23:24:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:24:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:24:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 122 @ 366 updates, score 1.64) (writing took 0.3881790440000259 seconds)\n",
            "2024-01-06 23:24:57 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
            "2024-01-06 23:24:57 | INFO | train | epoch 122 | loss 1.837 | nll_loss 1.051 | ppl 2.07 | wps 808.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 366 | lr 0.000366 | gnorm 1.164 | clip 66.7 | train_wall 10 | wall 1310\n",
            "2024-01-06 23:24:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 123:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:24:57 | INFO | fairseq.trainer | begin training epoch 123\n",
            "2024-01-06 23:24:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 123:  67% 2/3 [00:09<00:04,  4.71s/it]2024-01-06 23:25:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 123 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:25:08 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 1.709 | nll_loss 0.673 | ppl 1.59 | wps 2762.3 | wpb 921 | bsz 75 | num_updates 369 | best_loss 1.64\n",
            "2024-01-06 23:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 369 updates\n",
            "2024-01-06 23:25:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 123 @ 369 updates, score 1.709) (writing took 0.16939028300021164 seconds)\n",
            "2024-01-06 23:25:08 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
            "2024-01-06 23:25:08 | INFO | train | epoch 123 | loss 1.798 | nll_loss 1.019 | ppl 2.03 | wps 825.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 369 | lr 0.000369 | gnorm 1.183 | clip 33.3 | train_wall 10 | wall 1320\n",
            "2024-01-06 23:25:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 124:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:25:08 | INFO | fairseq.trainer | begin training epoch 124\n",
            "2024-01-06 23:25:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 124:  67% 2/3 [00:04<00:01,  1.79s/it]2024-01-06 23:25:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 124 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:25:18 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 1.603 | nll_loss 0.57 | ppl 1.48 | wps 2792 | wpb 921 | bsz 75 | num_updates 372 | best_loss 1.603\n",
            "2024-01-06 23:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 372 updates\n",
            "2024-01-06 23:25:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:25:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 124 @ 372 updates, score 1.603) (writing took 0.3299498480000693 seconds)\n",
            "2024-01-06 23:25:18 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
            "2024-01-06 23:25:18 | INFO | train | epoch 124 | loss 1.759 | nll_loss 0.973 | ppl 1.96 | wps 826.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 372 | lr 0.000372 | gnorm 1.167 | clip 33.3 | train_wall 10 | wall 1331\n",
            "2024-01-06 23:25:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 125:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:25:18 | INFO | fairseq.trainer | begin training epoch 125\n",
            "2024-01-06 23:25:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 125:  67% 2/3 [00:09<00:04,  4.93s/it]2024-01-06 23:25:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 125 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:25:29 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 1.629 | nll_loss 0.587 | ppl 1.5 | wps 2861 | wpb 921 | bsz 75 | num_updates 375 | best_loss 1.603\n",
            "2024-01-06 23:25:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 375 updates\n",
            "2024-01-06 23:25:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 125 @ 375 updates, score 1.629) (writing took 0.17400400299993635 seconds)\n",
            "2024-01-06 23:25:29 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
            "2024-01-06 23:25:29 | INFO | train | epoch 125 | loss 1.731 | nll_loss 0.953 | ppl 1.94 | wps 826.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 375 | lr 0.000375 | gnorm 1.195 | clip 33.3 | train_wall 10 | wall 1342\n",
            "2024-01-06 23:25:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 126:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:25:29 | INFO | fairseq.trainer | begin training epoch 126\n",
            "2024-01-06 23:25:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 126:  67% 2/3 [00:04<00:02,  2.52s/it]2024-01-06 23:25:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 126 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:25:39 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 1.565 | nll_loss 0.512 | ppl 1.43 | wps 2855.2 | wpb 921 | bsz 75 | num_updates 378 | best_loss 1.565\n",
            "2024-01-06 23:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 378 updates\n",
            "2024-01-06 23:25:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:25:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:25:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 126 @ 378 updates, score 1.565) (writing took 0.3997387949998483 seconds)\n",
            "2024-01-06 23:25:40 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
            "2024-01-06 23:25:40 | INFO | train | epoch 126 | loss 1.752 | nll_loss 0.966 | ppl 1.95 | wps 816.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 378 | lr 0.000378 | gnorm 1.153 | clip 66.7 | train_wall 10 | wall 1352\n",
            "2024-01-06 23:25:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 127:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:25:40 | INFO | fairseq.trainer | begin training epoch 127\n",
            "2024-01-06 23:25:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 127:  67% 2/3 [00:09<00:04,  4.84s/it]2024-01-06 23:25:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 127 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:25:50 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 1.69 | nll_loss 0.637 | ppl 1.56 | wps 2908.9 | wpb 921 | bsz 75 | num_updates 381 | best_loss 1.565\n",
            "2024-01-06 23:25:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 381 updates\n",
            "2024-01-06 23:25:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:25:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 127 @ 381 updates, score 1.69) (writing took 0.18058250400008546 seconds)\n",
            "2024-01-06 23:25:50 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
            "2024-01-06 23:25:50 | INFO | train | epoch 127 | loss 1.757 | nll_loss 0.966 | ppl 1.95 | wps 825.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 381 | lr 0.000381 | gnorm 1.117 | clip 66.7 | train_wall 10 | wall 1363\n",
            "2024-01-06 23:25:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 128:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:25:50 | INFO | fairseq.trainer | begin training epoch 128\n",
            "2024-01-06 23:25:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 128:  67% 2/3 [00:05<00:02,  2.13s/it]2024-01-06 23:26:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 128 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.40it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:00 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 1.624 | nll_loss 0.579 | ppl 1.49 | wps 2259.5 | wpb 921 | bsz 75 | num_updates 384 | best_loss 1.565\n",
            "2024-01-06 23:26:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 384 updates\n",
            "2024-01-06 23:26:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 128 @ 384 updates, score 1.624) (writing took 0.3208849980001105 seconds)\n",
            "2024-01-06 23:26:01 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
            "2024-01-06 23:26:01 | INFO | train | epoch 128 | loss 1.73 | nll_loss 0.936 | ppl 1.91 | wps 850.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 384 | lr 0.000384 | gnorm 1.118 | clip 66.7 | train_wall 9 | wall 1373\n",
            "2024-01-06 23:26:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 129:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:01 | INFO | fairseq.trainer | begin training epoch 129\n",
            "2024-01-06 23:26:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 129:  67% 2/3 [00:04<00:01,  1.88s/it]2024-01-06 23:26:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 129 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:11 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 1.576 | nll_loss 0.52 | ppl 1.43 | wps 2900.2 | wpb 921 | bsz 75 | num_updates 387 | best_loss 1.565\n",
            "2024-01-06 23:26:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 387 updates\n",
            "2024-01-06 23:26:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 129 @ 387 updates, score 1.576) (writing took 0.2798016789997746 seconds)\n",
            "2024-01-06 23:26:11 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
            "2024-01-06 23:26:11 | INFO | train | epoch 129 | loss 1.707 | nll_loss 0.919 | ppl 1.89 | wps 844.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 387 | lr 0.000387 | gnorm 1.007 | clip 33.3 | train_wall 9 | wall 1384\n",
            "2024-01-06 23:26:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 130:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:11 | INFO | fairseq.trainer | begin training epoch 130\n",
            "2024-01-06 23:26:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 130:  67% 2/3 [00:09<00:04,  4.57s/it]2024-01-06 23:26:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 130 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:21 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 1.561 | nll_loss 0.499 | ppl 1.41 | wps 2814.7 | wpb 921 | bsz 75 | num_updates 390 | best_loss 1.561\n",
            "2024-01-06 23:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 390 updates\n",
            "2024-01-06 23:26:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:26:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:26:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 130 @ 390 updates, score 1.561) (writing took 0.3947312939999392 seconds)\n",
            "2024-01-06 23:26:22 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
            "2024-01-06 23:26:22 | INFO | train | epoch 130 | loss 1.683 | nll_loss 0.89 | ppl 1.85 | wps 829.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 390 | lr 0.00039 | gnorm 1.361 | clip 33.3 | train_wall 9 | wall 1394\n",
            "2024-01-06 23:26:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 131:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:22 | INFO | fairseq.trainer | begin training epoch 131\n",
            "2024-01-06 23:26:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 131:  67% 2/3 [00:04<00:02,  2.74s/it]2024-01-06 23:26:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 131 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 131 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:32 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 1.527 | nll_loss 0.463 | ppl 1.38 | wps 2760.6 | wpb 921 | bsz 75 | num_updates 393 | best_loss 1.527\n",
            "2024-01-06 23:26:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 393 updates\n",
            "2024-01-06 23:26:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:26:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 131 @ 393 updates, score 1.527) (writing took 0.341988216000118 seconds)\n",
            "2024-01-06 23:26:32 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)\n",
            "2024-01-06 23:26:32 | INFO | train | epoch 131 | loss 1.702 | nll_loss 0.905 | ppl 1.87 | wps 820.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 393 | lr 0.000393 | gnorm 1.051 | clip 33.3 | train_wall 10 | wall 1405\n",
            "2024-01-06 23:26:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 132:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:32 | INFO | fairseq.trainer | begin training epoch 132\n",
            "2024-01-06 23:26:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 132:  67% 2/3 [00:04<00:02,  2.72s/it]2024-01-06 23:26:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 132 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 132 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:43 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 1.547 | nll_loss 0.482 | ppl 1.4 | wps 2820.5 | wpb 921 | bsz 75 | num_updates 396 | best_loss 1.527\n",
            "2024-01-06 23:26:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 396 updates\n",
            "2024-01-06 23:26:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 132 @ 396 updates, score 1.547) (writing took 0.20119747199987614 seconds)\n",
            "2024-01-06 23:26:43 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)\n",
            "2024-01-06 23:26:43 | INFO | train | epoch 132 | loss 1.678 | nll_loss 0.881 | ppl 1.84 | wps 841.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 396 | lr 0.000396 | gnorm 1.138 | clip 33.3 | train_wall 10 | wall 1415\n",
            "2024-01-06 23:26:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 133:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:43 | INFO | fairseq.trainer | begin training epoch 133\n",
            "2024-01-06 23:26:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 133:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:26:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 133 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 133 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:26:53 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 1.566 | nll_loss 0.503 | ppl 1.42 | wps 2806.2 | wpb 921 | bsz 75 | num_updates 399 | best_loss 1.527\n",
            "2024-01-06 23:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 399 updates\n",
            "2024-01-06 23:26:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:26:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 133 @ 399 updates, score 1.566) (writing took 0.19154282199997397 seconds)\n",
            "2024-01-06 23:26:53 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)\n",
            "2024-01-06 23:26:53 | INFO | train | epoch 133 | loss 1.648 | nll_loss 0.846 | ppl 1.8 | wps 832.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 399 | lr 0.000399 | gnorm 1.166 | clip 33.3 | train_wall 10 | wall 1426\n",
            "2024-01-06 23:26:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 134:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:26:53 | INFO | fairseq.trainer | begin training epoch 134\n",
            "2024-01-06 23:26:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 134:  67% 2/3 [00:05<00:02,  3.00s/it, loss=1.902, nll_loss=1.145, ppl=2.21, wps=817, ups=0.28, wpb=2900.1, bsz=231, num_updates=400, lr=0.0004, gnorm=1.238, clip=48, train_wall=317, wall=1426]2024-01-06 23:27:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 134 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 134 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:04 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 1.541 | nll_loss 0.48 | ppl 1.4 | wps 2708 | wpb 921 | bsz 75 | num_updates 402 | best_loss 1.527\n",
            "2024-01-06 23:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 402 updates\n",
            "2024-01-06 23:27:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 134 @ 402 updates, score 1.541) (writing took 0.170710703000168 seconds)\n",
            "2024-01-06 23:27:04 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)\n",
            "2024-01-06 23:27:04 | INFO | train | epoch 134 | loss 1.661 | nll_loss 0.863 | ppl 1.82 | wps 838.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 402 | lr 0.000402 | gnorm 1.012 | clip 33.3 | train_wall 10 | wall 1437\n",
            "2024-01-06 23:27:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 135:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:04 | INFO | fairseq.trainer | begin training epoch 135\n",
            "2024-01-06 23:27:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 135:  67% 2/3 [00:04<00:02,  2.49s/it]2024-01-06 23:27:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 135 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 135 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:14 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 1.526 | nll_loss 0.454 | ppl 1.37 | wps 2831.5 | wpb 921 | bsz 75 | num_updates 405 | best_loss 1.526\n",
            "2024-01-06 23:27:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 405 updates\n",
            "2024-01-06 23:27:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:27:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 135 @ 405 updates, score 1.526) (writing took 0.33563054600017495 seconds)\n",
            "2024-01-06 23:27:14 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)\n",
            "2024-01-06 23:27:14 | INFO | train | epoch 135 | loss 1.64 | nll_loss 0.839 | ppl 1.79 | wps 824.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 405 | lr 0.000405 | gnorm 1.006 | clip 33.3 | train_wall 10 | wall 1447\n",
            "2024-01-06 23:27:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 136:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:15 | INFO | fairseq.trainer | begin training epoch 136\n",
            "2024-01-06 23:27:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 136:  67% 2/3 [00:09<00:04,  4.74s/it]2024-01-06 23:27:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 136 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 136 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:25 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 1.58 | nll_loss 0.523 | ppl 1.44 | wps 2974.8 | wpb 921 | bsz 75 | num_updates 408 | best_loss 1.526\n",
            "2024-01-06 23:27:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 408 updates\n",
            "2024-01-06 23:27:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 136 @ 408 updates, score 1.58) (writing took 0.19095549199982997 seconds)\n",
            "2024-01-06 23:27:25 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)\n",
            "2024-01-06 23:27:25 | INFO | train | epoch 136 | loss 1.627 | nll_loss 0.825 | ppl 1.77 | wps 831.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 408 | lr 0.000408 | gnorm 0.929 | clip 33.3 | train_wall 10 | wall 1458\n",
            "2024-01-06 23:27:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 137:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:25 | INFO | fairseq.trainer | begin training epoch 137\n",
            "2024-01-06 23:27:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 137:  67% 2/3 [00:09<00:04,  4.74s/it]2024-01-06 23:27:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 137 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 137 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:35 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 1.486 | nll_loss 0.406 | ppl 1.33 | wps 2759 | wpb 921 | bsz 75 | num_updates 411 | best_loss 1.486\n",
            "2024-01-06 23:27:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 411 updates\n",
            "2024-01-06 23:27:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 137 @ 411 updates, score 1.486) (writing took 0.36730946499983474 seconds)\n",
            "2024-01-06 23:27:36 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)\n",
            "2024-01-06 23:27:36 | INFO | train | epoch 137 | loss 1.641 | nll_loss 0.834 | ppl 1.78 | wps 818.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 411 | lr 0.000411 | gnorm 0.879 | clip 0 | train_wall 9 | wall 1468\n",
            "2024-01-06 23:27:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 138:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:36 | INFO | fairseq.trainer | begin training epoch 138\n",
            "2024-01-06 23:27:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 138:  67% 2/3 [00:05<00:02,  2.15s/it]2024-01-06 23:27:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 138 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 138 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:46 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 1.509 | nll_loss 0.425 | ppl 1.34 | wps 2144.1 | wpb 921 | bsz 75 | num_updates 414 | best_loss 1.486\n",
            "2024-01-06 23:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 414 updates\n",
            "2024-01-06 23:27:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 138 @ 414 updates, score 1.509) (writing took 0.27459200800012695 seconds)\n",
            "2024-01-06 23:27:46 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)\n",
            "2024-01-06 23:27:46 | INFO | train | epoch 138 | loss 1.602 | nll_loss 0.79 | ppl 1.73 | wps 849.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 414 | lr 0.000414 | gnorm 0.826 | clip 0 | train_wall 9 | wall 1479\n",
            "2024-01-06 23:27:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 139:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:46 | INFO | fairseq.trainer | begin training epoch 139\n",
            "2024-01-06 23:27:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 139:  67% 2/3 [00:05<00:02,  2.23s/it]2024-01-06 23:27:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 139 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 139 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:27:56 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 1.55 | nll_loss 0.479 | ppl 1.39 | wps 2826.4 | wpb 921 | bsz 75 | num_updates 417 | best_loss 1.486\n",
            "2024-01-06 23:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 417 updates\n",
            "2024-01-06 23:27:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:27:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 139 @ 417 updates, score 1.55) (writing took 0.18999885199991695 seconds)\n",
            "2024-01-06 23:27:57 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)\n",
            "2024-01-06 23:27:57 | INFO | train | epoch 139 | loss 1.606 | nll_loss 0.789 | ppl 1.73 | wps 846.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 417 | lr 0.000417 | gnorm 0.778 | clip 33.3 | train_wall 9 | wall 1489\n",
            "2024-01-06 23:27:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 140:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:27:57 | INFO | fairseq.trainer | begin training epoch 140\n",
            "2024-01-06 23:27:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 140:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:28:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 140 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 140 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:28:07 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 1.464 | nll_loss 0.403 | ppl 1.32 | wps 2745.3 | wpb 921 | bsz 75 | num_updates 420 | best_loss 1.464\n",
            "2024-01-06 23:28:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 420 updates\n",
            "2024-01-06 23:28:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 140 @ 420 updates, score 1.464) (writing took 0.7864232370002355 seconds)\n",
            "2024-01-06 23:28:08 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)\n",
            "2024-01-06 23:28:08 | INFO | train | epoch 140 | loss 1.598 | nll_loss 0.786 | ppl 1.72 | wps 791.9 | ups 0.27 | wpb 2928.3 | bsz 233.3 | num_updates 420 | lr 0.00042 | gnorm 0.848 | clip 33.3 | train_wall 10 | wall 1500\n",
            "2024-01-06 23:28:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 141:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:28:08 | INFO | fairseq.trainer | begin training epoch 141\n",
            "2024-01-06 23:28:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 141:  67% 2/3 [00:04<00:01,  1.98s/it]2024-01-06 23:28:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 141 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 141 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:28:18 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 1.515 | nll_loss 0.453 | ppl 1.37 | wps 2711.9 | wpb 921 | bsz 75 | num_updates 423 | best_loss 1.464\n",
            "2024-01-06 23:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 423 updates\n",
            "2024-01-06 23:28:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 141 @ 423 updates, score 1.515) (writing took 0.2166053800001464 seconds)\n",
            "2024-01-06 23:28:18 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)\n",
            "2024-01-06 23:28:18 | INFO | train | epoch 141 | loss 1.617 | nll_loss 0.817 | ppl 1.76 | wps 829 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 423 | lr 0.000423 | gnorm 1.161 | clip 66.7 | train_wall 10 | wall 1511\n",
            "2024-01-06 23:28:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 142:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:28:18 | INFO | fairseq.trainer | begin training epoch 142\n",
            "2024-01-06 23:28:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 142:  67% 2/3 [00:09<00:04,  4.74s/it]2024-01-06 23:28:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 142 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 142 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:28:28 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 1.457 | nll_loss 0.386 | ppl 1.31 | wps 2910.1 | wpb 921 | bsz 75 | num_updates 426 | best_loss 1.457\n",
            "2024-01-06 23:28:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 426 updates\n",
            "2024-01-06 23:28:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:28:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:28:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 142 @ 426 updates, score 1.457) (writing took 0.3653094940000301 seconds)\n",
            "2024-01-06 23:28:29 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)\n",
            "2024-01-06 23:28:29 | INFO | train | epoch 142 | loss 1.579 | nll_loss 0.768 | ppl 1.7 | wps 827.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 426 | lr 0.000426 | gnorm 0.953 | clip 33.3 | train_wall 10 | wall 1521\n",
            "2024-01-06 23:28:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 143:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:28:29 | INFO | fairseq.trainer | begin training epoch 143\n",
            "2024-01-06 23:28:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 143:  67% 2/3 [00:05<00:02,  2.30s/it]2024-01-06 23:28:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 143 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 143 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:28:39 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 1.501 | nll_loss 0.434 | ppl 1.35 | wps 2827.4 | wpb 921 | bsz 75 | num_updates 429 | best_loss 1.457\n",
            "2024-01-06 23:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 429 updates\n",
            "2024-01-06 23:28:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 143 @ 429 updates, score 1.501) (writing took 0.17148680200034505 seconds)\n",
            "2024-01-06 23:28:39 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)\n",
            "2024-01-06 23:28:39 | INFO | train | epoch 143 | loss 1.567 | nll_loss 0.757 | ppl 1.69 | wps 836.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 429 | lr 0.000429 | gnorm 1.018 | clip 33.3 | train_wall 10 | wall 1532\n",
            "2024-01-06 23:28:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 144:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:28:39 | INFO | fairseq.trainer | begin training epoch 144\n",
            "2024-01-06 23:28:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 144:  67% 2/3 [00:04<00:01,  1.83s/it]2024-01-06 23:28:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 144 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 144 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:28:50 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 1.501 | nll_loss 0.434 | ppl 1.35 | wps 2850.3 | wpb 921 | bsz 75 | num_updates 432 | best_loss 1.457\n",
            "2024-01-06 23:28:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 432 updates\n",
            "2024-01-06 23:28:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 144 @ 432 updates, score 1.501) (writing took 0.2171806509995804 seconds)\n",
            "2024-01-06 23:28:50 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)\n",
            "2024-01-06 23:28:50 | INFO | train | epoch 144 | loss 1.58 | nll_loss 0.765 | ppl 1.7 | wps 827.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 432 | lr 0.000432 | gnorm 0.984 | clip 33.3 | train_wall 10 | wall 1543\n",
            "2024-01-06 23:28:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 145:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:28:50 | INFO | fairseq.trainer | begin training epoch 145\n",
            "2024-01-06 23:28:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 145:  67% 2/3 [00:09<00:04,  4.87s/it]2024-01-06 23:29:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 145 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 145 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:00 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 1.458 | nll_loss 0.387 | ppl 1.31 | wps 2848.7 | wpb 921 | bsz 75 | num_updates 435 | best_loss 1.457\n",
            "2024-01-06 23:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 435 updates\n",
            "2024-01-06 23:29:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 145 @ 435 updates, score 1.458) (writing took 0.16392189399994095 seconds)\n",
            "2024-01-06 23:29:00 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)\n",
            "2024-01-06 23:29:00 | INFO | train | epoch 145 | loss 1.539 | nll_loss 0.719 | ppl 1.65 | wps 832.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 435 | lr 0.000435 | gnorm 0.868 | clip 33.3 | train_wall 10 | wall 1553\n",
            "2024-01-06 23:29:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 146:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:01 | INFO | fairseq.trainer | begin training epoch 146\n",
            "2024-01-06 23:29:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 146:  67% 2/3 [00:09<00:04,  4.69s/it]2024-01-06 23:29:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 146 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 146 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:11 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 1.54 | nll_loss 0.469 | ppl 1.38 | wps 2939.2 | wpb 921 | bsz 75 | num_updates 438 | best_loss 1.457\n",
            "2024-01-06 23:29:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 438 updates\n",
            "2024-01-06 23:29:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 146 @ 438 updates, score 1.54) (writing took 0.17807964200028437 seconds)\n",
            "2024-01-06 23:29:11 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)\n",
            "2024-01-06 23:29:11 | INFO | train | epoch 146 | loss 1.562 | nll_loss 0.747 | ppl 1.68 | wps 839.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 438 | lr 0.000438 | gnorm 1.152 | clip 66.7 | train_wall 10 | wall 1564\n",
            "2024-01-06 23:29:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 147:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:11 | INFO | fairseq.trainer | begin training epoch 147\n",
            "2024-01-06 23:29:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 147:  67% 2/3 [00:09<00:04,  4.78s/it]2024-01-06 23:29:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 147 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 147 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:21 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 1.471 | nll_loss 0.397 | ppl 1.32 | wps 2890.3 | wpb 921 | bsz 75 | num_updates 441 | best_loss 1.457\n",
            "2024-01-06 23:29:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 441 updates\n",
            "2024-01-06 23:29:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 147 @ 441 updates, score 1.471) (writing took 0.18477297200024623 seconds)\n",
            "2024-01-06 23:29:21 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)\n",
            "2024-01-06 23:29:21 | INFO | train | epoch 147 | loss 1.553 | nll_loss 0.728 | ppl 1.66 | wps 835.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 441 | lr 0.000441 | gnorm 0.833 | clip 0 | train_wall 10 | wall 1574\n",
            "2024-01-06 23:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 148:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:21 | INFO | fairseq.trainer | begin training epoch 148\n",
            "2024-01-06 23:29:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 148:  67% 2/3 [00:04<00:02,  2.49s/it]2024-01-06 23:29:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 148 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 148 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:31 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 1.508 | nll_loss 0.436 | ppl 1.35 | wps 2113.2 | wpb 921 | bsz 75 | num_updates 444 | best_loss 1.457\n",
            "2024-01-06 23:29:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 444 updates\n",
            "2024-01-06 23:29:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 148 @ 444 updates, score 1.508) (writing took 0.29225080800006253 seconds)\n",
            "2024-01-06 23:29:32 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)\n",
            "2024-01-06 23:29:32 | INFO | train | epoch 148 | loss 1.569 | nll_loss 0.768 | ppl 1.7 | wps 853.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 444 | lr 0.000444 | gnorm 1.154 | clip 66.7 | train_wall 9 | wall 1584\n",
            "2024-01-06 23:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 149:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:32 | INFO | fairseq.trainer | begin training epoch 149\n",
            "2024-01-06 23:29:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 149:  67% 2/3 [00:05<00:02,  2.30s/it]2024-01-06 23:29:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 149 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 149 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:42 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 1.521 | nll_loss 0.458 | ppl 1.37 | wps 2769.3 | wpb 921 | bsz 75 | num_updates 447 | best_loss 1.457\n",
            "2024-01-06 23:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 447 updates\n",
            "2024-01-06 23:29:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 149 @ 447 updates, score 1.521) (writing took 0.18488085099988893 seconds)\n",
            "2024-01-06 23:29:42 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)\n",
            "2024-01-06 23:29:42 | INFO | train | epoch 149 | loss 1.539 | nll_loss 0.722 | ppl 1.65 | wps 840.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 447 | lr 0.000447 | gnorm 0.893 | clip 33.3 | train_wall 10 | wall 1595\n",
            "2024-01-06 23:29:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 150:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:42 | INFO | fairseq.trainer | begin training epoch 150\n",
            "2024-01-06 23:29:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 150:  67% 2/3 [00:05<00:02,  2.29s/it]2024-01-06 23:29:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 150 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 150 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:29:53 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 1.461 | nll_loss 0.391 | ppl 1.31 | wps 2722.2 | wpb 921 | bsz 75 | num_updates 450 | best_loss 1.457\n",
            "2024-01-06 23:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 450 updates\n",
            "2024-01-06 23:29:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:29:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 150 @ 450 updates, score 1.461) (writing took 0.1760916719999841 seconds)\n",
            "2024-01-06 23:29:53 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)\n",
            "2024-01-06 23:29:53 | INFO | train | epoch 150 | loss 1.557 | nll_loss 0.736 | ppl 1.67 | wps 836.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 450 | lr 0.00045 | gnorm 1.121 | clip 66.7 | train_wall 10 | wall 1605\n",
            "2024-01-06 23:29:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 151:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:29:53 | INFO | fairseq.trainer | begin training epoch 151\n",
            "2024-01-06 23:29:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 151:  67% 2/3 [00:05<00:03,  3.19s/it]2024-01-06 23:30:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 151 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 151 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:03 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 1.487 | nll_loss 0.415 | ppl 1.33 | wps 2838.1 | wpb 921 | bsz 75 | num_updates 453 | best_loss 1.457\n",
            "2024-01-06 23:30:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 453 updates\n",
            "2024-01-06 23:30:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 151 @ 453 updates, score 1.487) (writing took 0.1784196419998807 seconds)\n",
            "2024-01-06 23:30:03 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)\n",
            "2024-01-06 23:30:03 | INFO | train | epoch 151 | loss 1.534 | nll_loss 0.715 | ppl 1.64 | wps 838.2 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 453 | lr 0.000453 | gnorm 0.933 | clip 33.3 | train_wall 10 | wall 1616\n",
            "2024-01-06 23:30:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 152:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:03 | INFO | fairseq.trainer | begin training epoch 152\n",
            "2024-01-06 23:30:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 152:  67% 2/3 [00:05<00:02,  2.28s/it]2024-01-06 23:30:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 152 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 152 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:13 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 1.479 | nll_loss 0.411 | ppl 1.33 | wps 2874.5 | wpb 921 | bsz 75 | num_updates 456 | best_loss 1.457\n",
            "2024-01-06 23:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 456 updates\n",
            "2024-01-06 23:30:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 152 @ 456 updates, score 1.479) (writing took 0.18136402299978727 seconds)\n",
            "2024-01-06 23:30:14 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)\n",
            "2024-01-06 23:30:14 | INFO | train | epoch 152 | loss 1.516 | nll_loss 0.688 | ppl 1.61 | wps 842.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 456 | lr 0.000456 | gnorm 1.394 | clip 33.3 | train_wall 10 | wall 1626\n",
            "2024-01-06 23:30:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 153:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:14 | INFO | fairseq.trainer | begin training epoch 153\n",
            "2024-01-06 23:30:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 153:  67% 2/3 [00:04<00:01,  1.77s/it]2024-01-06 23:30:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 153 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 153 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:24 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 1.455 | nll_loss 0.381 | ppl 1.3 | wps 2795.2 | wpb 921 | bsz 75 | num_updates 459 | best_loss 1.455\n",
            "2024-01-06 23:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 459 updates\n",
            "2024-01-06 23:30:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:30:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:30:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 153 @ 459 updates, score 1.455) (writing took 0.3471207550001054 seconds)\n",
            "2024-01-06 23:30:24 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)\n",
            "2024-01-06 23:30:24 | INFO | train | epoch 153 | loss 1.526 | nll_loss 0.702 | ppl 1.63 | wps 823.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 459 | lr 0.000459 | gnorm 0.905 | clip 33.3 | train_wall 10 | wall 1637\n",
            "2024-01-06 23:30:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 154:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:24 | INFO | fairseq.trainer | begin training epoch 154\n",
            "2024-01-06 23:30:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 154:  67% 2/3 [00:05<00:02,  2.98s/it]2024-01-06 23:30:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 154 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 154 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:35 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 1.471 | nll_loss 0.393 | ppl 1.31 | wps 2914.5 | wpb 921 | bsz 75 | num_updates 462 | best_loss 1.455\n",
            "2024-01-06 23:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 462 updates\n",
            "2024-01-06 23:30:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 154 @ 462 updates, score 1.471) (writing took 0.2067393219999758 seconds)\n",
            "2024-01-06 23:30:35 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)\n",
            "2024-01-06 23:30:35 | INFO | train | epoch 154 | loss 1.535 | nll_loss 0.708 | ppl 1.63 | wps 834.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 462 | lr 0.000462 | gnorm 1.011 | clip 33.3 | train_wall 10 | wall 1647\n",
            "2024-01-06 23:30:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 155:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:35 | INFO | fairseq.trainer | begin training epoch 155\n",
            "2024-01-06 23:30:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 155:  67% 2/3 [00:05<00:02,  2.14s/it]2024-01-06 23:30:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 155 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 155 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:45 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 1.438 | nll_loss 0.38 | ppl 1.3 | wps 2788 | wpb 921 | bsz 75 | num_updates 465 | best_loss 1.438\n",
            "2024-01-06 23:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 465 updates\n",
            "2024-01-06 23:30:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:30:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 155 @ 465 updates, score 1.438) (writing took 0.3238550060000307 seconds)\n",
            "2024-01-06 23:30:45 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)\n",
            "2024-01-06 23:30:45 | INFO | train | epoch 155 | loss 1.491 | nll_loss 0.661 | ppl 1.58 | wps 825.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 465 | lr 0.000465 | gnorm 0.902 | clip 33.3 | train_wall 10 | wall 1658\n",
            "2024-01-06 23:30:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 156:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:45 | INFO | fairseq.trainer | begin training epoch 156\n",
            "2024-01-06 23:30:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 156:  67% 2/3 [00:05<00:03,  3.00s/it]2024-01-06 23:30:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 156 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 156 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:30:56 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 1.44 | nll_loss 0.376 | ppl 1.3 | wps 2573.3 | wpb 921 | bsz 75 | num_updates 468 | best_loss 1.438\n",
            "2024-01-06 23:30:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 468 updates\n",
            "2024-01-06 23:30:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:30:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 156 @ 468 updates, score 1.44) (writing took 0.17030313300028865 seconds)\n",
            "2024-01-06 23:30:56 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
            "2024-01-06 23:30:56 | INFO | train | epoch 156 | loss 1.503 | nll_loss 0.694 | ppl 1.62 | wps 831.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 468 | lr 0.000468 | gnorm 1.019 | clip 66.7 | train_wall 10 | wall 1669\n",
            "2024-01-06 23:30:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 157:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:30:56 | INFO | fairseq.trainer | begin training epoch 157\n",
            "2024-01-06 23:30:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 157:  67% 2/3 [00:04<00:02,  2.53s/it]2024-01-06 23:31:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 157 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 157 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:06 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 1.441 | nll_loss 0.365 | ppl 1.29 | wps 2136.3 | wpb 921 | bsz 75 | num_updates 471 | best_loss 1.438\n",
            "2024-01-06 23:31:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 471 updates\n",
            "2024-01-06 23:31:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 157 @ 471 updates, score 1.441) (writing took 0.28120021699987774 seconds)\n",
            "2024-01-06 23:31:07 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
            "2024-01-06 23:31:07 | INFO | train | epoch 157 | loss 1.514 | nll_loss 0.688 | ppl 1.61 | wps 825.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 471 | lr 0.000471 | gnorm 1.062 | clip 33.3 | train_wall 9 | wall 1679\n",
            "2024-01-06 23:31:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 158:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:31:07 | INFO | fairseq.trainer | begin training epoch 158\n",
            "2024-01-06 23:31:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 158:  67% 2/3 [00:05<00:02,  2.13s/it]2024-01-06 23:31:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 158 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 158 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:16 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 1.43 | nll_loss 0.35 | ppl 1.27 | wps 2977.4 | wpb 921 | bsz 75 | num_updates 474 | best_loss 1.43\n",
            "2024-01-06 23:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 474 updates\n",
            "2024-01-06 23:31:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:31:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:31:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 158 @ 474 updates, score 1.43) (writing took 0.46590578099994673 seconds)\n",
            "2024-01-06 23:31:17 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
            "2024-01-06 23:31:17 | INFO | train | epoch 158 | loss 1.503 | nll_loss 0.673 | ppl 1.59 | wps 853.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 474 | lr 0.000474 | gnorm 0.913 | clip 33.3 | train_wall 9 | wall 1690\n",
            "2024-01-06 23:31:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 159:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:31:17 | INFO | fairseq.trainer | begin training epoch 159\n",
            "2024-01-06 23:31:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 159:  67% 2/3 [00:05<00:02,  2.32s/it]2024-01-06 23:31:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 159 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 159 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:27 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 1.53 | nll_loss 0.47 | ppl 1.38 | wps 2889.8 | wpb 921 | bsz 75 | num_updates 477 | best_loss 1.43\n",
            "2024-01-06 23:31:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 477 updates\n",
            "2024-01-06 23:31:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 159 @ 477 updates, score 1.53) (writing took 0.19438180199995259 seconds)\n",
            "2024-01-06 23:31:28 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
            "2024-01-06 23:31:28 | INFO | train | epoch 159 | loss 1.499 | nll_loss 0.666 | ppl 1.59 | wps 829.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 477 | lr 0.000477 | gnorm 0.867 | clip 0 | train_wall 10 | wall 1700\n",
            "2024-01-06 23:31:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 160:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:31:28 | INFO | fairseq.trainer | begin training epoch 160\n",
            "2024-01-06 23:31:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 160:  67% 2/3 [00:09<00:04,  4.85s/it]2024-01-06 23:31:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 160 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 160 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:38 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 1.443 | nll_loss 0.377 | ppl 1.3 | wps 2864.3 | wpb 921 | bsz 75 | num_updates 480 | best_loss 1.43\n",
            "2024-01-06 23:31:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 480 updates\n",
            "2024-01-06 23:31:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 160 @ 480 updates, score 1.443) (writing took 0.18424843200000396 seconds)\n",
            "2024-01-06 23:31:38 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
            "2024-01-06 23:31:38 | INFO | train | epoch 160 | loss 1.515 | nll_loss 0.685 | ppl 1.61 | wps 821.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 480 | lr 0.00048 | gnorm 1.212 | clip 66.7 | train_wall 10 | wall 1711\n",
            "2024-01-06 23:31:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 161:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:31:38 | INFO | fairseq.trainer | begin training epoch 161\n",
            "2024-01-06 23:31:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 161:  67% 2/3 [00:09<00:04,  4.80s/it]2024-01-06 23:31:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 161 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 161 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:49 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 1.452 | nll_loss 0.385 | ppl 1.31 | wps 2872.2 | wpb 921 | bsz 75 | num_updates 483 | best_loss 1.43\n",
            "2024-01-06 23:31:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 483 updates\n",
            "2024-01-06 23:31:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 161 @ 483 updates, score 1.452) (writing took 0.1723663329998999 seconds)\n",
            "2024-01-06 23:31:49 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
            "2024-01-06 23:31:49 | INFO | train | epoch 161 | loss 1.536 | nll_loss 0.713 | ppl 1.64 | wps 834 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 483 | lr 0.000483 | gnorm 1.308 | clip 100 | train_wall 10 | wall 1721\n",
            "2024-01-06 23:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 162:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:31:49 | INFO | fairseq.trainer | begin training epoch 162\n",
            "2024-01-06 23:31:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 162:  67% 2/3 [00:04<00:02,  2.70s/it]2024-01-06 23:31:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 162 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 162 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:31:59 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 1.419 | nll_loss 0.359 | ppl 1.28 | wps 2728 | wpb 921 | bsz 75 | num_updates 486 | best_loss 1.419\n",
            "2024-01-06 23:31:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 486 updates\n",
            "2024-01-06 23:31:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 162 @ 486 updates, score 1.419) (writing took 0.3996047640002871 seconds)\n",
            "2024-01-06 23:31:59 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
            "2024-01-06 23:31:59 | INFO | train | epoch 162 | loss 1.49 | nll_loss 0.668 | ppl 1.59 | wps 819.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 486 | lr 0.000486 | gnorm 0.899 | clip 33.3 | train_wall 10 | wall 1732\n",
            "2024-01-06 23:32:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 163:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:00 | INFO | fairseq.trainer | begin training epoch 163\n",
            "2024-01-06 23:32:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 163:  67% 2/3 [00:05<00:02,  2.28s/it]2024-01-06 23:32:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 163 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 163 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:32:10 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 1.449 | nll_loss 0.395 | ppl 1.32 | wps 2843.4 | wpb 921 | bsz 75 | num_updates 489 | best_loss 1.419\n",
            "2024-01-06 23:32:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 489 updates\n",
            "2024-01-06 23:32:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 163 @ 489 updates, score 1.449) (writing took 0.15566378300036376 seconds)\n",
            "2024-01-06 23:32:10 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
            "2024-01-06 23:32:10 | INFO | train | epoch 163 | loss 1.454 | nll_loss 0.631 | ppl 1.55 | wps 833.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 489 | lr 0.000489 | gnorm 0.956 | clip 33.3 | train_wall 10 | wall 1743\n",
            "2024-01-06 23:32:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 164:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:10 | INFO | fairseq.trainer | begin training epoch 164\n",
            "2024-01-06 23:32:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 164:  67% 2/3 [00:05<00:02,  2.15s/it]2024-01-06 23:32:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 164 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 164 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:32:20 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 1.411 | nll_loss 0.354 | ppl 1.28 | wps 2977.8 | wpb 921 | bsz 75 | num_updates 492 | best_loss 1.411\n",
            "2024-01-06 23:32:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 492 updates\n",
            "2024-01-06 23:32:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:32:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 164 @ 492 updates, score 1.411) (writing took 0.3068785969999226 seconds)\n",
            "2024-01-06 23:32:21 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
            "2024-01-06 23:32:21 | INFO | train | epoch 164 | loss 1.466 | nll_loss 0.647 | ppl 1.57 | wps 825.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 492 | lr 0.000492 | gnorm 1.007 | clip 33.3 | train_wall 10 | wall 1753\n",
            "2024-01-06 23:32:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 165:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:21 | INFO | fairseq.trainer | begin training epoch 165\n",
            "2024-01-06 23:32:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 165:  67% 2/3 [00:04<00:01,  1.84s/it]2024-01-06 23:32:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 165 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 165 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:32:31 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 1.429 | nll_loss 0.359 | ppl 1.28 | wps 2766.4 | wpb 921 | bsz 75 | num_updates 495 | best_loss 1.411\n",
            "2024-01-06 23:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 495 updates\n",
            "2024-01-06 23:32:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 165 @ 495 updates, score 1.429) (writing took 0.15706181300038224 seconds)\n",
            "2024-01-06 23:32:31 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
            "2024-01-06 23:32:31 | INFO | train | epoch 165 | loss 1.448 | nll_loss 0.621 | ppl 1.54 | wps 831.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 495 | lr 0.000495 | gnorm 0.851 | clip 33.3 | train_wall 10 | wall 1764\n",
            "2024-01-06 23:32:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 166:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:31 | INFO | fairseq.trainer | begin training epoch 166\n",
            "2024-01-06 23:32:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 166:  67% 2/3 [00:04<00:02,  2.48s/it]2024-01-06 23:32:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 166 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 166 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:32:41 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 1.386 | nll_loss 0.321 | ppl 1.25 | wps 2767.7 | wpb 921 | bsz 75 | num_updates 498 | best_loss 1.386\n",
            "2024-01-06 23:32:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 498 updates\n",
            "2024-01-06 23:32:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:32:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 166 @ 498 updates, score 1.386) (writing took 0.3283207360000233 seconds)\n",
            "2024-01-06 23:32:42 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
            "2024-01-06 23:32:42 | INFO | train | epoch 166 | loss 1.44 | nll_loss 0.601 | ppl 1.52 | wps 834.9 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 498 | lr 0.000498 | gnorm 0.803 | clip 33.3 | train_wall 9 | wall 1774\n",
            "2024-01-06 23:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 167:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:42 | INFO | fairseq.trainer | begin training epoch 167\n",
            "2024-01-06 23:32:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 167:  67% 2/3 [00:04<00:01,  1.78s/it, loss=1.543, nll_loss=0.725, ppl=1.65, wps=834.5, ups=0.28, wpb=2941.3, bsz=235, num_updates=500, lr=0.0005, gnorm=0.989, clip=37, train_wall=319, wall=1779]2024-01-06 23:32:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 167 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 167 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:32:52 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 1.391 | nll_loss 0.333 | ppl 1.26 | wps 2204.5 | wpb 921 | bsz 75 | num_updates 501 | best_loss 1.386\n",
            "2024-01-06 23:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 501 updates\n",
            "2024-01-06 23:32:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:32:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 167 @ 501 updates, score 1.391) (writing took 0.23791069100025197 seconds)\n",
            "2024-01-06 23:32:52 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)\n",
            "2024-01-06 23:32:52 | INFO | train | epoch 167 | loss 1.435 | nll_loss 0.6 | ppl 1.52 | wps 850.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 501 | lr 0.000501 | gnorm 1.197 | clip 33.3 | train_wall 9 | wall 1785\n",
            "2024-01-06 23:32:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 168:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:32:52 | INFO | fairseq.trainer | begin training epoch 168\n",
            "2024-01-06 23:32:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 168:  67% 2/3 [00:04<00:01,  1.86s/it]2024-01-06 23:33:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 168 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 168 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:02 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 1.389 | nll_loss 0.343 | ppl 1.27 | wps 2847.5 | wpb 921 | bsz 75 | num_updates 504 | best_loss 1.386\n",
            "2024-01-06 23:33:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 504 updates\n",
            "2024-01-06 23:33:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 168 @ 504 updates, score 1.389) (writing took 0.15321720399970218 seconds)\n",
            "2024-01-06 23:33:02 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)\n",
            "2024-01-06 23:33:02 | INFO | train | epoch 168 | loss 1.431 | nll_loss 0.603 | ppl 1.52 | wps 856.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 504 | lr 0.000504 | gnorm 0.93 | clip 33.3 | train_wall 9 | wall 1795\n",
            "2024-01-06 23:33:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 169:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:02 | INFO | fairseq.trainer | begin training epoch 169\n",
            "2024-01-06 23:33:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 169:  67% 2/3 [00:09<00:04,  4.75s/it]2024-01-06 23:33:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 169 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 169 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:13 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 1.401 | nll_loss 0.351 | ppl 1.28 | wps 1776.2 | wpb 921 | bsz 75 | num_updates 507 | best_loss 1.386\n",
            "2024-01-06 23:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 507 updates\n",
            "2024-01-06 23:33:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 169 @ 507 updates, score 1.401) (writing took 0.17664680300003965 seconds)\n",
            "2024-01-06 23:33:13 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)\n",
            "2024-01-06 23:33:13 | INFO | train | epoch 169 | loss 1.421 | nll_loss 0.602 | ppl 1.52 | wps 838.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 507 | lr 0.000507 | gnorm 0.779 | clip 33.3 | train_wall 10 | wall 1806\n",
            "2024-01-06 23:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 170:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:13 | INFO | fairseq.trainer | begin training epoch 170\n",
            "2024-01-06 23:33:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 170:  67% 2/3 [00:04<00:02,  2.76s/it]2024-01-06 23:33:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 170 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 170 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.62it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:23 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 1.402 | nll_loss 0.334 | ppl 1.26 | wps 2854.5 | wpb 921 | bsz 75 | num_updates 510 | best_loss 1.386\n",
            "2024-01-06 23:33:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 510 updates\n",
            "2024-01-06 23:33:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 170 @ 510 updates, score 1.402) (writing took 0.16309562300011748 seconds)\n",
            "2024-01-06 23:33:23 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)\n",
            "2024-01-06 23:33:23 | INFO | train | epoch 170 | loss 1.434 | nll_loss 0.604 | ppl 1.52 | wps 826.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 510 | lr 0.00051 | gnorm 0.744 | clip 0 | train_wall 10 | wall 1816\n",
            "2024-01-06 23:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 171:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:23 | INFO | fairseq.trainer | begin training epoch 171\n",
            "2024-01-06 23:33:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 171:  67% 2/3 [00:09<00:04,  4.86s/it]2024-01-06 23:33:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 171 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 171 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.61it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:34 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 1.366 | nll_loss 0.307 | ppl 1.24 | wps 2786 | wpb 921 | bsz 75 | num_updates 513 | best_loss 1.366\n",
            "2024-01-06 23:33:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 513 updates\n",
            "2024-01-06 23:33:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:33:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:33:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 171 @ 513 updates, score 1.366) (writing took 0.32796301600001243 seconds)\n",
            "2024-01-06 23:33:34 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)\n",
            "2024-01-06 23:33:34 | INFO | train | epoch 171 | loss 1.407 | nll_loss 0.568 | ppl 1.48 | wps 811.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 513 | lr 0.000513 | gnorm 0.892 | clip 33.3 | train_wall 10 | wall 1827\n",
            "2024-01-06 23:33:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 172:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:34 | INFO | fairseq.trainer | begin training epoch 172\n",
            "2024-01-06 23:33:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 172:  67% 2/3 [00:04<00:02,  2.72s/it]2024-01-06 23:33:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 172 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 172 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:45 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 1.41 | nll_loss 0.363 | ppl 1.29 | wps 2828.7 | wpb 921 | bsz 75 | num_updates 516 | best_loss 1.366\n",
            "2024-01-06 23:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 516 updates\n",
            "2024-01-06 23:33:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 172 @ 516 updates, score 1.41) (writing took 0.15611170400006813 seconds)\n",
            "2024-01-06 23:33:45 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)\n",
            "2024-01-06 23:33:45 | INFO | train | epoch 172 | loss 1.478 | nll_loss 0.665 | ppl 1.59 | wps 828.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 516 | lr 0.000516 | gnorm 1.109 | clip 100 | train_wall 10 | wall 1838\n",
            "2024-01-06 23:33:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 173:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:45 | INFO | fairseq.trainer | begin training epoch 173\n",
            "2024-01-06 23:33:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 173:  67% 2/3 [00:04<00:01,  1.84s/it]2024-01-06 23:33:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 173 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 173 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:33:55 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 1.439 | nll_loss 0.377 | ppl 1.3 | wps 2665.4 | wpb 921 | bsz 75 | num_updates 519 | best_loss 1.366\n",
            "2024-01-06 23:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 519 updates\n",
            "2024-01-06 23:33:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 173 @ 519 updates, score 1.439) (writing took 0.16088548300012917 seconds)\n",
            "2024-01-06 23:33:55 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)\n",
            "2024-01-06 23:33:55 | INFO | train | epoch 173 | loss 1.458 | nll_loss 0.625 | ppl 1.54 | wps 829.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 519 | lr 0.000519 | gnorm 0.941 | clip 66.7 | train_wall 10 | wall 1848\n",
            "2024-01-06 23:33:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 174:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:33:56 | INFO | fairseq.trainer | begin training epoch 174\n",
            "2024-01-06 23:33:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 174:  67% 2/3 [00:05<00:02,  2.12s/it]2024-01-06 23:34:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 174 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 174 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:34:06 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 1.388 | nll_loss 0.33 | ppl 1.26 | wps 2923.9 | wpb 921 | bsz 75 | num_updates 522 | best_loss 1.366\n",
            "2024-01-06 23:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 522 updates\n",
            "2024-01-06 23:34:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 174 @ 522 updates, score 1.388) (writing took 0.15471152300005997 seconds)\n",
            "2024-01-06 23:34:06 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)\n",
            "2024-01-06 23:34:06 | INFO | train | epoch 174 | loss 1.438 | nll_loss 0.601 | ppl 1.52 | wps 846.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 522 | lr 0.000522 | gnorm 0.975 | clip 33.3 | train_wall 10 | wall 1859\n",
            "2024-01-06 23:34:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 175:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:34:06 | INFO | fairseq.trainer | begin training epoch 175\n",
            "2024-01-06 23:34:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 175:  67% 2/3 [00:05<00:02,  2.98s/it]2024-01-06 23:34:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 175 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 175 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:34:16 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 1.422 | nll_loss 0.359 | ppl 1.28 | wps 2854.8 | wpb 921 | bsz 75 | num_updates 525 | best_loss 1.366\n",
            "2024-01-06 23:34:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 525 updates\n",
            "2024-01-06 23:34:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 175 @ 525 updates, score 1.422) (writing took 0.16074697399972138 seconds)\n",
            "2024-01-06 23:34:16 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)\n",
            "2024-01-06 23:34:16 | INFO | train | epoch 175 | loss 1.456 | nll_loss 0.624 | ppl 1.54 | wps 837.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 525 | lr 0.000525 | gnorm 1.122 | clip 66.7 | train_wall 10 | wall 1869\n",
            "2024-01-06 23:34:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 176:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:34:16 | INFO | fairseq.trainer | begin training epoch 176\n",
            "2024-01-06 23:34:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 176:  67% 2/3 [00:06<00:02,  2.88s/it]2024-01-06 23:34:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 176 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 176 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:34:30 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 1.399 | nll_loss 0.335 | ppl 1.26 | wps 2926.5 | wpb 921 | bsz 75 | num_updates 528 | best_loss 1.366\n",
            "2024-01-06 23:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 528 updates\n",
            "2024-01-06 23:34:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 176 @ 528 updates, score 1.399) (writing took 0.2628509379997013 seconds)\n",
            "2024-01-06 23:34:31 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)\n",
            "2024-01-06 23:34:31 | INFO | train | epoch 176 | loss 1.431 | nll_loss 0.589 | ppl 1.5 | wps 620.6 | ups 0.21 | wpb 2928.3 | bsz 233.3 | num_updates 528 | lr 0.000528 | gnorm 1.017 | clip 33.3 | train_wall 13 | wall 1883\n",
            "2024-01-06 23:34:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 177:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:34:31 | INFO | fairseq.trainer | begin training epoch 177\n",
            "2024-01-06 23:34:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 177:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:34:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 177 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 177 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:34:41 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 1.337 | nll_loss 0.283 | ppl 1.22 | wps 1575.5 | wpb 921 | bsz 75 | num_updates 531 | best_loss 1.337\n",
            "2024-01-06 23:34:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 531 updates\n",
            "2024-01-06 23:34:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 177 @ 531 updates, score 1.337) (writing took 0.36805040399985955 seconds)\n",
            "2024-01-06 23:34:41 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)\n",
            "2024-01-06 23:34:41 | INFO | train | epoch 177 | loss 1.395 | nll_loss 0.549 | ppl 1.46 | wps 832.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 531 | lr 0.000531 | gnorm 0.659 | clip 0 | train_wall 9 | wall 1894\n",
            "2024-01-06 23:34:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 178:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:34:41 | INFO | fairseq.trainer | begin training epoch 178\n",
            "2024-01-06 23:34:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 178:  67% 2/3 [00:04<00:01,  1.81s/it]2024-01-06 23:34:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 178 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 178 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:34:51 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 1.354 | nll_loss 0.316 | ppl 1.24 | wps 2937.6 | wpb 921 | bsz 75 | num_updates 534 | best_loss 1.337\n",
            "2024-01-06 23:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 534 updates\n",
            "2024-01-06 23:34:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:34:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 178 @ 534 updates, score 1.354) (writing took 0.2247780910001893 seconds)\n",
            "2024-01-06 23:34:52 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)\n",
            "2024-01-06 23:34:52 | INFO | train | epoch 178 | loss 1.407 | nll_loss 0.581 | ppl 1.5 | wps 828 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 534 | lr 0.000534 | gnorm 0.759 | clip 0 | train_wall 10 | wall 1904\n",
            "2024-01-06 23:34:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 179:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:34:52 | INFO | fairseq.trainer | begin training epoch 179\n",
            "2024-01-06 23:34:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 179:  67% 2/3 [00:04<00:02,  2.56s/it]2024-01-06 23:35:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 179 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 179 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.53it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:02 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 1.351 | nll_loss 0.313 | ppl 1.24 | wps 2824.8 | wpb 921 | bsz 75 | num_updates 537 | best_loss 1.337\n",
            "2024-01-06 23:35:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 537 updates\n",
            "2024-01-06 23:35:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 179 @ 537 updates, score 1.351) (writing took 0.18779736200031039 seconds)\n",
            "2024-01-06 23:35:02 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)\n",
            "2024-01-06 23:35:02 | INFO | train | epoch 179 | loss 1.395 | nll_loss 0.573 | ppl 1.49 | wps 825.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 537 | lr 0.000537 | gnorm 0.595 | clip 0 | train_wall 10 | wall 1915\n",
            "2024-01-06 23:35:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 180:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:02 | INFO | fairseq.trainer | begin training epoch 180\n",
            "2024-01-06 23:35:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 180:  67% 2/3 [00:04<00:01,  1.84s/it]2024-01-06 23:35:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 180 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 180 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:13 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 1.353 | nll_loss 0.306 | ppl 1.24 | wps 2032.8 | wpb 921 | bsz 75 | num_updates 540 | best_loss 1.337\n",
            "2024-01-06 23:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 540 updates\n",
            "2024-01-06 23:35:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 180 @ 540 updates, score 1.353) (writing took 0.2439818610000657 seconds)\n",
            "2024-01-06 23:35:13 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)\n",
            "2024-01-06 23:35:13 | INFO | train | epoch 180 | loss 1.386 | nll_loss 0.566 | ppl 1.48 | wps 833.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 540 | lr 0.00054 | gnorm 0.71 | clip 0 | train_wall 9 | wall 1926\n",
            "2024-01-06 23:35:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 181:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:13 | INFO | fairseq.trainer | begin training epoch 181\n",
            "2024-01-06 23:35:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 181:  67% 2/3 [00:04<00:02,  2.57s/it]2024-01-06 23:35:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 181 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 181 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:23 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 1.396 | nll_loss 0.346 | ppl 1.27 | wps 2683 | wpb 921 | bsz 75 | num_updates 543 | best_loss 1.337\n",
            "2024-01-06 23:35:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 543 updates\n",
            "2024-01-06 23:35:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 181 @ 543 updates, score 1.396) (writing took 0.17006403300001693 seconds)\n",
            "2024-01-06 23:35:23 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)\n",
            "2024-01-06 23:35:23 | INFO | train | epoch 181 | loss 1.376 | nll_loss 0.533 | ppl 1.45 | wps 853.2 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 543 | lr 0.000543 | gnorm 0.779 | clip 33.3 | train_wall 9 | wall 1936\n",
            "2024-01-06 23:35:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 182:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:23 | INFO | fairseq.trainer | begin training epoch 182\n",
            "2024-01-06 23:35:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 182:  67% 2/3 [00:04<00:02,  2.71s/it]2024-01-06 23:35:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 182 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 182 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:33 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 1.326 | nll_loss 0.266 | ppl 1.2 | wps 2916.4 | wpb 921 | bsz 75 | num_updates 546 | best_loss 1.326\n",
            "2024-01-06 23:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 546 updates\n",
            "2024-01-06 23:35:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 182 @ 546 updates, score 1.326) (writing took 0.35989466499995615 seconds)\n",
            "2024-01-06 23:35:34 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)\n",
            "2024-01-06 23:35:34 | INFO | train | epoch 182 | loss 1.366 | nll_loss 0.521 | ppl 1.44 | wps 831.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 546 | lr 0.000546 | gnorm 0.762 | clip 33.3 | train_wall 9 | wall 1946\n",
            "2024-01-06 23:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 183:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:34 | INFO | fairseq.trainer | begin training epoch 183\n",
            "2024-01-06 23:35:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 183:  67% 2/3 [00:09<00:04,  4.66s/it]2024-01-06 23:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 183 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 183 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.60it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:44 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 1.379 | nll_loss 0.338 | ppl 1.26 | wps 2891.4 | wpb 921 | bsz 75 | num_updates 549 | best_loss 1.326\n",
            "2024-01-06 23:35:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 549 updates\n",
            "2024-01-06 23:35:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 183 @ 549 updates, score 1.379) (writing took 0.16492980300017734 seconds)\n",
            "2024-01-06 23:35:44 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)\n",
            "2024-01-06 23:35:44 | INFO | train | epoch 183 | loss 1.362 | nll_loss 0.52 | ppl 1.43 | wps 831.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 549 | lr 0.000549 | gnorm 0.867 | clip 33.3 | train_wall 10 | wall 1957\n",
            "2024-01-06 23:35:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 184:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:44 | INFO | fairseq.trainer | begin training epoch 184\n",
            "2024-01-06 23:35:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 184:  67% 2/3 [00:05<00:03,  3.18s/it]2024-01-06 23:35:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 184 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 184 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:35:55 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 1.343 | nll_loss 0.303 | ppl 1.23 | wps 2770.9 | wpb 921 | bsz 75 | num_updates 552 | best_loss 1.326\n",
            "2024-01-06 23:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 552 updates\n",
            "2024-01-06 23:35:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 184 @ 552 updates, score 1.343) (writing took 0.17188613300004363 seconds)\n",
            "2024-01-06 23:35:55 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)\n",
            "2024-01-06 23:35:55 | INFO | train | epoch 184 | loss 1.405 | nll_loss 0.58 | ppl 1.5 | wps 840 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 552 | lr 0.000552 | gnorm 1.013 | clip 33.3 | train_wall 10 | wall 1967\n",
            "2024-01-06 23:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 185:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:35:55 | INFO | fairseq.trainer | begin training epoch 185\n",
            "2024-01-06 23:35:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 185:  67% 2/3 [00:04<00:02,  2.53s/it]2024-01-06 23:36:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 185 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 185 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:05 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 1.371 | nll_loss 0.32 | ppl 1.25 | wps 2893.9 | wpb 921 | bsz 75 | num_updates 555 | best_loss 1.326\n",
            "2024-01-06 23:36:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 555 updates\n",
            "2024-01-06 23:36:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 185 @ 555 updates, score 1.371) (writing took 0.22784327099998336 seconds)\n",
            "2024-01-06 23:36:05 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)\n",
            "2024-01-06 23:36:05 | INFO | train | epoch 185 | loss 1.385 | nll_loss 0.558 | ppl 1.47 | wps 836.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 555 | lr 0.000555 | gnorm 0.958 | clip 33.3 | train_wall 10 | wall 1978\n",
            "2024-01-06 23:36:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 186:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:05 | INFO | fairseq.trainer | begin training epoch 186\n",
            "2024-01-06 23:36:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 186:  67% 2/3 [00:09<00:04,  4.60s/it]2024-01-06 23:36:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 186 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 186 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:15 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 1.384 | nll_loss 0.333 | ppl 1.26 | wps 1651.3 | wpb 921 | bsz 75 | num_updates 558 | best_loss 1.326\n",
            "2024-01-06 23:36:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 558 updates\n",
            "2024-01-06 23:36:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 186 @ 558 updates, score 1.384) (writing took 0.16375269399986792 seconds)\n",
            "2024-01-06 23:36:16 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
            "2024-01-06 23:36:16 | INFO | train | epoch 186 | loss 1.371 | nll_loss 0.528 | ppl 1.44 | wps 854.1 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 558 | lr 0.000558 | gnorm 0.594 | clip 0 | train_wall 9 | wall 1988\n",
            "2024-01-06 23:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 187:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:16 | INFO | fairseq.trainer | begin training epoch 187\n",
            "2024-01-06 23:36:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 187:  67% 2/3 [00:09<00:04,  4.75s/it]2024-01-06 23:36:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 187 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 187 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:26 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 1.338 | nll_loss 0.285 | ppl 1.22 | wps 2668.7 | wpb 921 | bsz 75 | num_updates 561 | best_loss 1.326\n",
            "2024-01-06 23:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 561 updates\n",
            "2024-01-06 23:36:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 187 @ 561 updates, score 1.338) (writing took 0.16873646300018663 seconds)\n",
            "2024-01-06 23:36:26 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
            "2024-01-06 23:36:26 | INFO | train | epoch 187 | loss 1.351 | nll_loss 0.502 | ppl 1.42 | wps 829.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 561 | lr 0.000561 | gnorm 0.629 | clip 0 | train_wall 10 | wall 1999\n",
            "2024-01-06 23:36:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 188:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:26 | INFO | fairseq.trainer | begin training epoch 188\n",
            "2024-01-06 23:36:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 188:  67% 2/3 [00:05<00:02,  2.99s/it]2024-01-06 23:36:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 188 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 188 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:36 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 1.371 | nll_loss 0.323 | ppl 1.25 | wps 2850.9 | wpb 921 | bsz 75 | num_updates 564 | best_loss 1.326\n",
            "2024-01-06 23:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 564 updates\n",
            "2024-01-06 23:36:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 188 @ 564 updates, score 1.371) (writing took 0.15801319299998795 seconds)\n",
            "2024-01-06 23:36:37 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
            "2024-01-06 23:36:37 | INFO | train | epoch 188 | loss 1.37 | nll_loss 0.535 | ppl 1.45 | wps 834.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 564 | lr 0.000564 | gnorm 0.744 | clip 0 | train_wall 10 | wall 2009\n",
            "2024-01-06 23:36:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 189:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:37 | INFO | fairseq.trainer | begin training epoch 189\n",
            "2024-01-06 23:36:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 189:  67% 2/3 [00:09<00:04,  4.56s/it]2024-01-06 23:36:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 189 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 189 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:47 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 1.383 | nll_loss 0.348 | ppl 1.27 | wps 2077.1 | wpb 921 | bsz 75 | num_updates 567 | best_loss 1.326\n",
            "2024-01-06 23:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 567 updates\n",
            "2024-01-06 23:36:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 189 @ 567 updates, score 1.383) (writing took 0.23933913000018947 seconds)\n",
            "2024-01-06 23:36:47 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
            "2024-01-06 23:36:47 | INFO | train | epoch 189 | loss 1.34 | nll_loss 0.495 | ppl 1.41 | wps 831.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 567 | lr 0.000567 | gnorm 0.574 | clip 0 | train_wall 9 | wall 2020\n",
            "2024-01-06 23:36:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 190:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:47 | INFO | fairseq.trainer | begin training epoch 190\n",
            "2024-01-06 23:36:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 190:  67% 2/3 [00:04<00:01,  1.85s/it]2024-01-06 23:36:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 190 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 190 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.63it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:36:57 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 1.328 | nll_loss 0.286 | ppl 1.22 | wps 2926.6 | wpb 921 | bsz 75 | num_updates 570 | best_loss 1.326\n",
            "2024-01-06 23:36:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 570 updates\n",
            "2024-01-06 23:36:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:36:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 190 @ 570 updates, score 1.328) (writing took 0.16750711299982868 seconds)\n",
            "2024-01-06 23:36:57 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
            "2024-01-06 23:36:57 | INFO | train | epoch 190 | loss 1.343 | nll_loss 0.503 | ppl 1.42 | wps 856.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 570 | lr 0.00057 | gnorm 0.582 | clip 0 | train_wall 9 | wall 2030\n",
            "2024-01-06 23:36:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 191:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:36:57 | INFO | fairseq.trainer | begin training epoch 191\n",
            "2024-01-06 23:36:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 191:  67% 2/3 [00:09<00:04,  4.72s/it]2024-01-06 23:37:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 191 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 191 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:37:08 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 1.325 | nll_loss 0.279 | ppl 1.21 | wps 1799.4 | wpb 921 | bsz 75 | num_updates 573 | best_loss 1.325\n",
            "2024-01-06 23:37:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 573 updates\n",
            "2024-01-06 23:37:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 191 @ 573 updates, score 1.325) (writing took 0.34032817599972986 seconds)\n",
            "2024-01-06 23:37:08 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
            "2024-01-06 23:37:08 | INFO | train | epoch 191 | loss 1.334 | nll_loss 0.496 | ppl 1.41 | wps 832.1 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 573 | lr 0.000573 | gnorm 0.941 | clip 33.3 | train_wall 9 | wall 2041\n",
            "2024-01-06 23:37:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 192:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:37:08 | INFO | fairseq.trainer | begin training epoch 192\n",
            "2024-01-06 23:37:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 192:  67% 2/3 [00:09<00:04,  4.63s/it]2024-01-06 23:37:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 192 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 192 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:37:18 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 1.348 | nll_loss 0.31 | ppl 1.24 | wps 2854.3 | wpb 921 | bsz 75 | num_updates 576 | best_loss 1.325\n",
            "2024-01-06 23:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 576 updates\n",
            "2024-01-06 23:37:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 192 @ 576 updates, score 1.348) (writing took 0.18672660300035204 seconds)\n",
            "2024-01-06 23:37:19 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
            "2024-01-06 23:37:19 | INFO | train | epoch 192 | loss 1.324 | nll_loss 0.486 | ppl 1.4 | wps 836 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 576 | lr 0.000576 | gnorm 0.845 | clip 33.3 | train_wall 10 | wall 2051\n",
            "2024-01-06 23:37:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 193:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:37:19 | INFO | fairseq.trainer | begin training epoch 193\n",
            "2024-01-06 23:37:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 193:  67% 2/3 [00:04<00:02,  2.74s/it]2024-01-06 23:37:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 193 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 193 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:37:29 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 1.331 | nll_loss 0.283 | ppl 1.22 | wps 2425.7 | wpb 921 | bsz 75 | num_updates 579 | best_loss 1.325\n",
            "2024-01-06 23:37:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 579 updates\n",
            "2024-01-06 23:37:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 193 @ 579 updates, score 1.331) (writing took 0.3680737549998412 seconds)\n",
            "2024-01-06 23:37:29 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
            "2024-01-06 23:37:29 | INFO | train | epoch 193 | loss 1.346 | nll_loss 0.513 | ppl 1.43 | wps 817.6 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 579 | lr 0.000579 | gnorm 0.765 | clip 33.3 | train_wall 10 | wall 2062\n",
            "2024-01-06 23:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 194:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:37:29 | INFO | fairseq.trainer | begin training epoch 194\n",
            "2024-01-06 23:37:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 194:  67% 2/3 [00:04<00:01,  1.96s/it]2024-01-06 23:37:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 194 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 194 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:37:40 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 1.326 | nll_loss 0.276 | ppl 1.21 | wps 2830.7 | wpb 921 | bsz 75 | num_updates 582 | best_loss 1.325\n",
            "2024-01-06 23:37:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 582 updates\n",
            "2024-01-06 23:37:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 194 @ 582 updates, score 1.326) (writing took 0.217339431000255 seconds)\n",
            "2024-01-06 23:37:40 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
            "2024-01-06 23:37:40 | INFO | train | epoch 194 | loss 1.343 | nll_loss 0.501 | ppl 1.41 | wps 825.4 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 582 | lr 0.000582 | gnorm 0.837 | clip 33.3 | train_wall 10 | wall 2073\n",
            "2024-01-06 23:37:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 195:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:37:40 | INFO | fairseq.trainer | begin training epoch 195\n",
            "2024-01-06 23:37:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 195:  67% 2/3 [00:05<00:02,  2.17s/it]2024-01-06 23:37:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 195 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 195 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:37:50 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 1.335 | nll_loss 0.301 | ppl 1.23 | wps 2537.8 | wpb 921 | bsz 75 | num_updates 585 | best_loss 1.325\n",
            "2024-01-06 23:37:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 585 updates\n",
            "2024-01-06 23:37:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:37:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 195 @ 585 updates, score 1.335) (writing took 0.17803161299980275 seconds)\n",
            "2024-01-06 23:37:50 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)\n",
            "2024-01-06 23:37:50 | INFO | train | epoch 195 | loss 1.325 | nll_loss 0.484 | ppl 1.4 | wps 847.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 585 | lr 0.000585 | gnorm 0.628 | clip 0 | train_wall 9 | wall 2083\n",
            "2024-01-06 23:37:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 196:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:37:50 | INFO | fairseq.trainer | begin training epoch 196\n",
            "2024-01-06 23:37:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 196:  67% 2/3 [00:04<00:02,  2.51s/it]2024-01-06 23:38:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 196 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 196 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:01 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 1.34 | nll_loss 0.297 | ppl 1.23 | wps 2860.4 | wpb 921 | bsz 75 | num_updates 588 | best_loss 1.325\n",
            "2024-01-06 23:38:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 588 updates\n",
            "2024-01-06 23:38:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 196 @ 588 updates, score 1.34) (writing took 0.30413310799985993 seconds)\n",
            "2024-01-06 23:38:01 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)\n",
            "2024-01-06 23:38:01 | INFO | train | epoch 196 | loss 1.361 | nll_loss 0.531 | ppl 1.44 | wps 825 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 588 | lr 0.000588 | gnorm 0.869 | clip 33.3 | train_wall 10 | wall 2094\n",
            "2024-01-06 23:38:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 197:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:01 | INFO | fairseq.trainer | begin training epoch 197\n",
            "2024-01-06 23:38:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 197:  67% 2/3 [00:09<00:04,  4.77s/it]2024-01-06 23:38:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 197 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 197 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:11 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 1.323 | nll_loss 0.274 | ppl 1.21 | wps 2030.1 | wpb 921 | bsz 75 | num_updates 591 | best_loss 1.323\n",
            "2024-01-06 23:38:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 591 updates\n",
            "2024-01-06 23:38:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 197 @ 591 updates, score 1.323) (writing took 0.37385626500008584 seconds)\n",
            "2024-01-06 23:38:11 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)\n",
            "2024-01-06 23:38:11 | INFO | train | epoch 197 | loss 1.328 | nll_loss 0.482 | ppl 1.4 | wps 831.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 591 | lr 0.000591 | gnorm 0.711 | clip 33.3 | train_wall 9 | wall 2104\n",
            "2024-01-06 23:38:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 198:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:12 | INFO | fairseq.trainer | begin training epoch 198\n",
            "2024-01-06 23:38:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 198:  67% 2/3 [00:05<00:02,  2.14s/it]2024-01-06 23:38:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 198 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 198 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:22 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 1.335 | nll_loss 0.296 | ppl 1.23 | wps 2867.6 | wpb 921 | bsz 75 | num_updates 594 | best_loss 1.323\n",
            "2024-01-06 23:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 594 updates\n",
            "2024-01-06 23:38:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 198 @ 594 updates, score 1.335) (writing took 0.1946700220000821 seconds)\n",
            "2024-01-06 23:38:22 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)\n",
            "2024-01-06 23:38:22 | INFO | train | epoch 198 | loss 1.329 | nll_loss 0.48 | ppl 1.4 | wps 836.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 594 | lr 0.000594 | gnorm 0.59 | clip 0 | train_wall 10 | wall 2115\n",
            "2024-01-06 23:38:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 199:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:22 | INFO | fairseq.trainer | begin training epoch 199\n",
            "2024-01-06 23:38:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 199:  67% 2/3 [00:09<00:04,  4.67s/it]2024-01-06 23:38:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 199 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 199 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:32 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 1.313 | nll_loss 0.282 | ppl 1.22 | wps 2251.6 | wpb 921 | bsz 75 | num_updates 597 | best_loss 1.313\n",
            "2024-01-06 23:38:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 597 updates\n",
            "2024-01-06 23:38:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 199 @ 597 updates, score 1.313) (writing took 0.49381418999973903 seconds)\n",
            "2024-01-06 23:38:33 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)\n",
            "2024-01-06 23:38:33 | INFO | train | epoch 199 | loss 1.325 | nll_loss 0.484 | ppl 1.4 | wps 818.2 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 597 | lr 0.000597 | gnorm 0.483 | clip 0 | train_wall 9 | wall 2125\n",
            "2024-01-06 23:38:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 200:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:33 | INFO | fairseq.trainer | begin training epoch 200\n",
            "2024-01-06 23:38:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 200:  67% 2/3 [00:05<00:03,  3.11s/it]2024-01-06 23:38:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 200 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 200 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:43 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 1.335 | nll_loss 0.311 | ppl 1.24 | wps 2743.2 | wpb 921 | bsz 75 | num_updates 600 | best_loss 1.313\n",
            "2024-01-06 23:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 600 updates\n",
            "2024-01-06 23:38:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 200 @ 600 updates, score 1.335) (writing took 0.282582308000201 seconds)\n",
            "2024-01-06 23:38:43 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)\n",
            "2024-01-06 23:38:43 | INFO | train | epoch 200 | loss 1.36 | nll_loss 0.539 | ppl 1.45 | wps 835.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 600 | lr 0.0006 | gnorm 0.863 | clip 0 | train_wall 10 | wall 2136\n",
            "2024-01-06 23:38:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 201:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:43 | INFO | fairseq.trainer | begin training epoch 201\n",
            "2024-01-06 23:38:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 201:  67% 2/3 [00:05<00:02,  2.30s/it]2024-01-06 23:38:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 201 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 201 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:38:54 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 1.329 | nll_loss 0.293 | ppl 1.23 | wps 2820.6 | wpb 921 | bsz 75 | num_updates 603 | best_loss 1.313\n",
            "2024-01-06 23:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 603 updates\n",
            "2024-01-06 23:38:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:38:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 201 @ 603 updates, score 1.329) (writing took 0.17056829300008758 seconds)\n",
            "2024-01-06 23:38:54 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)\n",
            "2024-01-06 23:38:54 | INFO | train | epoch 201 | loss 1.342 | nll_loss 0.505 | ppl 1.42 | wps 834.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 603 | lr 0.000603 | gnorm 0.989 | clip 33.3 | train_wall 10 | wall 2146\n",
            "2024-01-06 23:38:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 202:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:38:54 | INFO | fairseq.trainer | begin training epoch 202\n",
            "2024-01-06 23:38:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 202:  67% 2/3 [00:05<00:03,  3.27s/it]2024-01-06 23:39:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 202 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 202 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:04 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 1.311 | nll_loss 0.27 | ppl 1.21 | wps 2917.1 | wpb 921 | bsz 75 | num_updates 606 | best_loss 1.311\n",
            "2024-01-06 23:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 606 updates\n",
            "2024-01-06 23:39:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:39:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 202 @ 606 updates, score 1.311) (writing took 0.3998231239997949 seconds)\n",
            "2024-01-06 23:39:05 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)\n",
            "2024-01-06 23:39:05 | INFO | train | epoch 202 | loss 1.318 | nll_loss 0.473 | ppl 1.39 | wps 813 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 606 | lr 0.000606 | gnorm 0.805 | clip 33.3 | train_wall 10 | wall 2157\n",
            "2024-01-06 23:39:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 203:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:05 | INFO | fairseq.trainer | begin training epoch 203\n",
            "2024-01-06 23:39:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 203:  67% 2/3 [00:04<00:02,  2.71s/it]2024-01-06 23:39:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 203 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 203 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:15 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 1.317 | nll_loss 0.277 | ppl 1.21 | wps 2698 | wpb 921 | bsz 75 | num_updates 609 | best_loss 1.311\n",
            "2024-01-06 23:39:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 609 updates\n",
            "2024-01-06 23:39:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 203 @ 609 updates, score 1.317) (writing took 0.2302845399999569 seconds)\n",
            "2024-01-06 23:39:15 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)\n",
            "2024-01-06 23:39:15 | INFO | train | epoch 203 | loss 1.339 | nll_loss 0.498 | ppl 1.41 | wps 829.7 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 609 | lr 0.000609 | gnorm 0.943 | clip 33.3 | train_wall 10 | wall 2168\n",
            "2024-01-06 23:39:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 204:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:15 | INFO | fairseq.trainer | begin training epoch 204\n",
            "2024-01-06 23:39:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 204:  67% 2/3 [00:09<00:04,  4.53s/it]2024-01-06 23:39:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 204 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 204 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:25 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 1.355 | nll_loss 0.336 | ppl 1.26 | wps 2917.5 | wpb 921 | bsz 75 | num_updates 612 | best_loss 1.311\n",
            "2024-01-06 23:39:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 612 updates\n",
            "2024-01-06 23:39:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 204 @ 612 updates, score 1.355) (writing took 0.18204578299992136 seconds)\n",
            "2024-01-06 23:39:25 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)\n",
            "2024-01-06 23:39:25 | INFO | train | epoch 204 | loss 1.295 | nll_loss 0.446 | ppl 1.36 | wps 855.5 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 612 | lr 0.000612 | gnorm 0.664 | clip 33.3 | train_wall 9 | wall 2178\n",
            "2024-01-06 23:39:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 205:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:25 | INFO | fairseq.trainer | begin training epoch 205\n",
            "2024-01-06 23:39:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 205:  67% 2/3 [00:04<00:02,  2.54s/it]2024-01-06 23:39:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 205 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 205 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:36 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 1.324 | nll_loss 0.301 | ppl 1.23 | wps 2342.9 | wpb 921 | bsz 75 | num_updates 615 | best_loss 1.311\n",
            "2024-01-06 23:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 615 updates\n",
            "2024-01-06 23:39:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 205 @ 615 updates, score 1.324) (writing took 0.2135304109997378 seconds)\n",
            "2024-01-06 23:39:36 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)\n",
            "2024-01-06 23:39:36 | INFO | train | epoch 205 | loss 1.356 | nll_loss 0.524 | ppl 1.44 | wps 827.3 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 615 | lr 0.000615 | gnorm 0.812 | clip 33.3 | train_wall 10 | wall 2189\n",
            "2024-01-06 23:39:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 206:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:36 | INFO | fairseq.trainer | begin training epoch 206\n",
            "2024-01-06 23:39:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 206:  67% 2/3 [00:09<00:04,  4.74s/it]2024-01-06 23:39:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 206 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 206 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:46 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 1.313 | nll_loss 0.281 | ppl 1.22 | wps 2715.6 | wpb 921 | bsz 75 | num_updates 618 | best_loss 1.311\n",
            "2024-01-06 23:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 618 updates\n",
            "2024-01-06 23:39:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 206 @ 618 updates, score 1.313) (writing took 0.18715124199979982 seconds)\n",
            "2024-01-06 23:39:47 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)\n",
            "2024-01-06 23:39:47 | INFO | train | epoch 206 | loss 1.331 | nll_loss 0.498 | ppl 1.41 | wps 830 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 618 | lr 0.000618 | gnorm 0.98 | clip 33.3 | train_wall 10 | wall 2199\n",
            "2024-01-06 23:39:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 207:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:47 | INFO | fairseq.trainer | begin training epoch 207\n",
            "2024-01-06 23:39:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 207:  67% 2/3 [00:09<00:04,  4.70s/it]2024-01-06 23:39:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 207 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 207 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:39:57 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 1.321 | nll_loss 0.289 | ppl 1.22 | wps 2658 | wpb 921 | bsz 75 | num_updates 621 | best_loss 1.311\n",
            "2024-01-06 23:39:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 621 updates\n",
            "2024-01-06 23:39:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:39:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 207 @ 621 updates, score 1.321) (writing took 0.1731828229999337 seconds)\n",
            "2024-01-06 23:39:57 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)\n",
            "2024-01-06 23:39:57 | INFO | train | epoch 207 | loss 1.318 | nll_loss 0.474 | ppl 1.39 | wps 837.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 621 | lr 0.000621 | gnorm 0.71 | clip 0 | train_wall 10 | wall 2210\n",
            "2024-01-06 23:39:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 208:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:39:57 | INFO | fairseq.trainer | begin training epoch 208\n",
            "2024-01-06 23:39:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 208:  67% 2/3 [00:09<00:04,  4.75s/it]2024-01-06 23:40:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 208 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 208 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:40:07 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 1.292 | nll_loss 0.262 | ppl 1.2 | wps 2875.5 | wpb 921 | bsz 75 | num_updates 624 | best_loss 1.292\n",
            "2024-01-06 23:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 624 updates\n",
            "2024-01-06 23:40:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 208 @ 624 updates, score 1.292) (writing took 0.3873967540002923 seconds)\n",
            "2024-01-06 23:40:08 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)\n",
            "2024-01-06 23:40:08 | INFO | train | epoch 208 | loss 1.32 | nll_loss 0.473 | ppl 1.39 | wps 820.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 624 | lr 0.000624 | gnorm 0.777 | clip 33.3 | train_wall 9 | wall 2221\n",
            "2024-01-06 23:40:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 209:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:40:08 | INFO | fairseq.trainer | begin training epoch 209\n",
            "2024-01-06 23:40:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 209:  67% 2/3 [00:09<00:04,  4.66s/it]2024-01-06 23:40:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 209 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 209 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:40:18 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 1.302 | nll_loss 0.285 | ppl 1.22 | wps 2096.1 | wpb 921 | bsz 75 | num_updates 627 | best_loss 1.292\n",
            "2024-01-06 23:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 627 updates\n",
            "2024-01-06 23:40:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 209 @ 627 updates, score 1.302) (writing took 0.27596044899974004 seconds)\n",
            "2024-01-06 23:40:18 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)\n",
            "2024-01-06 23:40:18 | INFO | train | epoch 209 | loss 1.307 | nll_loss 0.467 | ppl 1.38 | wps 835.7 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 627 | lr 0.000627 | gnorm 0.603 | clip 0 | train_wall 9 | wall 2231\n",
            "2024-01-06 23:40:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 210:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:40:18 | INFO | fairseq.trainer | begin training epoch 210\n",
            "2024-01-06 23:40:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 210:  67% 2/3 [00:09<00:04,  4.53s/it]2024-01-06 23:40:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 210 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 210 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:40:28 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 1.329 | nll_loss 0.318 | ppl 1.25 | wps 2839.7 | wpb 921 | bsz 75 | num_updates 630 | best_loss 1.292\n",
            "2024-01-06 23:40:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 630 updates\n",
            "2024-01-06 23:40:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 210 @ 630 updates, score 1.329) (writing took 0.17421669299983478 seconds)\n",
            "2024-01-06 23:40:29 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)\n",
            "2024-01-06 23:40:29 | INFO | train | epoch 210 | loss 1.304 | nll_loss 0.469 | ppl 1.38 | wps 857.8 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 630 | lr 0.00063 | gnorm 0.645 | clip 0 | train_wall 9 | wall 2241\n",
            "2024-01-06 23:40:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 211:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:40:29 | INFO | fairseq.trainer | begin training epoch 211\n",
            "2024-01-06 23:40:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 211:  67% 2/3 [00:05<00:03,  3.21s/it]2024-01-06 23:40:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 211 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 211 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:40:39 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 1.304 | nll_loss 0.294 | ppl 1.23 | wps 2868.1 | wpb 921 | bsz 75 | num_updates 633 | best_loss 1.292\n",
            "2024-01-06 23:40:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 633 updates\n",
            "2024-01-06 23:40:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 211 @ 633 updates, score 1.304) (writing took 0.17698539299999538 seconds)\n",
            "2024-01-06 23:40:39 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)\n",
            "2024-01-06 23:40:39 | INFO | train | epoch 211 | loss 1.319 | nll_loss 0.487 | ppl 1.4 | wps 837.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 633 | lr 0.000633 | gnorm 0.621 | clip 0 | train_wall 10 | wall 2252\n",
            "2024-01-06 23:40:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 212:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:40:39 | INFO | fairseq.trainer | begin training epoch 212\n",
            "2024-01-06 23:40:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 212:  67% 2/3 [00:05<00:02,  2.33s/it]2024-01-06 23:40:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 212 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 212 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:40:49 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 1.303 | nll_loss 0.283 | ppl 1.22 | wps 2588.3 | wpb 921 | bsz 75 | num_updates 636 | best_loss 1.292\n",
            "2024-01-06 23:40:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 636 updates\n",
            "2024-01-06 23:40:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 212 @ 636 updates, score 1.303) (writing took 0.17532915299989327 seconds)\n",
            "2024-01-06 23:40:50 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)\n",
            "2024-01-06 23:40:50 | INFO | train | epoch 212 | loss 1.32 | nll_loss 0.49 | ppl 1.4 | wps 831.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 636 | lr 0.000636 | gnorm 0.902 | clip 33.3 | train_wall 10 | wall 2262\n",
            "2024-01-06 23:40:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 213:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:40:50 | INFO | fairseq.trainer | begin training epoch 213\n",
            "2024-01-06 23:40:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 213:  67% 2/3 [00:04<00:01,  1.92s/it]2024-01-06 23:40:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 213 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 213 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.66it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:00 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 1.306 | nll_loss 0.295 | ppl 1.23 | wps 2910.6 | wpb 921 | bsz 75 | num_updates 639 | best_loss 1.292\n",
            "2024-01-06 23:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 639 updates\n",
            "2024-01-06 23:41:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 213 @ 639 updates, score 1.306) (writing took 0.20165539200024796 seconds)\n",
            "2024-01-06 23:41:00 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)\n",
            "2024-01-06 23:41:00 | INFO | train | epoch 213 | loss 1.304 | nll_loss 0.461 | ppl 1.38 | wps 846 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 639 | lr 0.000639 | gnorm 0.666 | clip 0 | train_wall 9 | wall 2273\n",
            "2024-01-06 23:41:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 214:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:00 | INFO | fairseq.trainer | begin training epoch 214\n",
            "2024-01-06 23:41:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 214:  67% 2/3 [00:09<00:04,  4.54s/it]2024-01-06 23:41:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 214 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 214 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:10 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 1.281 | nll_loss 0.271 | ppl 1.21 | wps 1912.1 | wpb 921 | bsz 75 | num_updates 642 | best_loss 1.281\n",
            "2024-01-06 23:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 642 updates\n",
            "2024-01-06 23:41:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:41:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 214 @ 642 updates, score 1.281) (writing took 0.4034935439999572 seconds)\n",
            "2024-01-06 23:41:11 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)\n",
            "2024-01-06 23:41:11 | INFO | train | epoch 214 | loss 1.297 | nll_loss 0.457 | ppl 1.37 | wps 839.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 642 | lr 0.000642 | gnorm 0.741 | clip 33.3 | train_wall 9 | wall 2283\n",
            "2024-01-06 23:41:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 215:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:11 | INFO | fairseq.trainer | begin training epoch 215\n",
            "2024-01-06 23:41:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 215:  67% 2/3 [00:05<00:02,  2.15s/it]2024-01-06 23:41:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 215 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 215 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:21 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 1.318 | nll_loss 0.3 | ppl 1.23 | wps 2900.9 | wpb 921 | bsz 75 | num_updates 645 | best_loss 1.281\n",
            "2024-01-06 23:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 645 updates\n",
            "2024-01-06 23:41:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 215 @ 645 updates, score 1.318) (writing took 0.19009957300022506 seconds)\n",
            "2024-01-06 23:41:21 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)\n",
            "2024-01-06 23:41:21 | INFO | train | epoch 215 | loss 1.332 | nll_loss 0.5 | ppl 1.41 | wps 842.6 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 645 | lr 0.000645 | gnorm 0.826 | clip 33.3 | train_wall 10 | wall 2294\n",
            "2024-01-06 23:41:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 216:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:21 | INFO | fairseq.trainer | begin training epoch 216\n",
            "2024-01-06 23:41:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 216:  67% 2/3 [00:09<00:04,  4.74s/it]2024-01-06 23:41:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 216 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 216 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:31 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 1.299 | nll_loss 0.28 | ppl 1.21 | wps 2938.9 | wpb 921 | bsz 75 | num_updates 648 | best_loss 1.281\n",
            "2024-01-06 23:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 648 updates\n",
            "2024-01-06 23:41:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 216 @ 648 updates, score 1.299) (writing took 0.1984348019996105 seconds)\n",
            "2024-01-06 23:41:32 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)\n",
            "2024-01-06 23:41:32 | INFO | train | epoch 216 | loss 1.316 | nll_loss 0.47 | ppl 1.39 | wps 830.8 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 648 | lr 0.000648 | gnorm 0.735 | clip 33.3 | train_wall 10 | wall 2304\n",
            "2024-01-06 23:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 217:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:32 | INFO | fairseq.trainer | begin training epoch 217\n",
            "2024-01-06 23:41:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 217:  67% 2/3 [00:04<00:02,  2.54s/it]2024-01-06 23:41:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 217 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 217 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:42 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 1.293 | nll_loss 0.274 | ppl 1.21 | wps 2622.8 | wpb 921 | bsz 75 | num_updates 651 | best_loss 1.281\n",
            "2024-01-06 23:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 651 updates\n",
            "2024-01-06 23:41:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 217 @ 651 updates, score 1.293) (writing took 0.20680715199978295 seconds)\n",
            "2024-01-06 23:41:42 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)\n",
            "2024-01-06 23:41:42 | INFO | train | epoch 217 | loss 1.299 | nll_loss 0.463 | ppl 1.38 | wps 831.5 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 651 | lr 0.000651 | gnorm 0.661 | clip 0 | train_wall 10 | wall 2315\n",
            "2024-01-06 23:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 218:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:42 | INFO | fairseq.trainer | begin training epoch 218\n",
            "2024-01-06 23:41:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 218:  67% 2/3 [00:04<00:02,  2.46s/it]2024-01-06 23:41:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 218 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 218 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:41:52 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 1.288 | nll_loss 0.266 | ppl 1.2 | wps 2121.7 | wpb 921 | bsz 75 | num_updates 654 | best_loss 1.281\n",
            "2024-01-06 23:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 654 updates\n",
            "2024-01-06 23:41:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:41:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 218 @ 654 updates, score 1.288) (writing took 0.25912629999947967 seconds)\n",
            "2024-01-06 23:41:52 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)\n",
            "2024-01-06 23:41:52 | INFO | train | epoch 218 | loss 1.309 | nll_loss 0.474 | ppl 1.39 | wps 852.4 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 654 | lr 0.000654 | gnorm 0.701 | clip 0 | train_wall 9 | wall 2325\n",
            "2024-01-06 23:41:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 219:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:41:52 | INFO | fairseq.trainer | begin training epoch 219\n",
            "2024-01-06 23:41:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 219:  67% 2/3 [00:05<00:03,  3.04s/it]2024-01-06 23:42:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 219 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 219 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:42:03 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 1.274 | nll_loss 0.254 | ppl 1.19 | wps 2736.6 | wpb 921 | bsz 75 | num_updates 657 | best_loss 1.274\n",
            "2024-01-06 23:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 657 updates\n",
            "2024-01-06 23:42:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:42:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 219 @ 657 updates, score 1.274) (writing took 0.3163425370003097 seconds)\n",
            "2024-01-06 23:42:03 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)\n",
            "2024-01-06 23:42:03 | INFO | train | epoch 219 | loss 1.283 | nll_loss 0.441 | ppl 1.36 | wps 840.3 | ups 0.29 | wpb 2928.3 | bsz 233.3 | num_updates 657 | lr 0.000657 | gnorm 0.562 | clip 0 | train_wall 9 | wall 2336\n",
            "2024-01-06 23:42:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 220:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:42:03 | INFO | fairseq.trainer | begin training epoch 220\n",
            "2024-01-06 23:42:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 220:  67% 2/3 [00:05<00:02,  2.32s/it]2024-01-06 23:42:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 220 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 220 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:42:13 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 1.276 | nll_loss 0.258 | ppl 1.2 | wps 2955.5 | wpb 921 | bsz 75 | num_updates 660 | best_loss 1.274\n",
            "2024-01-06 23:42:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 660 updates\n",
            "2024-01-06 23:42:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-06 23:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 220 @ 660 updates, score 1.276) (writing took 0.19053732299926196 seconds)\n",
            "2024-01-06 23:42:13 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)\n",
            "2024-01-06 23:42:13 | INFO | train | epoch 220 | loss 1.271 | nll_loss 0.425 | ppl 1.34 | wps 833.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 660 | lr 0.00066 | gnorm 0.465 | clip 0 | train_wall 10 | wall 2346\n",
            "2024-01-06 23:42:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 221:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:42:13 | INFO | fairseq.trainer | begin training epoch 221\n",
            "2024-01-06 23:42:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 221:  67% 2/3 [00:09<00:04,  4.76s/it]2024-01-06 23:42:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 221 | valid on 'valid' subset:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 221 | valid on 'valid' subset:  50% 1/2 [00:00<00:00,  1.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-01-06 23:42:24 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 1.273 | nll_loss 0.26 | ppl 1.2 | wps 2800.1 | wpb 921 | bsz 75 | num_updates 663 | best_loss 1.273\n",
            "2024-01-06 23:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 663 updates\n",
            "2024-01-06 23:42:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:42:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 23:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 221 @ 663 updates, score 1.273) (writing took 0.39863918399987597 seconds)\n",
            "2024-01-06 23:42:24 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)\n",
            "2024-01-06 23:42:24 | INFO | train | epoch 221 | loss 1.263 | nll_loss 0.42 | ppl 1.34 | wps 822.9 | ups 0.28 | wpb 2928.3 | bsz 233.3 | num_updates 663 | lr 0.000663 | gnorm 0.502 | clip 0 | train_wall 10 | wall 2357\n",
            "2024-01-06 23:42:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3\n",
            "epoch 222:   0% 0/3 [00:00<?, ?it/s]2024-01-06 23:42:24 | INFO | fairseq.trainer | begin training epoch 222\n",
            "2024-01-06 23:42:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 824, in train_step\n",
            "    loss, sample_size_i, logging_output = self.task.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 519, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate predictions on test data\n",
        "\n",
        "Generate predictions on test data - read in all the inputs from tst.esp.input and generate outputs to the file tst.esp.output (this is slow and takes about a minute)"
      ],
      "metadata": {
        "id": "C70_-Z3hGSXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoints/dan-models/checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxe16BNGRzh",
        "outputId": "4f4d4bbb-1b02-448d-b9aa-104e98a27b46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 22:54:42.220435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 22:54:42.220512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 22:54:42.222690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 22:54:42.232371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 22:54:44.180374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-06 22:54:48 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/dan-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-06 22:54:48 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 48 types\n",
            "2024-01-06 22:54:48 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-06 22:54:48 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-06 22:54:49 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-06 22:54:49 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-06 22:55:03 | INFO | fairseq_cli.interactive | Total time: 14.336 seconds; translation time: 13.288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the generated outputs and inputs and display the first 20 side-by-side\n",
        "linesinput = [l.strip() for l in open(\"tst.dan.input\")]\n",
        "linesoutput = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "tuple(zip(linesinput, linesoutput))[:20] # Look at 20 first test inputs and predicted outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vGn473BGQUU",
        "outputId": "7b850ee0-4e99-4322-bc0a-f41920208dbf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('< t a g e t e s > N DEF NOM SG', '< t a g e s t e n s >'),\n",
              " ('< k o r s > N INDF NOM SG', '< k r o s >'),\n",
              " ('< p r o n o m e n > N DEF NOM SG', '< p r o n o n e s >'),\n",
              " ('< b i o l o g i > N INDF GEN PL', '< b i l o g i e r s >'),\n",
              " ('< i n f o r m a t i o n > N INDF GEN PL', '< i n i f o r a m e r s >'),\n",
              " ('< o r d k l a s s e > N INDF NOM PL', '< o r d l a s e r s >'),\n",
              " ('< p r æ s i d e n t > N INDF NOM SG', '< p r æ s i n d e >'),\n",
              " ('< j æ g e r > N DEF NOM SG', '< ø g e r n e s >'),\n",
              " ('< p i l o t > N DEF NOM SG', '< p i l o t e n s >'),\n",
              " ('< p å f u g l > N INDF GEN SG', '< p u f l g s >'),\n",
              " ('< f i l i a l > N INDF GEN SG', '< f i l a l s >'),\n",
              " ('< g a f f e l > N INDF GEN PL', '< g a f e l e r s >'),\n",
              " ('< r o o m i e > N INDF GEN PL', '< r o m e m i e r s >'),\n",
              " ('< n æ s e > N INDF NOM SG', '< n e s >'),\n",
              " ('< m o d e r s m å l > N INDF NOM PL', '< m o d r e s >'),\n",
              " ('< T V - p r o g r a m > N INDF NOM PL', '< h u p r o p r a m e r >'),\n",
              " ('< k r o k o d i l l e > N INDF GEN PL', '< k r o k o d i l e r s >'),\n",
              " ('< p a r t i c i p i u m > N INDF NOM PL', '< p a r i t i p i e r >'),\n",
              " ('< k o m i k e r > N INDF NOM PL', '< k o m i k e r >'),\n",
              " ('< u n g a r e r > N INDF GEN PL', '< u n g r a r e s >'))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate test accuracy and Levensthein distance"
      ],
      "metadata": {
        "id": "fmRuBQm-7CgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the predictions from the checkpoint\n",
        "!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t9HAmFd7CPJ",
        "outputId": "51186ee7-79c4-41c9-f620-6ac9945ee3d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 23:43:07.854789: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 23:43:07.854842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 23:43:07.855757: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 23:43:07.860900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 23:43:08.817805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-06 23:43:12 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-06 23:43:12 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 48 types\n",
            "2024-01-06 23:43:12 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-06 23:43:12 | INFO | fairseq_cli.interactive | loading model(s) from checkpoint_best.pt\n",
            "2024-01-06 23:43:13 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-06 23:43:13 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-06 23:43:28 | INFO | fairseq_cli.interactive | Total time: 15.395 seconds; translation time: 14.268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy between the ground truth and the predictions\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "hits = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "  if pred == ground:\n",
        "    hits += 1\n",
        "  lines += 1\n",
        "\n",
        "print(\"Accuracy: \" + str(hits/lines))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqHpNu-S7gIJ",
        "outputId": "fc2fa126-542b-4c4f-98ac-edfff544d90e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6266666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Levenshtein distance between the ground truth and the predictions\n",
        "\n",
        "# Function from ChatGPT\n",
        "def levenshtein_distance(str1, str2):\n",
        "    len_str1 = len(str1) + 1\n",
        "    len_str2 = len(str2) + 1\n",
        "\n",
        "    # Create a matrix to store the distances\n",
        "    matrix = [[0 for _ in range(len_str2)] for _ in range(len_str1)]\n",
        "\n",
        "    # Initialize the matrix\n",
        "    for i in range(len_str1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len_str2):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    # Fill in the matrix\n",
        "    for i in range(1, len_str1):\n",
        "        for j in range(1, len_str2):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i - 1][j] + 1,        # Deletion\n",
        "                matrix[i][j - 1] + 1,        # Insertion\n",
        "                matrix[i - 1][j - 1] + cost  # Substitution\n",
        "            )\n",
        "\n",
        "    # The bottom-right cell contains the Levenshtein distance\n",
        "    return matrix[-1][-1]\n",
        "\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "distances = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "\n",
        "  distances += levenshtein_distance(pred, ground)\n",
        "  lines += 1\n",
        "\n",
        "print(\"Levenshtein distance: \" + str(distances/lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOwSI5Mw702e",
        "outputId": "4791d8f3-4e21-47a3-e1ca-cfdb8edc53e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levenshtein distance: 1.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance = 0: The strings are identical. No edits are needed.\n",
        "\n",
        "Distance = 1: The strings are very similar. Typically, this means either a single insertion, deletion, or substitution is required to make them identical. For example, \"cat\" and \"cot\" have a Levenshtein distance of 1 because you can transform one into the other by changing a single character.\n",
        "\n",
        "Distance > 1: As the distance increases, the dissimilarity between the strings also increases. A distance of 2 or more indicates a greater degree of dissimilarity, involving multiple edits."
      ],
      "metadata": {
        "id": "Aeqp6-_YDHuY"
      }
    }
  ]
}