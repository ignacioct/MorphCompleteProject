{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnmSkrVyFnc6"
      },
      "source": [
        "# Alpha Model\n",
        "\n",
        "## Inputs & Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L44JJRSjdF8S",
        "outputId": "aea5d49d-b8cb-42e5-8cfe-203066bbf0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAAylMkWdHSj",
        "outputId": "6b8a4cff-c2bf-47f6-ff34-ffcf6083d171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login aa0b9ecff47af231f410704977e504d7928ffb05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "1W8fAQJEdaFW",
        "outputId": "0b7b5356-de09-4017-c336-ef2cfd807f5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mignacioct_\u001b[0m (\u001b[33mignacio_at_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240101_193658-s925wpte</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ignacio_at_ai/danish-inflection/runs/s925wpte' target=\"_blank\">usual-night-1</a></strong> to <a href='https://wandb.ai/ignacio_at_ai/danish-inflection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ignacio_at_ai/danish-inflection/runs/s925wpte' target=\"_blank\">https://wandb.ai/ignacio_at_ai/danish-inflection/runs/s925wpte</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ignacio_at_ai/danish-inflection/runs/s925wpte?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7cc0b7fe3400>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"danish-inflection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBSGWunsEwRs",
        "outputId": "15b8138f-8f56-44f4-f694-3d8a5fc30e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.7)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291796 sha256=a461f9e357cb46134ce5bd78201afacb50c464928c4dc5d9d1521e021075cb8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=bf2d6f56ab0238160c3b3bef9ed2ea152cfa389792d6ee42d97b37c281731643\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OafxnvaWYdsS",
        "outputId": "329377ff-af2b-4de6-b1c5-4b0f70c854c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR7TeZKoF_Qz"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0d-GDOTGBlD",
        "outputId": "0eab73ee-f4a1-4382-9db0-c9a8bf4d63ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-02 21:03:55.754547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 21:03:55.754644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 21:03:55.756908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 21:03:55.768246: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 21:03:57.638333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-02 21:04:00 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='dan.input', target_lang='dan.output', trainpref='train', validpref='dev', testpref=None, align_suffix=None, destdir='data-bin/dan', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-02 21:04:01 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 56 types\n",
            "2024-01-02 21:04:02 | INFO | fairseq_cli.preprocess | [dan.input] train.dan.input: 7842 sents, 112862 tokens, 0.0292% replaced (by <unk>)\n",
            "2024-01-02 21:04:02 | INFO | fairseq_cli.preprocess | [dan.input] Dictionary: 56 types\n",
            "2024-01-02 21:04:02 | INFO | fairseq_cli.preprocess | [dan.input] dev.dan.input: 1680 sents, 24157 tokens, 0.0124% replaced (by <unk>)\n",
            "2024-01-02 21:04:02 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-02 21:04:03 | INFO | fairseq_cli.preprocess | [dan.output] train.dan.output: 7842 sents, 97204 tokens, 0.036% replaced (by <unk>)\n",
            "2024-01-02 21:04:03 | INFO | fairseq_cli.preprocess | [dan.output] Dictionary: 40 types\n",
            "2024-01-02 21:04:03 | INFO | fairseq_cli.preprocess | [dan.output] dev.dan.output: 1680 sents, 20676 tokens, 0.029% replaced (by <unk>)\n",
            "2024-01-02 21:04:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/dan\n"
          ]
        }
      ],
      "source": [
        "!bash ./preprocess.sh dan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i4VwjnlGHG6"
      },
      "source": [
        "## Train\n",
        "\n",
        "Train with default parameters, roughly the baseline in SIGMORPHON 2020 shared task\n",
        "Let this run until the loss on the validation (dev) test no longer improves. (Maybe 10 minutes with a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1iVwFdGIPz",
        "outputId": "5bc9d4bc-8e34-4517-f078-7262e74023f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-01 19:37:36.672152: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 19:37:36.672235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 19:37:36.674361: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 19:37:36.694209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 19:37:39.643565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 19:37:41 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-01 19:37:45 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/dan-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/dan-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/dan', source_lang='dan.input', target_lang='dan.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/dan', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 19:37:45 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 56 types\n",
            "2024-01-01 19:37:45 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(56, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | num. shared model params: 10,553,344 (num. trained: 10,553,344)\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-01 19:37:46 | INFO | fairseq.data.data_utils | loaded 1,680 examples from: data-bin/dan/valid.dan.input-dan.output.dan.input\n",
            "2024-01-01 19:37:46 | INFO | fairseq.data.data_utils | loaded 1,680 examples from: data-bin/dan/valid.dan.input-dan.output.dan.output\n",
            "2024-01-01 19:37:46 | INFO | fairseq.tasks.translation | data-bin/dan valid dan.input-dan.output 1680 examples\n",
            "2024-01-01 19:37:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-01 19:37:46 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-01 19:37:46 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:37:47 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 35 @ 714 updates)\n",
            "2024-01-01 19:37:47 | INFO | fairseq.trainer | loading train data for epoch 35\n",
            "2024-01-01 19:37:47 | INFO | fairseq.data.data_utils | loaded 7,842 examples from: data-bin/dan/train.dan.input-dan.output.dan.input\n",
            "2024-01-01 19:37:47 | INFO | fairseq.data.data_utils | loaded 7,842 examples from: data-bin/dan/train.dan.input-dan.output.dan.output\n",
            "2024-01-01 19:37:47 | INFO | fairseq.tasks.translation | data-bin/dan train dan.input-dan.output 7842 examples\n",
            "2024-01-01 19:37:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 035:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:37:47 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-01-01 19:37:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 035:  95% 20/21 [02:00<00:05,  5.36s/it]2024-01-01 19:39:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  20% 1/5 [00:01<00:07,  1.98s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.03s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.98s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.09s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.60s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:40:04 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 1.224 | nll_loss 0.264 | ppl 1.2 | wps 2470.6 | wpb 4135.2 | bsz 336 | num_updates 735 | best_loss 1.211\n",
            "2024-01-01 19:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 735 updates\n",
            "2024-01-01 19:40:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:40:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 35 @ 735 updates, score 1.224) (writing took 0.327820108999731 seconds)\n",
            "2024-01-01 19:40:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-01-01 19:40:04 | INFO | train | epoch 035 | loss 1.299 | nll_loss 0.468 | ppl 1.38 | wps 715.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 735 | lr 0.000735 | gnorm 0.736 | clip 14.3 | train_wall 128 | wall 139\n",
            "2024-01-01 19:40:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 036:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:40:04 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-01-01 19:40:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  95% 20/21 [02:00<00:05,  5.90s/it]2024-01-01 19:42:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.51s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.97s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.30s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.25s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:42:21 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 1.207 | nll_loss 0.215 | ppl 1.16 | wps 2200.9 | wpb 4135.2 | bsz 336 | num_updates 756 | best_loss 1.207\n",
            "2024-01-01 19:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 756 updates\n",
            "2024-01-01 19:42:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:42:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 36 @ 756 updates, score 1.207) (writing took 0.6080201610002405 seconds)\n",
            "2024-01-01 19:42:21 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-01-01 19:42:21 | INFO | train | epoch 036 | loss 1.295 | nll_loss 0.468 | ppl 1.38 | wps 709.1 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 756 | lr 0.000756 | gnorm 0.726 | clip 9.5 | train_wall 127 | wall 276\n",
            "2024-01-01 19:42:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 037:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:42:21 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-01-01 19:42:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  95% 20/21 [01:57<00:04,  4.83s/it]2024-01-01 19:44:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.35s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.14s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.07s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.24s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:44:35 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 1.211 | nll_loss 0.237 | ppl 1.18 | wps 2407.2 | wpb 4135.2 | bsz 336 | num_updates 777 | best_loss 1.207\n",
            "2024-01-01 19:44:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 777 updates\n",
            "2024-01-01 19:44:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:44:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 37 @ 777 updates, score 1.211) (writing took 0.2994099420002385 seconds)\n",
            "2024-01-01 19:44:35 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-01-01 19:44:35 | INFO | train | epoch 037 | loss 1.227 | nll_loss 0.387 | ppl 1.31 | wps 727.2 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 777 | lr 0.000777 | gnorm 0.509 | clip 4.8 | train_wall 124 | wall 409\n",
            "2024-01-01 19:44:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 038:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:44:35 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-01-01 19:44:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  95% 20/21 [02:01<00:04,  4.32s/it]2024-01-01 19:46:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  20% 1/5 [00:01<00:07,  1.76s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.81s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.17s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.41s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.78s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:46:54 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 1.154 | nll_loss 0.192 | ppl 1.14 | wps 2182.4 | wpb 4135.2 | bsz 336 | num_updates 798 | best_loss 1.154\n",
            "2024-01-01 19:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 798 updates\n",
            "2024-01-01 19:46:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:46:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:46:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 38 @ 798 updates, score 1.154) (writing took 0.9926604679994853 seconds)\n",
            "2024-01-01 19:46:55 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-01-01 19:46:55 | INFO | train | epoch 038 | loss 1.203 | nll_loss 0.363 | ppl 1.29 | wps 696.8 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 798 | lr 0.000798 | gnorm 0.429 | clip 4.8 | train_wall 128 | wall 549\n",
            "2024-01-01 19:46:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 039:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:46:55 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-01-01 19:46:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  95% 20/21 [02:04<00:06,  6.22s/it, loss=1.255, nll_loss=0.42, ppl=1.34, wps=712.8, ups=0.16, wpb=4593.7, bsz=369.4, num_updates=800, lr=0.0008, gnorm=0.634, clip=9.3, train_wall=514, wall=557]2024-01-01 19:49:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.66s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.87s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.85s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.99s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.52s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:49:14 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 1.154 | nll_loss 0.19 | ppl 1.14 | wps 2557.5 | wpb 4135.2 | bsz 336 | num_updates 819 | best_loss 1.154\n",
            "2024-01-01 19:49:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 819 updates\n",
            "2024-01-01 19:49:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:49:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:49:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 39 @ 819 updates, score 1.154) (writing took 0.7388996879999468 seconds)\n",
            "2024-01-01 19:49:15 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-01-01 19:49:15 | INFO | train | epoch 039 | loss 1.205 | nll_loss 0.367 | ppl 1.29 | wps 692.8 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 819 | lr 0.000819 | gnorm 0.529 | clip 4.8 | train_wall 131 | wall 689\n",
            "2024-01-01 19:49:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 040:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:49:15 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-01-01 19:49:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  95% 20/21 [02:06<00:05,  5.78s/it]2024-01-01 19:51:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.17s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.35s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.74s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:51:34 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 1.206 | nll_loss 0.25 | ppl 1.19 | wps 2196.5 | wpb 4135.2 | bsz 336 | num_updates 840 | best_loss 1.154\n",
            "2024-01-01 19:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 840 updates\n",
            "2024-01-01 19:51:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 40 @ 840 updates, score 1.206) (writing took 0.30140738599948236 seconds)\n",
            "2024-01-01 19:51:35 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-01-01 19:51:35 | INFO | train | epoch 040 | loss 1.234 | nll_loss 0.399 | ppl 1.32 | wps 695.3 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 840 | lr 0.00084 | gnorm 0.658 | clip 9.5 | train_wall 130 | wall 829\n",
            "2024-01-01 19:51:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 041:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:51:35 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-01-01 19:51:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  95% 20/21 [01:55<00:05,  5.91s/it]2024-01-01 19:53:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  20% 1/5 [00:01<00:07,  1.77s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  40% 2/5 [00:04<00:07,  2.36s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.20s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.18s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.63s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:53:44 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 1.178 | nll_loss 0.205 | ppl 1.15 | wps 2271.3 | wpb 4135.2 | bsz 336 | num_updates 861 | best_loss 1.154\n",
            "2024-01-01 19:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 861 updates\n",
            "2024-01-01 19:53:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:53:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 19:53:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 41 @ 861 updates, score 1.178) (writing took 0.3203244069991342 seconds)\n",
            "2024-01-01 19:53:44 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-01-01 19:53:44 | INFO | train | epoch 041 | loss 1.274 | nll_loss 0.45 | ppl 1.37 | wps 750.3 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 861 | lr 0.000861 | gnorm 0.656 | clip 9.5 | train_wall 120 | wall 958\n",
            "2024-01-01 19:53:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 042:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:53:44 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-01-01 19:53:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  95% 20/21 [01:55<00:05,  5.70s/it]2024-01-01 19:55:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  20% 1/5 [00:02<00:08,  2.03s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.91s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.86s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.98s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.51s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:55:54 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 1.152 | nll_loss 0.179 | ppl 1.13 | wps 2662.3 | wpb 4135.2 | bsz 336 | num_updates 882 | best_loss 1.152\n",
            "2024-01-01 19:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 882 updates\n",
            "2024-01-01 19:55:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:55:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 42 @ 882 updates, score 1.152) (writing took 0.9029574180003692 seconds)\n",
            "2024-01-01 19:55:55 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-01-01 19:55:55 | INFO | train | epoch 042 | loss 1.221 | nll_loss 0.384 | ppl 1.3 | wps 745.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 882 | lr 0.000882 | gnorm 0.487 | clip 4.8 | train_wall 121 | wall 1089\n",
            "2024-01-01 19:55:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 043:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:55:55 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-01-01 19:55:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  95% 20/21 [02:06<00:05,  5.96s/it, loss=1.224, nll_loss=0.39, ppl=1.31, wps=726.8, ups=0.15, wpb=4697.7, bsz=380.1, num_updates=900, lr=0.0009, gnorm=0.51, clip=5, train_wall=607, wall=1203]2024-01-01 19:58:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  60% 3/5 [00:05<00:04,  2.03s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.38s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 19:58:11 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 1.147 | nll_loss 0.18 | ppl 1.13 | wps 2199.2 | wpb 4135.2 | bsz 336 | num_updates 903 | best_loss 1.147\n",
            "2024-01-01 19:58:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 903 updates\n",
            "2024-01-01 19:58:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 19:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 43 @ 903 updates, score 1.147) (writing took 0.7036570870004653 seconds)\n",
            "2024-01-01 19:58:12 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-01-01 19:58:12 | INFO | train | epoch 043 | loss 1.178 | nll_loss 0.337 | ppl 1.26 | wps 707.7 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 903 | lr 0.000903 | gnorm 0.485 | clip 4.8 | train_wall 127 | wall 1226\n",
            "2024-01-01 19:58:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 044:   0% 0/21 [00:00<?, ?it/s]2024-01-01 19:58:12 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-01-01 19:58:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  95% 20/21 [02:01<00:05,  5.49s/it]2024-01-01 20:00:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.55s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.74s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.91s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 5/5 [00:07<00:00,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:00:28 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 1.144 | nll_loss 0.182 | ppl 1.13 | wps 2676.1 | wpb 4135.2 | bsz 336 | num_updates 924 | best_loss 1.144\n",
            "2024-01-01 20:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 924 updates\n",
            "2024-01-01 20:00:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:00:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 44 @ 924 updates, score 1.144) (writing took 0.6684493789998669 seconds)\n",
            "2024-01-01 20:00:28 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-01-01 20:00:28 | INFO | train | epoch 044 | loss 1.189 | nll_loss 0.352 | ppl 1.28 | wps 713.3 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 924 | lr 0.000924 | gnorm 0.491 | clip 4.8 | train_wall 127 | wall 1362\n",
            "2024-01-01 20:00:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 045:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:00:28 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-01-01 20:00:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  95% 20/21 [01:58<00:07,  7.17s/it]2024-01-01 20:02:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.56s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.28s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.23s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.20s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.65s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:02:43 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 1.138 | nll_loss 0.174 | ppl 1.13 | wps 2210.1 | wpb 4135.2 | bsz 336 | num_updates 945 | best_loss 1.138\n",
            "2024-01-01 20:02:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 945 updates\n",
            "2024-01-01 20:02:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:02:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:02:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 45 @ 945 updates, score 1.138) (writing took 0.5209387099985179 seconds)\n",
            "2024-01-01 20:02:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-01-01 20:02:43 | INFO | train | epoch 045 | loss 1.189 | nll_loss 0.351 | ppl 1.28 | wps 719.9 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 945 | lr 0.000945 | gnorm 0.546 | clip 4.8 | train_wall 125 | wall 1497\n",
            "2024-01-01 20:02:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 046:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:02:43 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-01-01 20:02:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  95% 20/21 [01:59<00:04,  4.58s/it]2024-01-01 20:04:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.75s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.95s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.62s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:04:57 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 1.129 | nll_loss 0.174 | ppl 1.13 | wps 2487.6 | wpb 4135.2 | bsz 336 | num_updates 966 | best_loss 1.129\n",
            "2024-01-01 20:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 966 updates\n",
            "2024-01-01 20:04:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:04:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 46 @ 966 updates, score 1.129) (writing took 0.7448678910004674 seconds)\n",
            "2024-01-01 20:04:58 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-01-01 20:04:58 | INFO | train | epoch 046 | loss 1.172 | nll_loss 0.332 | ppl 1.26 | wps 723.9 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 966 | lr 0.000966 | gnorm 0.531 | clip 4.8 | train_wall 124 | wall 1632\n",
            "2024-01-01 20:04:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 047:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:04:58 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-01-01 20:04:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  95% 20/21 [02:00<00:06,  6.10s/it]2024-01-01 20:07:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  20% 1/5 [00:01<00:07,  1.98s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  40% 2/5 [00:04<00:07,  2.44s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  60% 3/5 [00:07<00:04,  2.40s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  80% 4/5 [00:10<00:02,  2.87s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 5/5 [00:11<00:00,  2.14s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:07:13 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 1.122 | nll_loss 0.164 | ppl 1.12 | wps 1807.7 | wpb 4135.2 | bsz 336 | num_updates 987 | best_loss 1.122\n",
            "2024-01-01 20:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 987 updates\n",
            "2024-01-01 20:07:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:07:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:07:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 47 @ 987 updates, score 1.122) (writing took 1.9128821349986538 seconds)\n",
            "2024-01-01 20:07:15 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-01-01 20:07:15 | INFO | train | epoch 047 | loss 1.169 | nll_loss 0.332 | ppl 1.26 | wps 705.9 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 987 | lr 0.000987 | gnorm 0.472 | clip 4.8 | train_wall 124 | wall 1769\n",
            "2024-01-01 20:07:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 048:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:07:15 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-01-01 20:07:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  95% 20/21 [02:03<00:06,  6.82s/it, loss=1.178, nll_loss=0.34, ppl=1.27, wps=707.3, ups=0.16, wpb=4524.3, bsz=368.1, num_updates=1000, lr=0.001, gnorm=0.552, clip=6, train_wall=586, wall=1843]2024-01-01 20:09:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.31s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.09s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  60% 3/5 [00:06<00:03,  1.96s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.04s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.54s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:09:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 1.131 | nll_loss 0.181 | ppl 1.13 | wps 2624.2 | wpb 4135.2 | bsz 336 | num_updates 1008 | best_loss 1.122\n",
            "2024-01-01 20:09:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1008 updates\n",
            "2024-01-01 20:09:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 48 @ 1008 updates, score 1.131) (writing took 0.26855977100058226 seconds)\n",
            "2024-01-01 20:09:35 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-01-01 20:09:35 | INFO | train | epoch 048 | loss 1.156 | nll_loss 0.316 | ppl 1.24 | wps 696.8 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1008 | lr 0.000996024 | gnorm 0.501 | clip 4.8 | train_wall 130 | wall 1909\n",
            "2024-01-01 20:09:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 049:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:09:35 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-01-01 20:09:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  95% 20/21 [01:56<00:05,  5.31s/it]2024-01-01 20:11:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.72s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.76s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  80% 4/5 [00:07<00:02,  2.17s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:11:48 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 1.164 | nll_loss 0.22 | ppl 1.16 | wps 2299.2 | wpb 4135.2 | bsz 336 | num_updates 1029 | best_loss 1.122\n",
            "2024-01-01 20:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1029 updates\n",
            "2024-01-01 20:11:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 49 @ 1029 updates, score 1.164) (writing took 0.4230387530005828 seconds)\n",
            "2024-01-01 20:11:49 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-01-01 20:11:49 | INFO | train | epoch 049 | loss 1.18 | nll_loss 0.345 | ppl 1.27 | wps 726.1 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1029 | lr 0.000985808 | gnorm 0.497 | clip 4.8 | train_wall 124 | wall 2043\n",
            "2024-01-01 20:11:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 050:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:11:49 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-01-01 20:11:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  95% 20/21 [01:57<00:04,  4.76s/it]2024-01-01 20:13:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.35s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.28s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.06s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.22s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.66s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:14:03 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 1.157 | nll_loss 0.209 | ppl 1.16 | wps 2406.6 | wpb 4135.2 | bsz 336 | num_updates 1050 | best_loss 1.122\n",
            "2024-01-01 20:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1050 updates\n",
            "2024-01-01 20:14:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:14:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:14:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 50 @ 1050 updates, score 1.157) (writing took 0.2561421459995472 seconds)\n",
            "2024-01-01 20:14:03 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-01-01 20:14:03 | INFO | train | epoch 050 | loss 1.186 | nll_loss 0.353 | ppl 1.28 | wps 724.7 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1050 | lr 0.0009759 | gnorm 0.507 | clip 9.5 | train_wall 124 | wall 2177\n",
            "2024-01-01 20:14:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 051:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:14:03 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-01-01 20:14:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  95% 20/21 [01:58<00:05,  5.65s/it]2024-01-01 20:16:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.44s/it]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.93s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:16:16 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 1.116 | nll_loss 0.168 | ppl 1.12 | wps 2107.4 | wpb 4135.2 | bsz 336 | num_updates 1071 | best_loss 1.116\n",
            "2024-01-01 20:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1071 updates\n",
            "2024-01-01 20:16:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:16:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:16:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 51 @ 1071 updates, score 1.116) (writing took 0.6908848590010166 seconds)\n",
            "2024-01-01 20:16:17 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-01-01 20:16:17 | INFO | train | epoch 051 | loss 1.176 | nll_loss 0.34 | ppl 1.27 | wps 724.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1071 | lr 0.000966285 | gnorm 0.423 | clip 4.8 | train_wall 123 | wall 2311\n",
            "2024-01-01 20:16:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 052:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:16:17 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-01-01 20:16:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  95% 20/21 [02:03<00:05,  5.98s/it]2024-01-01 20:18:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.72s/it]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.76s/it]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.93s/it]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:18:37 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 1.118 | nll_loss 0.169 | ppl 1.12 | wps 2640.6 | wpb 4135.2 | bsz 336 | num_updates 1092 | best_loss 1.116\n",
            "2024-01-01 20:18:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1092 updates\n",
            "2024-01-01 20:18:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:18:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 52 @ 1092 updates, score 1.118) (writing took 0.31067627300035383 seconds)\n",
            "2024-01-01 20:18:37 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-01-01 20:18:37 | INFO | train | epoch 052 | loss 1.15 | nll_loss 0.309 | ppl 1.24 | wps 693.7 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1092 | lr 0.000956949 | gnorm 0.467 | clip 4.8 | train_wall 131 | wall 2451\n",
            "2024-01-01 20:18:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 053:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:18:37 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-01-01 20:18:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  95% 20/21 [01:57<00:05,  5.37s/it, loss=1.167, nll_loss=0.33, ppl=1.26, wps=718, ups=0.15, wpb=4748.6, bsz=376.1, num_updates=1100, lr=0.000953463, gnorm=0.443, clip=5, train_wall=612, wall=2504]2024-01-01 20:20:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  40% 2/5 [00:03<00:06,  2.03s/it]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.32s/it]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.27s/it]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.70s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:20:51 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 1.114 | nll_loss 0.165 | ppl 1.12 | wps 2173.2 | wpb 4135.2 | bsz 336 | num_updates 1113 | best_loss 1.114\n",
            "2024-01-01 20:20:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1113 updates\n",
            "2024-01-01 20:20:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:20:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 53 @ 1113 updates, score 1.114) (writing took 0.4849444339997717 seconds)\n",
            "2024-01-01 20:20:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-01-01 20:20:51 | INFO | train | epoch 053 | loss 1.139 | nll_loss 0.298 | ppl 1.23 | wps 723.4 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1113 | lr 0.000947878 | gnorm 0.438 | clip 4.8 | train_wall 124 | wall 2586\n",
            "2024-01-01 20:20:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 054:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:20:51 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-01-01 20:20:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  95% 20/21 [01:55<00:05,  5.99s/it]2024-01-01 20:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.94s/it]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:23:03 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 1.106 | nll_loss 0.171 | ppl 1.13 | wps 2629.7 | wpb 4135.2 | bsz 336 | num_updates 1134 | best_loss 1.106\n",
            "2024-01-01 20:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1134 updates\n",
            "2024-01-01 20:23:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:23:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:23:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 54 @ 1134 updates, score 1.106) (writing took 0.5702993490012886 seconds)\n",
            "2024-01-01 20:23:04 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-01-01 20:23:04 | INFO | train | epoch 054 | loss 1.139 | nll_loss 0.3 | ppl 1.23 | wps 735.8 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1134 | lr 0.00093906 | gnorm 0.489 | clip 4.8 | train_wall 123 | wall 2718\n",
            "2024-01-01 20:23:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 055:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:23:04 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-01-01 20:23:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  95% 20/21 [01:59<00:05,  5.75s/it]2024-01-01 20:25:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.72s/it]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.93s/it]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.42s/it]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.80s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:25:17 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 1.099 | nll_loss 0.161 | ppl 1.12 | wps 2178 | wpb 4135.2 | bsz 336 | num_updates 1155 | best_loss 1.099\n",
            "2024-01-01 20:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1155 updates\n",
            "2024-01-01 20:25:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:25:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:25:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 55 @ 1155 updates, score 1.099) (writing took 0.5309796940000524 seconds)\n",
            "2024-01-01 20:25:17 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-01-01 20:25:17 | INFO | train | epoch 055 | loss 1.126 | nll_loss 0.283 | ppl 1.22 | wps 726.9 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1155 | lr 0.000930484 | gnorm 0.44 | clip 4.8 | train_wall 123 | wall 2851\n",
            "2024-01-01 20:25:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 056:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:25:17 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-01-01 20:25:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  95% 20/21 [02:01<00:05,  5.17s/it]2024-01-01 20:27:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.22s/it]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  60% 3/5 [00:07<00:05,  2.86s/it]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  80% 4/5 [00:11<00:03,  3.10s/it]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset: 100% 5/5 [00:12<00:00,  2.35s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:27:39 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 1.099 | nll_loss 0.161 | ppl 1.12 | wps 1595.1 | wpb 4135.2 | bsz 336 | num_updates 1176 | best_loss 1.099\n",
            "2024-01-01 20:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1176 updates\n",
            "2024-01-01 20:27:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:27:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 56 @ 1176 updates, score 1.099) (writing took 0.9484260740009631 seconds)\n",
            "2024-01-01 20:27:40 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-01-01 20:27:40 | INFO | train | epoch 056 | loss 1.121 | nll_loss 0.281 | ppl 1.22 | wps 682.9 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1176 | lr 0.000922139 | gnorm 0.44 | clip 4.8 | train_wall 128 | wall 2994\n",
            "2024-01-01 20:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 057:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:27:40 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-01-01 20:27:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  95% 20/21 [01:58<00:04,  4.68s/it]2024-01-01 20:29:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  20% 1/5 [00:01<00:07,  1.98s/it]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.93s/it]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.89s/it]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  2.00s/it]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.52s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:29:53 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 1.12 | nll_loss 0.188 | ppl 1.14 | wps 2620.3 | wpb 4135.2 | bsz 336 | num_updates 1197 | best_loss 1.099\n",
            "2024-01-01 20:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1197 updates\n",
            "2024-01-01 20:29:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:29:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 57 @ 1197 updates, score 1.12) (writing took 0.297310160998677 seconds)\n",
            "2024-01-01 20:29:53 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-01-01 20:29:53 | INFO | train | epoch 057 | loss 1.129 | nll_loss 0.291 | ppl 1.22 | wps 728.2 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1197 | lr 0.000914014 | gnorm 0.358 | clip 4.8 | train_wall 124 | wall 3127\n",
            "2024-01-01 20:29:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 058:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:29:53 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-01-01 20:29:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  95% 20/21 [01:59<00:06,  6.49s/it, loss=1.129, nll_loss=0.289, ppl=1.22, wps=713.7, ups=0.16, wpb=4593.1, bsz=373.7, num_updates=1200, lr=0.000912871, gnorm=0.441, clip=5, train_wall=590, wall=3148]2024-01-01 20:32:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.99s/it]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.32s/it]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.27s/it]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:32:09 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 1.139 | nll_loss 0.206 | ppl 1.15 | wps 2185.4 | wpb 4135.2 | bsz 336 | num_updates 1218 | best_loss 1.099\n",
            "2024-01-01 20:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1218 updates\n",
            "2024-01-01 20:32:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:32:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:32:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 58 @ 1218 updates, score 1.139) (writing took 0.2996658930005651 seconds)\n",
            "2024-01-01 20:32:09 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-01-01 20:32:09 | INFO | train | epoch 058 | loss 1.149 | nll_loss 0.314 | ppl 1.24 | wps 712.8 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1218 | lr 0.0009061 | gnorm 0.828 | clip 9.5 | train_wall 126 | wall 3264\n",
            "2024-01-01 20:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 059:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:32:09 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-01-01 20:32:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  95% 20/21 [01:57<00:05,  5.99s/it]2024-01-01 20:34:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.55s/it]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.74s/it]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.79s/it]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.95s/it]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:34:22 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 1.109 | nll_loss 0.177 | ppl 1.13 | wps 2609.2 | wpb 4135.2 | bsz 336 | num_updates 1239 | best_loss 1.099\n",
            "2024-01-01 20:34:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1239 updates\n",
            "2024-01-01 20:34:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 59 @ 1239 updates, score 1.109) (writing took 0.30256986699896515 seconds)\n",
            "2024-01-01 20:34:22 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-01-01 20:34:22 | INFO | train | epoch 059 | loss 1.165 | nll_loss 0.333 | ppl 1.26 | wps 731.5 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1239 | lr 0.000898389 | gnorm 0.538 | clip 9.5 | train_wall 124 | wall 3397\n",
            "2024-01-01 20:34:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 060:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:34:22 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-01-01 20:34:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  95% 20/21 [01:59<00:07,  7.55s/it]2024-01-01 20:36:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.51s/it]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  60% 3/5 [00:05<00:04,  2.09s/it]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.37s/it]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:36:36 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 1.105 | nll_loss 0.157 | ppl 1.11 | wps 2187.4 | wpb 4135.2 | bsz 336 | num_updates 1260 | best_loss 1.099\n",
            "2024-01-01 20:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1260 updates\n",
            "2024-01-01 20:36:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:36:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 60 @ 1260 updates, score 1.105) (writing took 0.28052867199949105 seconds)\n",
            "2024-01-01 20:36:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-01-01 20:36:36 | INFO | train | epoch 060 | loss 1.143 | nll_loss 0.308 | ppl 1.24 | wps 726.3 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1260 | lr 0.000890871 | gnorm 0.476 | clip 4.8 | train_wall 124 | wall 3530\n",
            "2024-01-01 20:36:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 061:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:36:36 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-01-01 20:36:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  95% 20/21 [01:56<00:06,  6.21s/it]2024-01-01 20:38:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.38s/it]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.25s/it]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.06s/it]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.12s/it]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.60s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:38:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 1.096 | nll_loss 0.161 | ppl 1.12 | wps 2499.1 | wpb 4135.2 | bsz 336 | num_updates 1281 | best_loss 1.096\n",
            "2024-01-01 20:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1281 updates\n",
            "2024-01-01 20:38:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:38:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 61 @ 1281 updates, score 1.096) (writing took 0.5574848139985988 seconds)\n",
            "2024-01-01 20:38:51 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-01-01 20:38:51 | INFO | train | epoch 061 | loss 1.12 | nll_loss 0.277 | ppl 1.21 | wps 718.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1281 | lr 0.000883538 | gnorm 0.345 | clip 4.8 | train_wall 125 | wall 3666\n",
            "2024-01-01 20:38:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 062:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:38:51 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-01-01 20:38:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  95% 20/21 [01:59<00:06,  6.49s/it, loss=1.14, nll_loss=0.303, ppl=1.23, wps=729.3, ups=0.16, wpb=4603.1, bsz=372.1, num_updates=1300, lr=0.000877058, gnorm=0.536, clip=7, train_wall=592, wall=3779]2024-01-01 20:40:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.51s/it]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.72s/it]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.27s/it]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:41:06 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 1.093 | nll_loss 0.153 | ppl 1.11 | wps 2229.4 | wpb 4135.2 | bsz 336 | num_updates 1302 | best_loss 1.093\n",
            "2024-01-01 20:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1302 updates\n",
            "2024-01-01 20:41:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 62 @ 1302 updates, score 1.093) (writing took 0.8401064460013004 seconds)\n",
            "2024-01-01 20:41:06 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-01-01 20:41:06 | INFO | train | epoch 062 | loss 1.115 | nll_loss 0.276 | ppl 1.21 | wps 720.1 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1302 | lr 0.000876384 | gnorm 0.439 | clip 4.8 | train_wall 125 | wall 3801\n",
            "2024-01-01 20:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 063:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:41:06 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-01-01 20:41:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  95% 20/21 [01:59<00:06,  6.28s/it]2024-01-01 20:43:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.31s/it]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.06s/it]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  60% 3/5 [00:06<00:03,  1.95s/it]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.05s/it]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.57s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:43:20 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 1.092 | nll_loss 0.15 | ppl 1.11 | wps 2606 | wpb 4135.2 | bsz 336 | num_updates 1323 | best_loss 1.092\n",
            "2024-01-01 20:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1323 updates\n",
            "2024-01-01 20:43:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 63 @ 1323 updates, score 1.092) (writing took 0.5933987169992179 seconds)\n",
            "2024-01-01 20:43:20 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-01-01 20:43:20 | INFO | train | epoch 063 | loss 1.11 | nll_loss 0.266 | ppl 1.2 | wps 725.3 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1323 | lr 0.000869401 | gnorm 0.356 | clip 4.8 | train_wall 124 | wall 3935\n",
            "2024-01-01 20:43:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 064:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:43:20 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-01-01 20:43:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  95% 20/21 [02:01<00:06,  6.82s/it]2024-01-01 20:45:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.95s/it]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.42s/it]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.79s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:45:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 1.09 | nll_loss 0.149 | ppl 1.11 | wps 2180.3 | wpb 4135.2 | bsz 336 | num_updates 1344 | best_loss 1.09\n",
            "2024-01-01 20:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1344 updates\n",
            "2024-01-01 20:45:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:45:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 64 @ 1344 updates, score 1.09) (writing took 0.5196049139995012 seconds)\n",
            "2024-01-01 20:45:38 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-01-01 20:45:38 | INFO | train | epoch 064 | loss 1.109 | nll_loss 0.27 | ppl 1.21 | wps 708.9 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1344 | lr 0.000862582 | gnorm 0.305 | clip 4.8 | train_wall 127 | wall 4072\n",
            "2024-01-01 20:45:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 065:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:45:38 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-01-01 20:45:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  95% 20/21 [02:04<00:06,  6.86s/it]2024-01-01 20:47:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.67s/it]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.78s/it]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.78s/it]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  80% 4/5 [00:09<00:02,  2.69s/it]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset: 100% 5/5 [00:10<00:00,  2.00s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:47:52 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 1.091 | nll_loss 0.151 | ppl 1.11 | wps 2025.9 | wpb 4135.2 | bsz 336 | num_updates 1365 | best_loss 1.09\n",
            "2024-01-01 20:47:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 1365 updates\n",
            "2024-01-01 20:47:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 20:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 65 @ 1365 updates, score 1.091) (writing took 1.0806954269992275 seconds)\n",
            "2024-01-01 20:47:53 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-01-01 20:47:53 | INFO | train | epoch 065 | loss 1.105 | nll_loss 0.264 | ppl 1.2 | wps 715.2 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1365 | lr 0.000855921 | gnorm 0.36 | clip 4.8 | train_wall 124 | wall 4208\n",
            "2024-01-01 20:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 066:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:47:54 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-01-01 20:47:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  95% 20/21 [02:05<00:05,  5.89s/it]2024-01-01 20:50:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.32s/it]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.11s/it]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  60% 3/5 [00:06<00:03,  2.00s/it]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.08s/it]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:50:14 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 1.09 | nll_loss 0.158 | ppl 1.12 | wps 2561.6 | wpb 4135.2 | bsz 336 | num_updates 1386 | best_loss 1.09\n",
            "2024-01-01 20:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 1386 updates\n",
            "2024-01-01 20:50:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:50:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:50:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 66 @ 1386 updates, score 1.09) (writing took 0.4486635600005684 seconds)\n",
            "2024-01-01 20:50:14 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-01-01 20:50:14 | INFO | train | epoch 066 | loss 1.103 | nll_loss 0.262 | ppl 1.2 | wps 690.7 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1386 | lr 0.000849412 | gnorm 0.397 | clip 4.8 | train_wall 130 | wall 4348\n",
            "2024-01-01 20:50:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 067:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:50:14 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-01-01 20:50:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  95% 20/21 [01:57<00:05,  5.32s/it, loss=1.106, nll_loss=0.265, ppl=1.2, wps=704.4, ups=0.15, wpb=4603.8, bsz=372.1, num_updates=1400, lr=0.000845154, gnorm=0.36, clip=5, train_wall=600, wall=4433]2024-01-01 20:52:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.75s/it]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.21s/it]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.38s/it]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.77s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:52:30 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 1.089 | nll_loss 0.149 | ppl 1.11 | wps 2158.9 | wpb 4135.2 | bsz 336 | num_updates 1407 | best_loss 1.089\n",
            "2024-01-01 20:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1407 updates\n",
            "2024-01-01 20:52:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 67 @ 1407 updates, score 1.089) (writing took 0.5248735629993462 seconds)\n",
            "2024-01-01 20:52:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-01-01 20:52:31 | INFO | train | epoch 067 | loss 1.108 | nll_loss 0.271 | ppl 1.21 | wps 711.4 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1407 | lr 0.000843049 | gnorm 0.357 | clip 4.8 | train_wall 126 | wall 4485\n",
            "2024-01-01 20:52:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 068:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:52:31 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-01-01 20:52:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  95% 20/21 [02:02<00:05,  5.38s/it]2024-01-01 20:54:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.51s/it]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.72s/it]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  80% 4/5 [00:07<00:02,  2.07s/it]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:54:47 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 1.083 | nll_loss 0.151 | ppl 1.11 | wps 2376.2 | wpb 4135.2 | bsz 336 | num_updates 1428 | best_loss 1.083\n",
            "2024-01-01 20:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1428 updates\n",
            "2024-01-01 20:54:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 68 @ 1428 updates, score 1.083) (writing took 0.7491806559992256 seconds)\n",
            "2024-01-01 20:54:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-01-01 20:54:48 | INFO | train | epoch 068 | loss 1.103 | nll_loss 0.262 | ppl 1.2 | wps 708.3 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1428 | lr 0.000836827 | gnorm 0.334 | clip 4.8 | train_wall 127 | wall 4622\n",
            "2024-01-01 20:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 069:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:54:48 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-01-01 20:54:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  95% 20/21 [01:59<00:05,  5.91s/it]2024-01-01 20:56:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.26s/it]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.16s/it]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.00s/it]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.08s/it]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:57:03 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 1.082 | nll_loss 0.15 | ppl 1.11 | wps 2541.3 | wpb 4135.2 | bsz 336 | num_updates 1449 | best_loss 1.082\n",
            "2024-01-01 20:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1449 updates\n",
            "2024-01-01 20:57:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:57:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 69 @ 1449 updates, score 1.082) (writing took 0.9069663930004026 seconds)\n",
            "2024-01-01 20:57:04 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-01-01 20:57:04 | INFO | train | epoch 069 | loss 1.122 | nll_loss 0.286 | ppl 1.22 | wps 717.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1449 | lr 0.000830741 | gnorm 0.391 | clip 4.8 | train_wall 125 | wall 4758\n",
            "2024-01-01 20:57:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 070:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:57:04 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-01-01 20:57:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  95% 20/21 [02:03<00:05,  5.31s/it]2024-01-01 20:59:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.97s/it]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.44s/it]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 20:59:21 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 1.079 | nll_loss 0.154 | ppl 1.11 | wps 2156.9 | wpb 4135.2 | bsz 336 | num_updates 1470 | best_loss 1.079\n",
            "2024-01-01 20:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1470 updates\n",
            "2024-01-01 20:59:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:59:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 20:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 70 @ 1470 updates, score 1.079) (writing took 0.6138570540006185 seconds)\n",
            "2024-01-01 20:59:22 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-01-01 20:59:22 | INFO | train | epoch 070 | loss 1.106 | nll_loss 0.265 | ppl 1.2 | wps 703.5 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1470 | lr 0.000824786 | gnorm 0.303 | clip 4.8 | train_wall 128 | wall 4896\n",
            "2024-01-01 20:59:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 071:   0% 0/21 [00:00<?, ?it/s]2024-01-01 20:59:22 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-01-01 20:59:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  95% 20/21 [01:55<00:06,  6.60s/it]2024-01-01 21:01:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.42s/it]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.16s/it]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.14s/it]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.17s/it]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.64s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:01:34 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 1.084 | nll_loss 0.154 | ppl 1.11 | wps 2458.5 | wpb 4135.2 | bsz 336 | num_updates 1491 | best_loss 1.079\n",
            "2024-01-01 21:01:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1491 updates\n",
            "2024-01-01 21:01:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 71 @ 1491 updates, score 1.084) (writing took 0.2767537910003739 seconds)\n",
            "2024-01-01 21:01:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-01-01 21:01:34 | INFO | train | epoch 071 | loss 1.101 | nll_loss 0.26 | ppl 1.2 | wps 733.5 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1491 | lr 0.000818957 | gnorm 0.453 | clip 4.8 | train_wall 122 | wall 5028\n",
            "2024-01-01 21:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 072:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:01:34 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-01-01 21:01:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  95% 20/21 [01:56<00:06,  6.50s/it, loss=1.106, nll_loss=0.266, ppl=1.2, wps=719.3, ups=0.15, wpb=4681.1, bsz=377.7, num_updates=1500, lr=0.000816497, gnorm=0.347, clip=4, train_wall=599, wall=5083]2024-01-01 21:03:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.76s/it]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.93s/it]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.56s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:03:49 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 1.079 | nll_loss 0.142 | ppl 1.1 | wps 2548.9 | wpb 4135.2 | bsz 336 | num_updates 1512 | best_loss 1.079\n",
            "2024-01-01 21:03:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1512 updates\n",
            "2024-01-01 21:03:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:03:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 72 @ 1512 updates, score 1.079) (writing took 0.8023459609994461 seconds)\n",
            "2024-01-01 21:03:50 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-01-01 21:03:50 | INFO | train | epoch 072 | loss 1.09 | nll_loss 0.249 | ppl 1.19 | wps 718.4 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1512 | lr 0.00081325 | gnorm 0.299 | clip 4.8 | train_wall 126 | wall 5164\n",
            "2024-01-01 21:03:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 073:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:03:50 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-01-01 21:03:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  95% 20/21 [02:00<00:04,  4.28s/it]2024-01-01 21:05:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.57s/it]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.94s/it]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.32s/it]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.33s/it]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.74s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:06:05 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 1.075 | nll_loss 0.145 | ppl 1.11 | wps 2148.7 | wpb 4135.2 | bsz 336 | num_updates 1533 | best_loss 1.075\n",
            "2024-01-01 21:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1533 updates\n",
            "2024-01-01 21:06:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:06:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:06:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 73 @ 1533 updates, score 1.075) (writing took 0.5400955080003769 seconds)\n",
            "2024-01-01 21:06:05 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-01-01 21:06:05 | INFO | train | epoch 073 | loss 1.088 | nll_loss 0.246 | ppl 1.19 | wps 715.4 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1533 | lr 0.000807661 | gnorm 0.351 | clip 4.8 | train_wall 125 | wall 5300\n",
            "2024-01-01 21:06:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 074:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:06:05 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-01-01 21:06:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  95% 20/21 [01:58<00:06,  6.32s/it]2024-01-01 21:08:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.98s/it]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.64s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:08:21 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 1.076 | nll_loss 0.145 | ppl 1.11 | wps 2450.7 | wpb 4135.2 | bsz 336 | num_updates 1554 | best_loss 1.075\n",
            "2024-01-01 21:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 1554 updates\n",
            "2024-01-01 21:08:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:08:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 74 @ 1554 updates, score 1.076) (writing took 0.44488079400071 seconds)\n",
            "2024-01-01 21:08:21 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-01-01 21:08:21 | INFO | train | epoch 074 | loss 1.086 | nll_loss 0.243 | ppl 1.18 | wps 716.2 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1554 | lr 0.000802185 | gnorm 0.254 | clip 4.8 | train_wall 126 | wall 5435\n",
            "2024-01-01 21:08:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 075:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:08:21 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-01-01 21:08:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  95% 20/21 [01:58<00:05,  5.00s/it]2024-01-01 21:10:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  60% 3/5 [00:05<00:04,  2.01s/it]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.41s/it]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.78s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:10:35 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 1.074 | nll_loss 0.143 | ppl 1.1 | wps 2178.7 | wpb 4135.2 | bsz 336 | num_updates 1575 | best_loss 1.074\n",
            "2024-01-01 21:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 1575 updates\n",
            "2024-01-01 21:10:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 75 @ 1575 updates, score 1.074) (writing took 0.7005415000003268 seconds)\n",
            "2024-01-01 21:10:36 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-01-01 21:10:36 | INFO | train | epoch 075 | loss 1.086 | nll_loss 0.245 | ppl 1.18 | wps 721.3 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1575 | lr 0.000796819 | gnorm 0.266 | clip 0 | train_wall 124 | wall 5570\n",
            "2024-01-01 21:10:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 076:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:10:36 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-01-01 21:10:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  95% 20/21 [02:04<00:06,  6.17s/it]2024-01-01 21:12:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  20% 1/5 [00:02<00:08,  2.09s/it]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  40% 2/5 [00:04<00:05,  2.00s/it]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.91s/it]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.01s/it]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.53s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:12:49 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 1.073 | nll_loss 0.143 | ppl 1.1 | wps 2614.1 | wpb 4135.2 | bsz 336 | num_updates 1596 | best_loss 1.073\n",
            "2024-01-01 21:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 1596 updates\n",
            "2024-01-01 21:12:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 76 @ 1596 updates, score 1.073) (writing took 0.5485191400002805 seconds)\n",
            "2024-01-01 21:12:50 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-01-01 21:12:50 | INFO | train | epoch 076 | loss 1.088 | nll_loss 0.246 | ppl 1.19 | wps 726.3 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1596 | lr 0.000791559 | gnorm 0.319 | clip 4.8 | train_wall 124 | wall 5704\n",
            "2024-01-01 21:12:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 077:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:12:50 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-01-01 21:12:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  95% 20/21 [01:56<00:06,  6.04s/it, loss=1.088, nll_loss=0.246, ppl=1.19, wps=714.4, ups=0.16, wpb=4597.2, bsz=368.1, num_updates=1600, lr=0.000790569, gnorm=0.309, clip=4, train_wall=594, wall=5727]2024-01-01 21:14:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.74s/it]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.83s/it]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.40s/it]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.85s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:15:04 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 1.074 | nll_loss 0.149 | ppl 1.11 | wps 2163.6 | wpb 4135.2 | bsz 336 | num_updates 1617 | best_loss 1.073\n",
            "2024-01-01 21:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 1617 updates\n",
            "2024-01-01 21:15:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 77 @ 1617 updates, score 1.074) (writing took 0.3166066129997489 seconds)\n",
            "2024-01-01 21:15:04 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-01-01 21:15:04 | INFO | train | epoch 077 | loss 1.085 | nll_loss 0.241 | ppl 1.18 | wps 724.4 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1617 | lr 0.000786403 | gnorm 0.259 | clip 0 | train_wall 124 | wall 5838\n",
            "2024-01-01 21:15:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 078:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:15:04 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-01-01 21:15:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  95% 20/21 [01:57<00:06,  6.31s/it]2024-01-01 21:17:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.32s/it]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.07s/it]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  60% 3/5 [00:06<00:03,  1.96s/it]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.06s/it]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.56s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:17:17 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 1.074 | nll_loss 0.15 | ppl 1.11 | wps 2602.1 | wpb 4135.2 | bsz 336 | num_updates 1638 | best_loss 1.073\n",
            "2024-01-01 21:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 1638 updates\n",
            "2024-01-01 21:17:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 78 @ 1638 updates, score 1.074) (writing took 0.3205743580001581 seconds)\n",
            "2024-01-01 21:17:17 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-01-01 21:17:17 | INFO | train | epoch 078 | loss 1.084 | nll_loss 0.243 | ppl 1.18 | wps 728.5 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1638 | lr 0.000781345 | gnorm 0.302 | clip 4.8 | train_wall 124 | wall 5972\n",
            "2024-01-01 21:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 079:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:17:17 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-01-01 21:17:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  95% 20/21 [01:59<00:05,  5.16s/it]2024-01-01 21:19:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.55s/it]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.80s/it]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.25s/it]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.38s/it]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:19:34 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 1.076 | nll_loss 0.152 | ppl 1.11 | wps 2149.8 | wpb 4135.2 | bsz 336 | num_updates 1659 | best_loss 1.073\n",
            "2024-01-01 21:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 1659 updates\n",
            "2024-01-01 21:19:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:19:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:19:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 79 @ 1659 updates, score 1.076) (writing took 0.31154824100121914 seconds)\n",
            "2024-01-01 21:19:34 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-01-01 21:19:34 | INFO | train | epoch 079 | loss 1.081 | nll_loss 0.24 | ppl 1.18 | wps 711.5 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1659 | lr 0.000776384 | gnorm 0.315 | clip 4.8 | train_wall 126 | wall 6108\n",
            "2024-01-01 21:19:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 080:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:19:34 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-01-01 21:19:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  95% 20/21 [02:05<00:06,  6.59s/it]2024-01-01 21:21:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.78s/it]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.40s/it]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.86s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:21:53 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 1.078 | nll_loss 0.154 | ppl 1.11 | wps 2170.7 | wpb 4135.2 | bsz 336 | num_updates 1680 | best_loss 1.073\n",
            "2024-01-01 21:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1680 updates\n",
            "2024-01-01 21:21:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:21:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:21:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 80 @ 1680 updates, score 1.078) (writing took 0.31579536099889083 seconds)\n",
            "2024-01-01 21:21:54 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-01-01 21:21:54 | INFO | train | epoch 080 | loss 1.089 | nll_loss 0.25 | ppl 1.19 | wps 695.2 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1680 | lr 0.000771517 | gnorm 0.308 | clip 4.8 | train_wall 129 | wall 6248\n",
            "2024-01-01 21:21:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 081:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:21:54 | INFO | fairseq.trainer | begin training epoch 81\n",
            "2024-01-01 21:21:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 081:  95% 20/21 [02:04<00:04,  4.84s/it, loss=1.086, nll_loss=0.245, ppl=1.19, wps=720.6, ups=0.15, wpb=4651.1, bsz=376.1, num_updates=1700, lr=0.000766965, gnorm=0.312, clip=4, train_wall=604, wall=6372]2024-01-01 21:24:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 081 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.54s/it]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.93s/it]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:24:12 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 1.076 | nll_loss 0.145 | ppl 1.11 | wps 2628.5 | wpb 4135.2 | bsz 336 | num_updates 1701 | best_loss 1.073\n",
            "2024-01-01 21:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 1701 updates\n",
            "2024-01-01 21:24:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 81 @ 1701 updates, score 1.076) (writing took 0.31474636500024644 seconds)\n",
            "2024-01-01 21:24:12 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
            "2024-01-01 21:24:12 | INFO | train | epoch 081 | loss 1.091 | nll_loss 0.25 | ppl 1.19 | wps 701.2 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1701 | lr 0.00076674 | gnorm 0.385 | clip 4.8 | train_wall 129 | wall 6387\n",
            "2024-01-01 21:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 082:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:24:12 | INFO | fairseq.trainer | begin training epoch 82\n",
            "2024-01-01 21:24:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 082:  95% 20/21 [01:59<00:06,  6.36s/it]2024-01-01 21:26:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 082 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.67s/it]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  40% 2/5 [00:04<00:07,  2.35s/it]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.26s/it]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.26s/it]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:26:30 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 1.07 | nll_loss 0.15 | ppl 1.11 | wps 2174.3 | wpb 4135.2 | bsz 336 | num_updates 1722 | best_loss 1.07\n",
            "2024-01-01 21:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 1722 updates\n",
            "2024-01-01 21:26:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:26:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:26:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 82 @ 1722 updates, score 1.07) (writing took 0.5109723419991496 seconds)\n",
            "2024-01-01 21:26:31 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
            "2024-01-01 21:26:31 | INFO | train | epoch 082 | loss 1.107 | nll_loss 0.27 | ppl 1.21 | wps 703.3 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1722 | lr 0.00076205 | gnorm 0.48 | clip 9.5 | train_wall 127 | wall 6525\n",
            "2024-01-01 21:26:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 083:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:26:31 | INFO | fairseq.trainer | begin training epoch 83\n",
            "2024-01-01 21:26:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 083:  95% 20/21 [01:56<00:06,  6.32s/it]2024-01-01 21:28:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 083 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.53s/it]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.73s/it]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.93s/it]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:28:42 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 1.071 | nll_loss 0.141 | ppl 1.1 | wps 2638.1 | wpb 4135.2 | bsz 336 | num_updates 1743 | best_loss 1.07\n",
            "2024-01-01 21:28:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 1743 updates\n",
            "2024-01-01 21:28:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 83 @ 1743 updates, score 1.071) (writing took 0.3009361379990878 seconds)\n",
            "2024-01-01 21:28:42 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
            "2024-01-01 21:28:42 | INFO | train | epoch 083 | loss 1.086 | nll_loss 0.247 | ppl 1.19 | wps 738 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1743 | lr 0.000757445 | gnorm 0.271 | clip 4.8 | train_wall 123 | wall 6657\n",
            "2024-01-01 21:28:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 084:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:28:42 | INFO | fairseq.trainer | begin training epoch 84\n",
            "2024-01-01 21:28:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 084:  95% 20/21 [01:57<00:05,  5.52s/it]2024-01-01 21:30:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 084 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.55s/it]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.74s/it]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  80% 4/5 [00:07<00:02,  2.04s/it]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:30:55 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 1.071 | nll_loss 0.141 | ppl 1.1 | wps 2405.5 | wpb 4135.2 | bsz 336 | num_updates 1764 | best_loss 1.07\n",
            "2024-01-01 21:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 1764 updates\n",
            "2024-01-01 21:30:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:30:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:30:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 84 @ 1764 updates, score 1.071) (writing took 0.46678555899961793 seconds)\n",
            "2024-01-01 21:30:55 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
            "2024-01-01 21:30:55 | INFO | train | epoch 084 | loss 1.079 | nll_loss 0.237 | ppl 1.18 | wps 730.6 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1764 | lr 0.000752923 | gnorm 0.332 | clip 4.8 | train_wall 124 | wall 6790\n",
            "2024-01-01 21:30:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 085:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:30:55 | INFO | fairseq.trainer | begin training epoch 85\n",
            "2024-01-01 21:30:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 085:  95% 20/21 [02:03<00:05,  5.31s/it]2024-01-01 21:33:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 085 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  20% 1/5 [00:02<00:08,  2.04s/it]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  40% 2/5 [00:04<00:07,  2.44s/it]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.16s/it]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.17s/it]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.63s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:33:09 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 1.067 | nll_loss 0.139 | ppl 1.1 | wps 2322.5 | wpb 4135.2 | bsz 336 | num_updates 1785 | best_loss 1.067\n",
            "2024-01-01 21:33:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 1785 updates\n",
            "2024-01-01 21:33:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:33:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:33:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 85 @ 1785 updates, score 1.067) (writing took 0.5431534959989222 seconds)\n",
            "2024-01-01 21:33:10 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
            "2024-01-01 21:33:10 | INFO | train | epoch 085 | loss 1.076 | nll_loss 0.233 | ppl 1.18 | wps 724.1 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1785 | lr 0.000748481 | gnorm 0.986 | clip 4.8 | train_wall 124 | wall 6924\n",
            "2024-01-01 21:33:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 086:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:33:10 | INFO | fairseq.trainer | begin training epoch 86\n",
            "2024-01-01 21:33:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 086:  95% 20/21 [02:06<00:05,  5.70s/it, loss=1.086, nll_loss=0.245, ppl=1.19, wps=713.2, ups=0.15, wpb=4643, bsz=376.1, num_updates=1800, lr=0.000745356, gnorm=0.471, clip=5, train_wall=602, wall=7023]2024-01-01 21:35:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 086 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.55s/it]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.74s/it]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  60% 3/5 [00:06<00:04,  2.20s/it]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.36s/it]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.76s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:35:30 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 1.066 | nll_loss 0.143 | ppl 1.1 | wps 2170.7 | wpb 4135.2 | bsz 336 | num_updates 1806 | best_loss 1.066\n",
            "2024-01-01 21:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 1806 updates\n",
            "2024-01-01 21:35:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:35:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 86 @ 1806 updates, score 1.066) (writing took 0.5370368830008374 seconds)\n",
            "2024-01-01 21:35:30 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
            "2024-01-01 21:35:30 | INFO | train | epoch 086 | loss 1.075 | nll_loss 0.234 | ppl 1.18 | wps 691.4 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1806 | lr 0.000744117 | gnorm 0.255 | clip 0 | train_wall 130 | wall 7064\n",
            "2024-01-01 21:35:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 087:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:35:30 | INFO | fairseq.trainer | begin training epoch 87\n",
            "2024-01-01 21:35:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 087:  95% 20/21 [02:01<00:06,  6.83s/it]2024-01-01 21:37:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 087 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  20% 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  40% 2/5 [00:03<00:05,  1.71s/it]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  60% 3/5 [00:05<00:03,  1.77s/it]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  80% 4/5 [00:07<00:01,  1.93s/it]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset: 100% 5/5 [00:08<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:37:45 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 1.065 | nll_loss 0.144 | ppl 1.11 | wps 2638.2 | wpb 4135.2 | bsz 336 | num_updates 1827 | best_loss 1.065\n",
            "2024-01-01 21:37:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 1827 updates\n",
            "2024-01-01 21:37:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:37:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_best.pt (epoch 87 @ 1827 updates, score 1.065) (writing took 0.7146922419997281 seconds)\n",
            "2024-01-01 21:37:45 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
            "2024-01-01 21:37:45 | INFO | train | epoch 087 | loss 1.077 | nll_loss 0.235 | ppl 1.18 | wps 718.8 | ups 0.16 | wpb 4628.8 | bsz 373.4 | num_updates 1827 | lr 0.000739828 | gnorm 0.354 | clip 4.8 | train_wall 126 | wall 7200\n",
            "2024-01-01 21:37:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 088:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:37:46 | INFO | fairseq.trainer | begin training epoch 88\n",
            "2024-01-01 21:37:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 088:  95% 20/21 [02:01<00:05,  5.89s/it]2024-01-01 21:39:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 088 | valid on 'valid' subset:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  20% 1/5 [00:02<00:09,  2.37s/it]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  40% 2/5 [00:04<00:06,  2.11s/it]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  60% 3/5 [00:06<00:03,  1.98s/it]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  80% 4/5 [00:08<00:02,  2.07s/it]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset: 100% 5/5 [00:09<00:00,  1.57s/it]\u001b[A\n",
            "                                                                      \u001b[A2024-01-01 21:40:03 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 1.069 | nll_loss 0.147 | ppl 1.11 | wps 2590.7 | wpb 4135.2 | bsz 336 | num_updates 1848 | best_loss 1.065\n",
            "2024-01-01 21:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 1848 updates\n",
            "2024-01-01 21:40:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/dan-models/checkpoint_last.pt\n",
            "2024-01-01 21:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/dan-models/checkpoint_last.pt (epoch 88 @ 1848 updates, score 1.069) (writing took 0.2930010370000673 seconds)\n",
            "2024-01-01 21:40:03 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
            "2024-01-01 21:40:03 | INFO | train | epoch 088 | loss 1.078 | nll_loss 0.238 | ppl 1.18 | wps 704.1 | ups 0.15 | wpb 4628.8 | bsz 373.4 | num_updates 1848 | lr 0.000735612 | gnorm 0.281 | clip 4.8 | train_wall 128 | wall 7338\n",
            "2024-01-01 21:40:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21\n",
            "epoch 089:   0% 0/21 [00:00<?, ?it/s]2024-01-01 21:40:04 | INFO | fairseq.trainer | begin training epoch 89\n",
            "2024-01-01 21:40:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 824, in train_step\n",
            "    loss, sample_size_i, logging_output = self.task.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 519, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!bash ./train.sh dan --patience 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70_-Z3hGSXe"
      },
      "source": [
        "## Generate predictions on test data\n",
        "\n",
        "Generate predictions on test data - read in all the inputs from tst.esp.input and generate outputs to the file tst.esp.output (this is slow and takes about a minute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxe16BNGRzh",
        "outputId": "ed45af64-8bd5-4e3b-f461-56e4b12560e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-01 21:40:20.491792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 21:40:20.491900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 21:40:20.499200: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 21:40:20.519614: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 21:40:22.910911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 21:40:30 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/dan-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 21:40:30 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 56 types\n",
            "2024-01-01 21:40:30 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-01 21:40:30 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/dan-models/checkpoint_best.pt\n",
            "2024-01-01 21:40:31 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-01 21:40:31 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-01 21:44:09 | INFO | fairseq_cli.interactive | Total time: 219.074 seconds; translation time: 211.284\n"
          ]
        }
      ],
      "source": [
        "!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoints/dan-models/checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vGn473BGQUU",
        "outputId": "75823a68-2f67-4fa3-ba34-541fa0ab4009"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('< i n d b r u d > N DEF NOM PL', '< i n d b r u d d e n e s >'),\n",
              " ('< c i g a r e t s k o d > N DEF NOM PL',\n",
              "  '< c i g a r e t s k o d d e n e s >'),\n",
              " ('< r o t t e > N INDF GEN PL', '< r o t t e r s >'),\n",
              " ('< m a s s ø r > N INDF NOM SG', '< m a s s ø r >'),\n",
              " ('< n u l > N INDF NOM PL', '< n u l l e r >'),\n",
              " ('< d a t a > N INDF GEN PL', '< d a t a s >'),\n",
              " ('< k r æ s e > V PASS IND PST', '< k r æ s e d e s >'),\n",
              " ('< b r æ k j e r n > N DEF NOM SG', '< b r æ k j e r n e t s >'),\n",
              " ('< w c - r u l l e > N DEF NOM SG', '< w c - r u l l e n s >'),\n",
              " ('< t e p o s e > N DEF NOM SG', '< t e p o s e n s >'),\n",
              " ('< p a t i e n t > N DEF NOM SG', '< p a t i e n t e n s >'),\n",
              " ('< s t ø r r e l s e > N DEF NOM PL', '< s t ø r r e l s e r n e s >'),\n",
              " ('< b y g n i n g > N INDF GEN SG', '< b y g n i n g s >'),\n",
              " ('< k l a v i a t u r > N INDF NOM PL', '< k l a v i a t u r e r >'),\n",
              " ('< d ø g n > N DEF NOM PL', '< d ø g n e n e s >'),\n",
              " ('< p a r k e r i n g s g a r a g e > N INDF GEN SG',\n",
              "  '< p a r k e r i n g s g a r a g e s >'),\n",
              " ('< j o r d s k æ l v > N INDF GEN SG', '< j o r d s k æ l v s >'),\n",
              " ('< g a m e r > N INDF GEN SG', '< g a m e r s >'),\n",
              " ('< b i l i s t > N INDF GEN SG', '< b i l i s t s >'),\n",
              " ('< v o g n > N INDF GEN SG', '< v o g n s >'))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read in the generated outputs and inputs and display the first 20 side-by-side\n",
        "linesinput = [l.strip() for l in open(\"tst.dan.input\")]\n",
        "linesoutput = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "tuple(zip(linesinput, linesoutput))[:20] # Look at 20 first test inputs and predicted outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmRuBQm-7CgS"
      },
      "source": [
        "## Calculate test accuracy and Levensthein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t9HAmFd7CPJ",
        "outputId": "68d6282b-8939-4f03-9298-74233d063f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-02 21:04:23.114571: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 21:04:23.114656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 21:04:23.116171: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 21:04:23.128370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 21:04:24.824427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-02 21:04:29 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.dan.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/dan/', 'source_lang': 'dan.input', 'target_lang': 'dan.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-02 21:04:29 | INFO | fairseq.tasks.translation | [dan.input] dictionary: 56 types\n",
            "2024-01-02 21:04:29 | INFO | fairseq.tasks.translation | [dan.output] dictionary: 40 types\n",
            "2024-01-02 21:04:29 | INFO | fairseq_cli.interactive | loading model(s) from checkpoint_best.pt\n",
            "2024-01-02 21:04:30 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-02 21:04:30 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-01-02 21:08:25 | INFO | fairseq_cli.interactive | Total time: 235.769 seconds; translation time: 228.143\n"
          ]
        }
      ],
      "source": [
        "# Creating the predictions from the checkpoint\n",
        "!fairseq-interactive data-bin/dan/ --source-lang=dan.input --target-lang=dan.output --path=checkpoint_best.pt --input=tst.dan.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.dan.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqHpNu-S7gIJ",
        "outputId": "80dd296e-39e2-4760-82fb-c9b93a5dd123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8292682926829268\n"
          ]
        }
      ],
      "source": [
        "# Calculating the accuracy between the ground truth and the predictions\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "hits = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "  if pred == ground:\n",
        "    hits += 1\n",
        "  lines += 1\n",
        "\n",
        "print(\"Accuracy: \" + str(hits/lines))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOwSI5Mw702e",
        "outputId": "001765e6-032c-485f-a7bd-7a4692756297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "6\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "3\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "4\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "12\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "6\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "18\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "6\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "6\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "16\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "16\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "9\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "4\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "6\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "12\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "Levenshtein distance: 0.44259369422962525\n"
          ]
        }
      ],
      "source": [
        "# Calculating the Levenshtein distance between the ground truth and the predictions\n",
        "\n",
        "# Function from ChatGPT\n",
        "def levenshtein_distance(str1, str2):\n",
        "    len_str1 = len(str1) + 1\n",
        "    len_str2 = len(str2) + 1\n",
        "\n",
        "    # Create a matrix to store the distances\n",
        "    matrix = [[0 for _ in range(len_str2)] for _ in range(len_str1)]\n",
        "\n",
        "    # Initialize the matrix\n",
        "    for i in range(len_str1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len_str2):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    # Fill in the matrix\n",
        "    for i in range(1, len_str1):\n",
        "        for j in range(1, len_str2):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i - 1][j] + 1,        # Deletion\n",
        "                matrix[i][j - 1] + 1,        # Insertion\n",
        "                matrix[i - 1][j - 1] + cost  # Substitution\n",
        "            )\n",
        "\n",
        "    # The bottom-right cell contains the Levenshtein distance\n",
        "    return matrix[-1][-1]\n",
        "\n",
        "linesprediction = [l.strip() for l in open(\"tst.dan.prediction\")]\n",
        "linesground = [l.strip() for l in open(\"tst.dan.output\")]\n",
        "\n",
        "# Checking if both files have the same number of lines\n",
        "assert sum(1 for _ in enumerate(linesprediction)) == sum(1 for _ in enumerate(linesground))\n",
        "assert sum(1 for _ in enumerate(linesprediction)) != 0\n",
        "assert sum(1 for _ in enumerate(linesground)) != 0\n",
        "\n",
        "distances = 0\n",
        "lines = 0\n",
        "\n",
        "\n",
        "for pred, ground in zip(linesprediction, linesground):\n",
        "\n",
        "  distances += levenshtein_distance(pred, ground)\n",
        "  lines += 1\n",
        "\n",
        "print(\"Levenshtein distance: \" + str(distances/lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aeqp6-_YDHuY"
      },
      "source": [
        "Distance = 0: The strings are identical. No edits are needed.\n",
        "\n",
        "Distance = 1: The strings are very similar. Typically, this means either a single insertion, deletion, or substitution is required to make them identical. For example, \"cat\" and \"cot\" have a Levenshtein distance of 1 because you can transform one into the other by changing a single character.\n",
        "\n",
        "Distance > 1: As the distance increases, the dissimilarity between the strings also increases. A distance of 2 or more indicates a greater degree of dissimilarity, involving multiple edits."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
