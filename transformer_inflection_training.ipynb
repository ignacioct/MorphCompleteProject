{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dwxnw8EQCP0"
   },
   "source": [
    "<div align=right>\n",
    "LAP 3 / EMLCT Computational Morphology<br>\n",
    "Hulden<br>\n",
    "Fall 2022\n",
    "</div>\n",
    "\n",
    "<h1 align=center>Training a transformer model</h1>\n",
    "\n",
    "(You can access this notebook on Colab directly, [here](https://colab.research.google.com/drive/1GU9HUZ0lnTNcuym-tvXY19FGDQ_9QSLO?usp=sharing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlBJt-lcQGaw"
   },
   "source": [
    "# Seq2seq training with Fairseq\n",
    "\n",
    "This notebook illustrates basic training of a seq2seq model using a GPU and the Transformer model implemented in the Fairseq package. The task is to learn to inflect Spanish verbs from citation forms and grammatical information. \n",
    "\n",
    "You should download the file `fairseqexample.tar.gz` and place the files into your Google drive. You will need to mount the Google drive as the working directory into the notebook (see cells below), so that you can run Fairseq on the training/dev/test files.\n",
    "\n",
    "## The data\n",
    "\n",
    "The training / dev / test data are organized into the following files: train.esp.input, train.esp.output, dev.esp.input, dev.esp.output, tst.esp.input (there is no gold output for test as we are only training an example model and generating outputs, not evaluating.)\n",
    "\n",
    "For example, the verb \"manducar\" inflected in the past participle masculine singular (V.PTCP PST MASC SG) is \"manducado\". This is reflected in that the line in the file `train.esp.input`:\n",
    "\n",
    "`m a n d u c a r # V.PTCP PST MASC SG`\n",
    "\n",
    "corresponds to the line\n",
    "\n",
    "`m a n d u c a d o`\n",
    "\n",
    "in the file `train.esp.output`. The dev set is organized the same way.\n",
    "\n",
    "## GPU \n",
    "To train on a GPU you need to activate the GPU in the Colab notebook by going Edit > Notebook Settings and select GPU as the \"Hardware Accelerator\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6DbPVy7Mik0",
    "outputId": "59a35bc0-6daf-486a-b339-b37bd37b15e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fairseq\n",
      "  Downloading fairseq-0.12.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 5.2 MB/s \n",
      "\u001b[?25hCollecting omegaconf<2.1\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Collecting bitarray\n",
      "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 46.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.12.1+cu113)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.15.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.21.6)\n",
      "Collecting hydra-core<1.1,>=1.0.7\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 68.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.64.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.12.1+cu113)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.32)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2022.6.2)\n",
      "Collecting sacrebleu>=1.4.12\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 57.9 MB/s \n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 61.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (5.10.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq) (4.1.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq) (3.10.0)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=ba11b8641d9559ed5ade88b14849048181f077747363c4e1b1ad279087465a60\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.6.0 sacrebleu-2.3.1\n"
     ]
    }
   ],
   "source": [
    "# May need to install fairseq in Colab once for a notebook if it needs it\n",
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mfKJupOHm-3",
    "outputId": "55c47fa4-75c0-4dfd-c286-32d58c6b1447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.19.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwsQoKMlj8me",
    "outputId": "2199ab6d-e7e4-4900-a4c4-d0ad944a610f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "total 20\n",
      "drwxr-xr-x 1 root root 4096 Nov 25 09:28 .\n",
      "drwxr-xr-x 1 root root 4096 Nov 25 09:27 ..\n",
      "drwxr-xr-x 4 root root 4096 Nov 22 00:13 .config\n",
      "drwx------ 6 root root 4096 Nov 25 09:28 drive\n",
      "drwxr-xr-x 1 root root 4096 Nov 22 00:14 sample_data\n"
     ]
    }
   ],
   "source": [
    "# Here, we mount Google drive so that Colab can access files in your Google drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEvf7M2akKp6",
    "outputId": "8f0d0db2-ae9e-40c4-c314-ed16fae8d5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fairseqexample\n",
      "total 747\n",
      "drwx------ 2 root root   4096 Apr 19  2022 checkpoints\n",
      "drwx------ 2 root root   4096 Apr 19  2022 data-bin\n",
      "-rw------- 1 root root  37637 Oct 30  2020 dev.esp.input\n",
      "-rw------- 1 root root  24947 Oct 30  2020 dev.esp.output\n",
      "-rw------- 1 root root    552 Oct 30  2020 preprocess.sh\n",
      "-rw------- 1 root root 375069 Oct 30  2020 train.esp.input\n",
      "-rw------- 1 root root 248914 Oct 30  2020 train.esp.output\n",
      "-rw------- 1 root root   1723 Nov 26  2021 train.sh\n",
      "-rw------- 1 root root  37364 Oct 30  2020 tst.esp.input\n",
      "-rw------- 1 root root  24620 Apr 27  2022 tst.esp.output\n",
      "-rw------- 1 root root   3253 Apr 20  2022 tst.esp.output2\n"
     ]
    }
   ],
   "source": [
    "# You should navigate into the directory where the training/dev/test files \n",
    "# and the preprocessing and training scripts are. This may be different for you\n",
    "# depending on where you placed the files from fairseqexample.tar.gz\n",
    "%cd /content/drive/MyDrive/fairseqexample\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxN-N3JakLpr",
    "outputId": "d84a3da4-20b6-47f9-9a75-196fc22f7f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-25 09:30:06 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/esp', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='esp.input', srcdict=None, suppress_crashes=False, target_lang='esp.output', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer='space', tpu=False, trainpref='train', use_plasma_view=False, user_dir=None, validpref='dev', wandb_project=None, workers=1)\n",
      "2022-11-25 09:30:07 | INFO | fairseq_cli.preprocess | [esp.input] Dictionary: 64 types\n",
      "2022-11-25 09:30:08 | INFO | fairseq_cli.preprocess | [esp.input] train.esp.input: 10000 sents, 164823 tokens, 0.00182% replaced (by <unk>)\n",
      "2022-11-25 09:30:08 | INFO | fairseq_cli.preprocess | [esp.input] Dictionary: 64 types\n",
      "2022-11-25 09:30:08 | INFO | fairseq_cli.preprocess | [esp.input] dev.esp.input: 1000 sents, 16539 tokens, 0.0% replaced (by <unk>)\n",
      "2022-11-25 09:30:08 | INFO | fairseq_cli.preprocess | [esp.output] Dictionary: 40 types\n",
      "2022-11-25 09:30:09 | INFO | fairseq_cli.preprocess | [esp.output] train.esp.output: 10000 sents, 132847 tokens, 0.00226% replaced (by <unk>)\n",
      "2022-11-25 09:30:09 | INFO | fairseq_cli.preprocess | [esp.output] Dictionary: 40 types\n",
      "2022-11-25 09:30:09 | INFO | fairseq_cli.preprocess | [esp.output] dev.esp.output: 1000 sents, 13314 tokens, 0.0% replaced (by <unk>)\n",
      "2022-11-25 09:30:09 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/esp\n"
     ]
    }
   ],
   "source": [
    "# We have to preprocess the data so the tokens get analyzed\n",
    "!bash ./preprocess.sh esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQOXvHTHMexu",
    "outputId": "694fa12c-a9fc-4503-c9b6-49b0c1193ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/esp-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.3, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.3, azureml_logging=False, batch_size=400, batch_size_valid=400, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/esp', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=256, decoder_layerdrop=0, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=None, max_tokens_valid=None, max_update=6000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/esp-models', save_interval=1, save_interval_updates=0, scoring='bleu', seed=212, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='esp.input', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='esp.output', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=1000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/esp', 'source_lang': 'esp.input', 'target_lang': 'esp.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-25 09:31:12 | INFO | fairseq.tasks.translation | [esp.input] dictionary: 64 types\n",
      "2022-11-25 09:31:12 | INFO | fairseq.tasks.translation | [esp.output] dictionary: 40 types\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(64, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | num. shared model params: 10,555,392 (num. trained: 10,555,392)\n",
      "2022-11-25 09:31:12 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-25 09:31:12 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: data-bin/esp/valid.esp.input-esp.output.esp.input\n",
      "2022-11-25 09:31:12 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: data-bin/esp/valid.esp.input-esp.output.esp.output\n",
      "2022-11-25 09:31:12 | INFO | fairseq.tasks.translation | data-bin/esp valid esp.input-esp.output 1000 examples\n",
      "2022-11-25 09:31:14 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-11-25 09:31:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-25 09:31:14 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
      "2022-11-25 09:31:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-25 09:31:14 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-25 09:31:14 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
      "2022-11-25 09:31:14 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:31:14 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:31:14 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-25 09:31:14 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: data-bin/esp/train.esp.input-esp.output.esp.input\n",
      "2022-11-25 09:31:14 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: data-bin/esp/train.esp.input-esp.output.esp.output\n",
      "2022-11-25 09:31:14 | INFO | fairseq.tasks.translation | data-bin/esp train esp.input-esp.output 10000 examples\n",
      "2022-11-25 09:31:14 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-25 09:31:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 001:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:14 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-25 09:31:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/usr/local/lib/python3.7/dist-packages/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
      "epoch 001:  96% 24/25 [00:05<00:00,  7.22it/s]2022-11-25 09:31:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  8.57it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.162 | nll_loss 7.129 | ppl 139.98 | wps 102852 | wpb 4438 | bsz 333.3 | num_updates 25\n",
      "2022-11-25 09:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 25 updates\n",
      "2022-11-25 09:31:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 1 @ 25 updates, score 7.162) (writing took 0.830397363999964 seconds)\n",
      "2022-11-25 09:31:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-25 09:31:21 | INFO | train | epoch 001 | loss 9.52 | nll_loss 9.524 | ppl 736.11 | wps 28207 | ups 5.31 | wpb 5313.9 | bsz 400 | num_updates 25 | lr 2.5e-05 | gnorm 6.26 | clip 100 | train_wall 6 | gb_free 13.6 | wall 7\n",
      "2022-11-25 09:31:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 002:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:21 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-25 09:31:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  96% 24/25 [00:03<00:00,  6.71it/s]2022-11-25 09:31:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.888 | nll_loss 4.732 | ppl 26.58 | wps 99796.1 | wpb 4438 | bsz 333.3 | num_updates 50 | best_loss 4.888\n",
      "2022-11-25 09:31:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 50 updates\n",
      "2022-11-25 09:31:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 2 @ 50 updates, score 4.888) (writing took 0.8523443830000019 seconds)\n",
      "2022-11-25 09:31:26 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-25 09:31:26 | INFO | train | epoch 002 | loss 5.988 | nll_loss 5.909 | ppl 60.08 | wps 27618.1 | ups 5.2 | wpb 5313.9 | bsz 400 | num_updates 50 | lr 5e-05 | gnorm 2.275 | clip 100 | train_wall 4 | gb_free 13.4 | wall 12\n",
      "2022-11-25 09:31:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 003:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:26 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-25 09:31:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  96% 24/25 [00:03<00:00,  7.48it/s]2022-11-25 09:31:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.89it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.456 | nll_loss 4.234 | ppl 18.82 | wps 100117 | wpb 4438 | bsz 333.3 | num_updates 75 | best_loss 4.456\n",
      "2022-11-25 09:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 75 updates\n",
      "2022-11-25 09:31:30 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 3 @ 75 updates, score 4.456) (writing took 0.8360338499999784 seconds)\n",
      "2022-11-25 09:31:31 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-25 09:31:31 | INFO | train | epoch 003 | loss 4.748 | nll_loss 4.57 | ppl 23.75 | wps 27284.2 | ups 5.13 | wpb 5313.9 | bsz 400 | num_updates 75 | lr 7.5e-05 | gnorm 0.73 | clip 8 | train_wall 4 | gb_free 13.7 | wall 17\n",
      "2022-11-25 09:31:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 004:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:31 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-25 09:31:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  96% 24/25 [00:03<00:00,  7.14it/s]2022-11-25 09:31:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.42it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.013 | nll_loss 3.693 | ppl 12.93 | wps 97462.5 | wpb 4438 | bsz 333.3 | num_updates 100 | best_loss 4.013\n",
      "2022-11-25 09:31:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 100 updates\n",
      "2022-11-25 09:31:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 4 @ 100 updates, score 4.013) (writing took 0.8356148150000422 seconds)\n",
      "2022-11-25 09:31:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-25 09:31:36 | INFO | train | epoch 004 | loss 4.368 | nll_loss 4.133 | ppl 17.55 | wps 26875.3 | ups 5.06 | wpb 5313.9 | bsz 400 | num_updates 100 | lr 0.0001 | gnorm 0.69 | clip 0 | train_wall 4 | gb_free 13.5 | wall 22\n",
      "2022-11-25 09:31:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 005:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:36 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-25 09:31:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  96% 24/25 [00:03<00:00,  6.71it/s]2022-11-25 09:31:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:40 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.739 | nll_loss 3.321 | ppl 9.99 | wps 96796.1 | wpb 4438 | bsz 333.3 | num_updates 125 | best_loss 3.739\n",
      "2022-11-25 09:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 125 updates\n",
      "2022-11-25 09:31:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 5 @ 125 updates, score 3.739) (writing took 0.8307099589999893 seconds)\n",
      "2022-11-25 09:31:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-25 09:31:41 | INFO | train | epoch 005 | loss 3.965 | nll_loss 3.631 | ppl 12.38 | wps 27188.7 | ups 5.12 | wpb 5313.9 | bsz 400 | num_updates 125 | lr 0.000125 | gnorm 0.778 | clip 24 | train_wall 4 | gb_free 13.5 | wall 27\n",
      "2022-11-25 09:31:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 006:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:41 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-11-25 09:31:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  96% 24/25 [00:03<00:00,  7.51it/s]2022-11-25 09:31:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.39it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.512 | nll_loss 3.006 | ppl 8.03 | wps 93711.2 | wpb 4438 | bsz 333.3 | num_updates 150 | best_loss 3.512\n",
      "2022-11-25 09:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 150 updates\n",
      "2022-11-25 09:31:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 6 @ 150 updates, score 3.512) (writing took 0.9093422079999982 seconds)\n",
      "2022-11-25 09:31:46 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-11-25 09:31:46 | INFO | train | epoch 006 | loss 3.716 | nll_loss 3.329 | ppl 10.05 | wps 26634.2 | ups 5.01 | wpb 5313.9 | bsz 400 | num_updates 150 | lr 0.00015 | gnorm 1.015 | clip 52 | train_wall 4 | gb_free 13.3 | wall 32\n",
      "2022-11-25 09:31:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 007:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:46 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-11-25 09:31:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  96% 24/25 [00:03<00:00,  7.21it/s]2022-11-25 09:31:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.21it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.208 | nll_loss 2.618 | ppl 6.14 | wps 93688.7 | wpb 4438 | bsz 333.3 | num_updates 175 | best_loss 3.208\n",
      "2022-11-25 09:31:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 175 updates\n",
      "2022-11-25 09:31:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 7 @ 175 updates, score 3.208) (writing took 0.8306831880000232 seconds)\n",
      "2022-11-25 09:31:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-11-25 09:31:51 | INFO | train | epoch 007 | loss 3.401 | nll_loss 2.95 | ppl 7.73 | wps 27043.5 | ups 5.09 | wpb 5313.9 | bsz 400 | num_updates 175 | lr 0.000175 | gnorm 1.014 | clip 40 | train_wall 4 | gb_free 13.5 | wall 37\n",
      "2022-11-25 09:31:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 008:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:51 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-11-25 09:31:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  96% 24/25 [00:03<00:00,  6.85it/s]2022-11-25 09:31:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:31:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 2.841 | nll_loss 2.195 | ppl 4.58 | wps 90489.9 | wpb 4438 | bsz 333.3 | num_updates 200 | best_loss 2.841\n",
      "2022-11-25 09:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 200 updates\n",
      "2022-11-25 09:31:55 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:31:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 8 @ 200 updates, score 2.841) (writing took 0.8565851100000259 seconds)\n",
      "2022-11-25 09:31:56 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-11-25 09:31:56 | INFO | train | epoch 008 | loss 3.13 | nll_loss 2.614 | ppl 6.12 | wps 26348.6 | ups 4.96 | wpb 5313.9 | bsz 400 | num_updates 200 | lr 0.0002 | gnorm 1.033 | clip 40 | train_wall 4 | gb_free 13.5 | wall 42\n",
      "2022-11-25 09:31:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 009:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:31:56 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-11-25 09:31:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  96% 24/25 [00:03<00:00,  6.38it/s]2022-11-25 09:32:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 12.40it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 2.399 | nll_loss 1.633 | ppl 3.1 | wps 72631.7 | wpb 4438 | bsz 333.3 | num_updates 225 | best_loss 2.399\n",
      "2022-11-25 09:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 225 updates\n",
      "2022-11-25 09:32:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 9 @ 225 updates, score 2.399) (writing took 0.9783401509999976 seconds)\n",
      "2022-11-25 09:32:01 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-11-25 09:32:01 | INFO | train | epoch 009 | loss 2.831 | nll_loss 2.262 | ppl 4.8 | wps 24883.4 | ups 4.68 | wpb 5313.9 | bsz 400 | num_updates 225 | lr 0.000225 | gnorm 0.953 | clip 36 | train_wall 4 | gb_free 13.5 | wall 47\n",
      "2022-11-25 09:32:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 010:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:01 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-11-25 09:32:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  96% 24/25 [00:04<00:00,  6.62it/s]2022-11-25 09:32:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.33it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:06 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 2.013 | nll_loss 1.138 | ppl 2.2 | wps 88564.1 | wpb 4438 | bsz 333.3 | num_updates 250 | best_loss 2.013\n",
      "2022-11-25 09:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 250 updates\n",
      "2022-11-25 09:32:06 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 10 @ 250 updates, score 2.013) (writing took 1.0989473900000348 seconds)\n",
      "2022-11-25 09:32:07 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-11-25 09:32:07 | INFO | train | epoch 010 | loss 2.515 | nll_loss 1.881 | ppl 3.68 | wps 23600.5 | ups 4.44 | wpb 5313.9 | bsz 400 | num_updates 250 | lr 0.00025 | gnorm 1.035 | clip 56 | train_wall 4 | gb_free 13.7 | wall 53\n",
      "2022-11-25 09:32:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 011:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:07 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2022-11-25 09:32:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011:  96% 24/25 [00:03<00:00,  6.79it/s]2022-11-25 09:32:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:11 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.781 | nll_loss 0.825 | ppl 1.77 | wps 91078.6 | wpb 4438 | bsz 333.3 | num_updates 275 | best_loss 1.781\n",
      "2022-11-25 09:32:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 275 updates\n",
      "2022-11-25 09:32:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 11 @ 275 updates, score 1.781) (writing took 0.843342410000048 seconds)\n",
      "2022-11-25 09:32:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2022-11-25 09:32:12 | INFO | train | epoch 011 | loss 2.232 | nll_loss 1.537 | ppl 2.9 | wps 26301.6 | ups 4.95 | wpb 5313.9 | bsz 400 | num_updates 275 | lr 0.000275 | gnorm 0.982 | clip 32 | train_wall 4 | gb_free 13.3 | wall 58\n",
      "2022-11-25 09:32:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 012:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:12 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2022-11-25 09:32:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012:  96% 24/25 [00:03<00:00,  6.25it/s]2022-11-25 09:32:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.14it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 1.717 | nll_loss 0.709 | ppl 1.63 | wps 91095.8 | wpb 4438 | bsz 333.3 | num_updates 300 | best_loss 1.717\n",
      "2022-11-25 09:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 300 updates\n",
      "2022-11-25 09:32:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 12 @ 300 updates, score 1.717) (writing took 0.8439907320000088 seconds)\n",
      "2022-11-25 09:32:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2022-11-25 09:32:17 | INFO | train | epoch 012 | loss 2.023 | nll_loss 1.29 | ppl 2.45 | wps 25997.5 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 300 | lr 0.0003 | gnorm 0.968 | clip 36 | train_wall 4 | gb_free 13.6 | wall 63\n",
      "2022-11-25 09:32:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 013:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:17 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2022-11-25 09:32:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013:  96% 24/25 [00:03<00:00,  6.98it/s]2022-11-25 09:32:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:21 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 1.745 | nll_loss 0.728 | ppl 1.66 | wps 89756.1 | wpb 4438 | bsz 333.3 | num_updates 325 | best_loss 1.717\n",
      "2022-11-25 09:32:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 325 updates\n",
      "2022-11-25 09:32:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:32:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 13 @ 325 updates, score 1.745) (writing took 0.3748130230000015 seconds)\n",
      "2022-11-25 09:32:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2022-11-25 09:32:21 | INFO | train | epoch 013 | loss 1.908 | nll_loss 1.152 | ppl 2.22 | wps 29214.5 | ups 5.5 | wpb 5313.9 | bsz 400 | num_updates 325 | lr 0.000325 | gnorm 1.068 | clip 48 | train_wall 4 | gb_free 13.7 | wall 67\n",
      "2022-11-25 09:32:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 014:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:21 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2022-11-25 09:32:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014:  96% 24/25 [00:03<00:00,  7.04it/s]2022-11-25 09:32:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.38it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 1.555 | nll_loss 0.508 | ppl 1.42 | wps 95892.7 | wpb 4438 | bsz 333.3 | num_updates 350 | best_loss 1.555\n",
      "2022-11-25 09:32:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 350 updates\n",
      "2022-11-25 09:32:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 14 @ 350 updates, score 1.555) (writing took 1.0869405669999992 seconds)\n",
      "2022-11-25 09:32:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2022-11-25 09:32:27 | INFO | train | epoch 014 | loss 1.831 | nll_loss 1.06 | ppl 2.08 | wps 25214.5 | ups 4.75 | wpb 5313.9 | bsz 400 | num_updates 350 | lr 0.00035 | gnorm 1.162 | clip 52 | train_wall 4 | gb_free 13.7 | wall 73\n",
      "2022-11-25 09:32:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 015:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:27 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2022-11-25 09:32:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015:  96% 24/25 [00:03<00:00,  5.98it/s]2022-11-25 09:32:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:31 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 1.483 | nll_loss 0.425 | ppl 1.34 | wps 85970.5 | wpb 4438 | bsz 333.3 | num_updates 375 | best_loss 1.483\n",
      "2022-11-25 09:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 375 updates\n",
      "2022-11-25 09:32:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 15 @ 375 updates, score 1.483) (writing took 0.8096016860000077 seconds)\n",
      "2022-11-25 09:32:32 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2022-11-25 09:32:32 | INFO | train | epoch 015 | loss 1.71 | nll_loss 0.918 | ppl 1.89 | wps 26290.2 | ups 4.95 | wpb 5313.9 | bsz 400 | num_updates 375 | lr 0.000375 | gnorm 0.797 | clip 16 | train_wall 4 | gb_free 13.7 | wall 78\n",
      "2022-11-25 09:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 016:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:32 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2022-11-25 09:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016:  96% 24/25 [00:03<00:00,  6.35it/s]2022-11-25 09:32:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.29it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:36 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 1.448 | nll_loss 0.382 | ppl 1.3 | wps 86430.4 | wpb 4438 | bsz 333.3 | num_updates 400 | best_loss 1.448\n",
      "2022-11-25 09:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 400 updates\n",
      "2022-11-25 09:32:36 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 16 @ 400 updates, score 1.448) (writing took 0.8189954850000163 seconds)\n",
      "2022-11-25 09:32:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2022-11-25 09:32:37 | INFO | train | epoch 016 | loss 1.625 | nll_loss 0.82 | ppl 1.77 | wps 26155.6 | ups 4.92 | wpb 5313.9 | bsz 400 | num_updates 400 | lr 0.0004 | gnorm 0.745 | clip 8 | train_wall 4 | gb_free 13.8 | wall 83\n",
      "2022-11-25 09:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 017:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:37 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2022-11-25 09:32:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017:  96% 24/25 [00:03<00:00,  6.78it/s]2022-11-25 09:32:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.15it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:41 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 1.406 | nll_loss 0.349 | ppl 1.27 | wps 78614.2 | wpb 4438 | bsz 333.3 | num_updates 425 | best_loss 1.406\n",
      "2022-11-25 09:32:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 425 updates\n",
      "2022-11-25 09:32:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 17 @ 425 updates, score 1.406) (writing took 0.8213922170000387 seconds)\n",
      "2022-11-25 09:32:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2022-11-25 09:32:42 | INFO | train | epoch 017 | loss 1.552 | nll_loss 0.734 | ppl 1.66 | wps 26192.2 | ups 4.93 | wpb 5313.9 | bsz 400 | num_updates 425 | lr 0.000425 | gnorm 0.635 | clip 8 | train_wall 4 | gb_free 13.3 | wall 88\n",
      "2022-11-25 09:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 018:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:42 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2022-11-25 09:32:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018:  96% 24/25 [00:03<00:00,  5.62it/s]2022-11-25 09:32:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.94it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 1.382 | nll_loss 0.33 | ppl 1.26 | wps 84427.7 | wpb 4438 | bsz 333.3 | num_updates 450 | best_loss 1.382\n",
      "2022-11-25 09:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 450 updates\n",
      "2022-11-25 09:32:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 18 @ 450 updates, score 1.382) (writing took 0.8211850449999929 seconds)\n",
      "2022-11-25 09:32:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2022-11-25 09:32:47 | INFO | train | epoch 018 | loss 1.507 | nll_loss 0.684 | ppl 1.61 | wps 25816.1 | ups 4.86 | wpb 5313.9 | bsz 400 | num_updates 450 | lr 0.00045 | gnorm 0.716 | clip 4 | train_wall 4 | gb_free 13.7 | wall 93\n",
      "2022-11-25 09:32:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 019:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:47 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2022-11-25 09:32:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019:  96% 24/25 [00:03<00:00,  6.39it/s]2022-11-25 09:32:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.40it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:51 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 1.331 | nll_loss 0.289 | ppl 1.22 | wps 80242.9 | wpb 4438 | bsz 333.3 | num_updates 475 | best_loss 1.331\n",
      "2022-11-25 09:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 475 updates\n",
      "2022-11-25 09:32:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 19 @ 475 updates, score 1.331) (writing took 0.8383028490000015 seconds)\n",
      "2022-11-25 09:32:52 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2022-11-25 09:32:52 | INFO | train | epoch 019 | loss 1.511 | nll_loss 0.691 | ppl 1.61 | wps 25457.5 | ups 4.79 | wpb 5313.9 | bsz 400 | num_updates 475 | lr 0.000475 | gnorm 0.822 | clip 20 | train_wall 4 | gb_free 13.3 | wall 98\n",
      "2022-11-25 09:32:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 020:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:52 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2022-11-25 09:32:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020:  96% 24/25 [00:03<00:00,  6.88it/s]2022-11-25 09:32:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:32:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 1.298 | nll_loss 0.254 | ppl 1.19 | wps 73681.6 | wpb 4438 | bsz 333.3 | num_updates 500 | best_loss 1.298\n",
      "2022-11-25 09:32:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 500 updates\n",
      "2022-11-25 09:32:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:32:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 20 @ 500 updates, score 1.298) (writing took 0.8454416060000085 seconds)\n",
      "2022-11-25 09:32:57 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2022-11-25 09:32:57 | INFO | train | epoch 020 | loss 1.45 | nll_loss 0.625 | ppl 1.54 | wps 25426.3 | ups 4.78 | wpb 5313.9 | bsz 400 | num_updates 500 | lr 0.0005 | gnorm 0.655 | clip 8 | train_wall 4 | gb_free 13.3 | wall 103\n",
      "2022-11-25 09:32:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 021:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:32:57 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2022-11-25 09:32:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 021:  96% 24/25 [00:03<00:00,  5.49it/s]2022-11-25 09:33:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 021 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.96it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:02 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 1.274 | nll_loss 0.234 | ppl 1.18 | wps 77798.7 | wpb 4438 | bsz 333.3 | num_updates 525 | best_loss 1.274\n",
      "2022-11-25 09:33:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 525 updates\n",
      "2022-11-25 09:33:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 21 @ 525 updates, score 1.274) (writing took 0.8316771519999975 seconds)\n",
      "2022-11-25 09:33:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2022-11-25 09:33:03 | INFO | train | epoch 021 | loss 1.39 | nll_loss 0.555 | ppl 1.47 | wps 25492.7 | ups 4.8 | wpb 5313.9 | bsz 400 | num_updates 525 | lr 0.000525 | gnorm 0.51 | clip 0 | train_wall 4 | gb_free 13.6 | wall 109\n",
      "2022-11-25 09:33:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 022:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:03 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2022-11-25 09:33:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 022:  96% 24/25 [00:03<00:00,  6.75it/s]2022-11-25 09:33:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 022 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.04it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:07 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 1.266 | nll_loss 0.226 | ppl 1.17 | wps 86940.1 | wpb 4438 | bsz 333.3 | num_updates 550 | best_loss 1.266\n",
      "2022-11-25 09:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 550 updates\n",
      "2022-11-25 09:33:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 22 @ 550 updates, score 1.266) (writing took 0.8222052620000113 seconds)\n",
      "2022-11-25 09:33:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2022-11-25 09:33:08 | INFO | train | epoch 022 | loss 1.36 | nll_loss 0.524 | ppl 1.44 | wps 25678.4 | ups 4.83 | wpb 5313.9 | bsz 400 | num_updates 550 | lr 0.00055 | gnorm 0.547 | clip 4 | train_wall 4 | gb_free 13.5 | wall 114\n",
      "2022-11-25 09:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 023:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:08 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2022-11-25 09:33:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 023:  96% 24/25 [00:03<00:00,  6.13it/s]2022-11-25 09:33:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 023 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.79it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:12 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 1.244 | nll_loss 0.218 | ppl 1.16 | wps 86631.5 | wpb 4438 | bsz 333.3 | num_updates 575 | best_loss 1.244\n",
      "2022-11-25 09:33:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 575 updates\n",
      "2022-11-25 09:33:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 23 @ 575 updates, score 1.244) (writing took 0.8144044979999876 seconds)\n",
      "2022-11-25 09:33:13 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2022-11-25 09:33:13 | INFO | train | epoch 023 | loss 1.354 | nll_loss 0.521 | ppl 1.43 | wps 26152.5 | ups 4.92 | wpb 5313.9 | bsz 400 | num_updates 575 | lr 0.000575 | gnorm 0.561 | clip 4 | train_wall 4 | gb_free 12.9 | wall 119\n",
      "2022-11-25 09:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 024:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:13 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2022-11-25 09:33:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 024:  96% 24/25 [00:03<00:00,  6.57it/s]2022-11-25 09:33:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 024 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.54it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 1.218 | nll_loss 0.192 | ppl 1.14 | wps 79364.7 | wpb 4438 | bsz 333.3 | num_updates 600 | best_loss 1.218\n",
      "2022-11-25 09:33:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 600 updates\n",
      "2022-11-25 09:33:17 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 24 @ 600 updates, score 1.218) (writing took 0.8408895569999686 seconds)\n",
      "2022-11-25 09:33:18 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2022-11-25 09:33:18 | INFO | train | epoch 024 | loss 1.314 | nll_loss 0.474 | ppl 1.39 | wps 26117.3 | ups 4.91 | wpb 5313.9 | bsz 400 | num_updates 600 | lr 0.0006 | gnorm 0.483 | clip 0 | train_wall 4 | gb_free 13.5 | wall 124\n",
      "2022-11-25 09:33:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 025:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:18 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2022-11-25 09:33:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 025:  96% 24/25 [00:03<00:00,  7.30it/s]2022-11-25 09:33:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 025 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 1.207 | nll_loss 0.197 | ppl 1.15 | wps 93605.9 | wpb 4438 | bsz 333.3 | num_updates 625 | best_loss 1.207\n",
      "2022-11-25 09:33:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 625 updates\n",
      "2022-11-25 09:33:22 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 25 @ 625 updates, score 1.207) (writing took 0.8410199819999775 seconds)\n",
      "2022-11-25 09:33:23 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2022-11-25 09:33:23 | INFO | train | epoch 025 | loss 1.285 | nll_loss 0.444 | ppl 1.36 | wps 26349 | ups 4.96 | wpb 5313.9 | bsz 400 | num_updates 625 | lr 0.000625 | gnorm 0.452 | clip 0 | train_wall 4 | gb_free 13.5 | wall 129\n",
      "2022-11-25 09:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 026:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:23 | INFO | fairseq.trainer | begin training epoch 26\n",
      "2022-11-25 09:33:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 026:  96% 24/25 [00:03<00:00,  7.18it/s]2022-11-25 09:33:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 026 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:27 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 1.2 | nll_loss 0.189 | ppl 1.14 | wps 85485.2 | wpb 4438 | bsz 333.3 | num_updates 650 | best_loss 1.2\n",
      "2022-11-25 09:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 650 updates\n",
      "2022-11-25 09:33:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 26 @ 650 updates, score 1.2) (writing took 0.8710596429999669 seconds)\n",
      "2022-11-25 09:33:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2022-11-25 09:33:28 | INFO | train | epoch 026 | loss 1.271 | nll_loss 0.43 | ppl 1.35 | wps 26164.5 | ups 4.92 | wpb 5313.9 | bsz 400 | num_updates 650 | lr 0.00065 | gnorm 0.53 | clip 4 | train_wall 4 | gb_free 13.7 | wall 134\n",
      "2022-11-25 09:33:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 027:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:28 | INFO | fairseq.trainer | begin training epoch 27\n",
      "2022-11-25 09:33:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 027:  96% 24/25 [00:03<00:00,  6.28it/s]2022-11-25 09:33:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 027 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.50it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 1.184 | nll_loss 0.188 | ppl 1.14 | wps 88327.2 | wpb 4438 | bsz 333.3 | num_updates 675 | best_loss 1.184\n",
      "2022-11-25 09:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 675 updates\n",
      "2022-11-25 09:33:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 27 @ 675 updates, score 1.184) (writing took 0.8395910180000214 seconds)\n",
      "2022-11-25 09:33:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2022-11-25 09:33:33 | INFO | train | epoch 027 | loss 1.252 | nll_loss 0.41 | ppl 1.33 | wps 26397.7 | ups 4.97 | wpb 5313.9 | bsz 400 | num_updates 675 | lr 0.000675 | gnorm 0.47 | clip 4 | train_wall 4 | gb_free 13.5 | wall 139\n",
      "2022-11-25 09:33:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 028:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:33 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2022-11-25 09:33:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 028:  96% 24/25 [00:03<00:00,  6.53it/s]2022-11-25 09:33:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 028 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.55it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:37 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 1.165 | nll_loss 0.169 | ppl 1.12 | wps 83700.5 | wpb 4438 | bsz 333.3 | num_updates 700 | best_loss 1.165\n",
      "2022-11-25 09:33:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 700 updates\n",
      "2022-11-25 09:33:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 28 @ 700 updates, score 1.165) (writing took 0.873680203000049 seconds)\n",
      "2022-11-25 09:33:38 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2022-11-25 09:33:38 | INFO | train | epoch 028 | loss 1.237 | nll_loss 0.394 | ppl 1.31 | wps 26271.3 | ups 4.94 | wpb 5313.9 | bsz 400 | num_updates 700 | lr 0.0007 | gnorm 0.423 | clip 0 | train_wall 4 | gb_free 13.6 | wall 144\n",
      "2022-11-25 09:33:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 029:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:38 | INFO | fairseq.trainer | begin training epoch 29\n",
      "2022-11-25 09:33:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 029:  96% 24/25 [00:03<00:00,  6.37it/s]2022-11-25 09:33:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 029 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:42 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 1.172 | nll_loss 0.177 | ppl 1.13 | wps 86804.1 | wpb 4438 | bsz 333.3 | num_updates 725 | best_loss 1.165\n",
      "2022-11-25 09:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 725 updates\n",
      "2022-11-25 09:33:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:33:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:33:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 29 @ 725 updates, score 1.172) (writing took 0.3936876700000198 seconds)\n",
      "2022-11-25 09:33:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2022-11-25 09:33:43 | INFO | train | epoch 029 | loss 1.22 | nll_loss 0.377 | ppl 1.3 | wps 28842.7 | ups 5.43 | wpb 5313.9 | bsz 400 | num_updates 725 | lr 0.000725 | gnorm 0.424 | clip 0 | train_wall 4 | gb_free 13.5 | wall 149\n",
      "2022-11-25 09:33:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 030:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:43 | INFO | fairseq.trainer | begin training epoch 30\n",
      "2022-11-25 09:33:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 030:  96% 24/25 [00:03<00:00,  6.58it/s]2022-11-25 09:33:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 030 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  6.22it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:47 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 1.136 | nll_loss 0.155 | ppl 1.11 | wps 87682.9 | wpb 4438 | bsz 333.3 | num_updates 750 | best_loss 1.136\n",
      "2022-11-25 09:33:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 750 updates\n",
      "2022-11-25 09:33:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 30 @ 750 updates, score 1.136) (writing took 0.8523639860000003 seconds)\n",
      "2022-11-25 09:33:48 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2022-11-25 09:33:48 | INFO | train | epoch 030 | loss 1.199 | nll_loss 0.354 | ppl 1.28 | wps 25159.2 | ups 4.73 | wpb 5313.9 | bsz 400 | num_updates 750 | lr 0.00075 | gnorm 0.376 | clip 0 | train_wall 4 | gb_free 13.6 | wall 154\n",
      "2022-11-25 09:33:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 031:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:48 | INFO | fairseq.trainer | begin training epoch 31\n",
      "2022-11-25 09:33:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 031:  96% 24/25 [00:03<00:00,  7.05it/s]2022-11-25 09:33:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 031 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.55it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 1.146 | nll_loss 0.161 | ppl 1.12 | wps 87299 | wpb 4438 | bsz 333.3 | num_updates 775 | best_loss 1.136\n",
      "2022-11-25 09:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 775 updates\n",
      "2022-11-25 09:33:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:33:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:33:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 31 @ 775 updates, score 1.146) (writing took 0.4456690640000147 seconds)\n",
      "2022-11-25 09:33:53 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2022-11-25 09:33:53 | INFO | train | epoch 031 | loss 1.193 | nll_loss 0.349 | ppl 1.27 | wps 28164.8 | ups 5.3 | wpb 5313.9 | bsz 400 | num_updates 775 | lr 0.000775 | gnorm 0.385 | clip 4 | train_wall 4 | gb_free 13.3 | wall 159\n",
      "2022-11-25 09:33:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 032:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:53 | INFO | fairseq.trainer | begin training epoch 32\n",
      "2022-11-25 09:33:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 032:  96% 24/25 [00:04<00:00,  5.83it/s]2022-11-25 09:33:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 032 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.86it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:33:57 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 1.128 | nll_loss 0.146 | ppl 1.11 | wps 88795.1 | wpb 4438 | bsz 333.3 | num_updates 800 | best_loss 1.128\n",
      "2022-11-25 09:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 800 updates\n",
      "2022-11-25 09:33:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:33:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 32 @ 800 updates, score 1.128) (writing took 0.939711165999995 seconds)\n",
      "2022-11-25 09:33:58 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2022-11-25 09:33:58 | INFO | train | epoch 032 | loss 1.174 | nll_loss 0.33 | ppl 1.26 | wps 24706.6 | ups 4.65 | wpb 5313.9 | bsz 400 | num_updates 800 | lr 0.0008 | gnorm 0.343 | clip 0 | train_wall 4 | gb_free 13.6 | wall 164\n",
      "2022-11-25 09:33:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 033:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:33:58 | INFO | fairseq.trainer | begin training epoch 33\n",
      "2022-11-25 09:33:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 033:  96% 24/25 [00:03<00:00,  7.01it/s]2022-11-25 09:34:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 033 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.86it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:02 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 1.117 | nll_loss 0.142 | ppl 1.1 | wps 85056.9 | wpb 4438 | bsz 333.3 | num_updates 825 | best_loss 1.117\n",
      "2022-11-25 09:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 825 updates\n",
      "2022-11-25 09:34:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 33 @ 825 updates, score 1.117) (writing took 0.8386952999999835 seconds)\n",
      "2022-11-25 09:34:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2022-11-25 09:34:03 | INFO | train | epoch 033 | loss 1.15 | nll_loss 0.301 | ppl 1.23 | wps 25990.7 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 825 | lr 0.000825 | gnorm 0.298 | clip 0 | train_wall 4 | gb_free 13.4 | wall 169\n",
      "2022-11-25 09:34:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 034:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:03 | INFO | fairseq.trainer | begin training epoch 34\n",
      "2022-11-25 09:34:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 034:  96% 24/25 [00:03<00:00,  5.86it/s]2022-11-25 09:34:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 034 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.60it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:08 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 1.126 | nll_loss 0.153 | ppl 1.11 | wps 81666.6 | wpb 4438 | bsz 333.3 | num_updates 850 | best_loss 1.117\n",
      "2022-11-25 09:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 850 updates\n",
      "2022-11-25 09:34:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 34 @ 850 updates, score 1.126) (writing took 0.36239469700001337 seconds)\n",
      "2022-11-25 09:34:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2022-11-25 09:34:08 | INFO | train | epoch 034 | loss 1.166 | nll_loss 0.324 | ppl 1.25 | wps 28412.6 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 850 | lr 0.00085 | gnorm 0.39 | clip 0 | train_wall 4 | gb_free 13.6 | wall 174\n",
      "2022-11-25 09:34:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 035:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:08 | INFO | fairseq.trainer | begin training epoch 35\n",
      "2022-11-25 09:34:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 035:  96% 24/25 [00:03<00:00,  5.92it/s]2022-11-25 09:34:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 035 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:12 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 1.109 | nll_loss 0.149 | ppl 1.11 | wps 92733.3 | wpb 4438 | bsz 333.3 | num_updates 875 | best_loss 1.109\n",
      "2022-11-25 09:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 875 updates\n",
      "2022-11-25 09:34:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 35 @ 875 updates, score 1.109) (writing took 0.883968450999987 seconds)\n",
      "2022-11-25 09:34:13 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2022-11-25 09:34:13 | INFO | train | epoch 035 | loss 1.169 | nll_loss 0.325 | ppl 1.25 | wps 25278.2 | ups 4.76 | wpb 5313.9 | bsz 400 | num_updates 875 | lr 0.000875 | gnorm 0.381 | clip 0 | train_wall 4 | gb_free 13.6 | wall 179\n",
      "2022-11-25 09:34:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 036:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:13 | INFO | fairseq.trainer | begin training epoch 36\n",
      "2022-11-25 09:34:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 036:  96% 24/25 [00:03<00:00,  6.38it/s]2022-11-25 09:34:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 036 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.85it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:18 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 1.12 | nll_loss 0.148 | ppl 1.11 | wps 85207.7 | wpb 4438 | bsz 333.3 | num_updates 900 | best_loss 1.109\n",
      "2022-11-25 09:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 900 updates\n",
      "2022-11-25 09:34:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 36 @ 900 updates, score 1.12) (writing took 0.3899607580000293 seconds)\n",
      "2022-11-25 09:34:18 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2022-11-25 09:34:18 | INFO | train | epoch 036 | loss 1.132 | nll_loss 0.286 | ppl 1.22 | wps 28424.7 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 900 | lr 0.0009 | gnorm 0.291 | clip 4 | train_wall 4 | gb_free 13.2 | wall 184\n",
      "2022-11-25 09:34:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 037:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:18 | INFO | fairseq.trainer | begin training epoch 37\n",
      "2022-11-25 09:34:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 037:  96% 24/25 [00:04<00:00,  5.75it/s]2022-11-25 09:34:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 037 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.08it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:22 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 1.102 | nll_loss 0.147 | ppl 1.11 | wps 84251.4 | wpb 4438 | bsz 333.3 | num_updates 925 | best_loss 1.102\n",
      "2022-11-25 09:34:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 925 updates\n",
      "2022-11-25 09:34:22 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 37 @ 925 updates, score 1.102) (writing took 0.8800890350000259 seconds)\n",
      "2022-11-25 09:34:23 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2022-11-25 09:34:23 | INFO | train | epoch 037 | loss 1.142 | nll_loss 0.298 | ppl 1.23 | wps 24965.5 | ups 4.7 | wpb 5313.9 | bsz 400 | num_updates 925 | lr 0.000925 | gnorm 0.339 | clip 0 | train_wall 4 | gb_free 13.5 | wall 189\n",
      "2022-11-25 09:34:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 038:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:23 | INFO | fairseq.trainer | begin training epoch 38\n",
      "2022-11-25 09:34:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 038:  96% 24/25 [00:03<00:00,  6.18it/s]2022-11-25 09:34:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 038 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.08it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 1.106 | nll_loss 0.142 | ppl 1.1 | wps 85102.2 | wpb 4438 | bsz 333.3 | num_updates 950 | best_loss 1.102\n",
      "2022-11-25 09:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 950 updates\n",
      "2022-11-25 09:34:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 38 @ 950 updates, score 1.106) (writing took 0.3669981410000105 seconds)\n",
      "2022-11-25 09:34:28 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2022-11-25 09:34:28 | INFO | train | epoch 038 | loss 1.125 | nll_loss 0.278 | ppl 1.21 | wps 28331.5 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 950 | lr 0.00095 | gnorm 0.311 | clip 0 | train_wall 4 | gb_free 13.6 | wall 194\n",
      "2022-11-25 09:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 039:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:28 | INFO | fairseq.trainer | begin training epoch 39\n",
      "2022-11-25 09:34:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 039:  96% 24/25 [00:04<00:00,  5.55it/s]2022-11-25 09:34:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 039 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:32 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 1.087 | nll_loss 0.124 | ppl 1.09 | wps 85970.9 | wpb 4438 | bsz 333.3 | num_updates 975 | best_loss 1.087\n",
      "2022-11-25 09:34:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 975 updates\n",
      "2022-11-25 09:34:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 39 @ 975 updates, score 1.087) (writing took 0.8823304280000457 seconds)\n",
      "2022-11-25 09:34:33 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2022-11-25 09:34:33 | INFO | train | epoch 039 | loss 1.111 | nll_loss 0.265 | ppl 1.2 | wps 24986.3 | ups 4.7 | wpb 5313.9 | bsz 400 | num_updates 975 | lr 0.000975 | gnorm 0.275 | clip 0 | train_wall 4 | gb_free 13.6 | wall 199\n",
      "2022-11-25 09:34:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 040:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:33 | INFO | fairseq.trainer | begin training epoch 40\n",
      "2022-11-25 09:34:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 040:  96% 24/25 [00:03<00:00,  6.82it/s]2022-11-25 09:34:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 040 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.86it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:38 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 1.092 | nll_loss 0.133 | ppl 1.1 | wps 85671 | wpb 4438 | bsz 333.3 | num_updates 1000 | best_loss 1.087\n",
      "2022-11-25 09:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1000 updates\n",
      "2022-11-25 09:34:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:34:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 40 @ 1000 updates, score 1.092) (writing took 0.3707408589999659 seconds)\n",
      "2022-11-25 09:34:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2022-11-25 09:34:38 | INFO | train | epoch 040 | loss 1.141 | nll_loss 0.299 | ppl 1.23 | wps 28325.8 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 1000 | lr 0.001 | gnorm 0.353 | clip 0 | train_wall 4 | gb_free 13.6 | wall 204\n",
      "2022-11-25 09:34:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 041:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:38 | INFO | fairseq.trainer | begin training epoch 41\n",
      "2022-11-25 09:34:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 041:  96% 24/25 [00:03<00:00,  6.01it/s]2022-11-25 09:34:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 041 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.18it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:42 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 1.087 | nll_loss 0.135 | ppl 1.1 | wps 85178.8 | wpb 4438 | bsz 333.3 | num_updates 1025 | best_loss 1.087\n",
      "2022-11-25 09:34:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1025 updates\n",
      "2022-11-25 09:34:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 41 @ 1025 updates, score 1.087) (writing took 0.919694278999998 seconds)\n",
      "2022-11-25 09:34:43 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2022-11-25 09:34:43 | INFO | train | epoch 041 | loss 1.131 | nll_loss 0.289 | ppl 1.22 | wps 25173.7 | ups 4.74 | wpb 5313.9 | bsz 400 | num_updates 1025 | lr 0.00098773 | gnorm 0.337 | clip 0 | train_wall 4 | gb_free 13.6 | wall 209\n",
      "2022-11-25 09:34:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 042:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:43 | INFO | fairseq.trainer | begin training epoch 42\n",
      "2022-11-25 09:34:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 042:  96% 24/25 [00:03<00:00,  6.47it/s]2022-11-25 09:34:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 042 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.42it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:47 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 1.075 | nll_loss 0.125 | ppl 1.09 | wps 86209.3 | wpb 4438 | bsz 333.3 | num_updates 1050 | best_loss 1.075\n",
      "2022-11-25 09:34:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1050 updates\n",
      "2022-11-25 09:34:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 42 @ 1050 updates, score 1.075) (writing took 0.861072269000033 seconds)\n",
      "2022-11-25 09:34:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2022-11-25 09:34:48 | INFO | train | epoch 042 | loss 1.105 | nll_loss 0.258 | ppl 1.2 | wps 25849.7 | ups 4.86 | wpb 5313.9 | bsz 400 | num_updates 1050 | lr 0.0009759 | gnorm 0.277 | clip 0 | train_wall 4 | gb_free 13.7 | wall 214\n",
      "2022-11-25 09:34:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 043:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:48 | INFO | fairseq.trainer | begin training epoch 43\n",
      "2022-11-25 09:34:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 043:  96% 24/25 [00:03<00:00,  7.18it/s]2022-11-25 09:34:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 043 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.91it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:53 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 1.071 | nll_loss 0.126 | ppl 1.09 | wps 87924.9 | wpb 4438 | bsz 333.3 | num_updates 1075 | best_loss 1.071\n",
      "2022-11-25 09:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1075 updates\n",
      "2022-11-25 09:34:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 43 @ 1075 updates, score 1.071) (writing took 0.8654705239999885 seconds)\n",
      "2022-11-25 09:34:53 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2022-11-25 09:34:53 | INFO | train | epoch 043 | loss 1.089 | nll_loss 0.244 | ppl 1.18 | wps 25770 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 1075 | lr 0.000964486 | gnorm 0.241 | clip 0 | train_wall 4 | gb_free 13.5 | wall 219\n",
      "2022-11-25 09:34:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 044:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:54 | INFO | fairseq.trainer | begin training epoch 44\n",
      "2022-11-25 09:34:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 044:  96% 24/25 [00:03<00:00,  6.87it/s]2022-11-25 09:34:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 044 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:34:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 1.071 | nll_loss 0.128 | ppl 1.09 | wps 82005.9 | wpb 4438 | bsz 333.3 | num_updates 1100 | best_loss 1.071\n",
      "2022-11-25 09:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1100 updates\n",
      "2022-11-25 09:34:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:34:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 44 @ 1100 updates, score 1.071) (writing took 0.8331410750000146 seconds)\n",
      "2022-11-25 09:34:59 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
      "2022-11-25 09:34:59 | INFO | train | epoch 044 | loss 1.079 | nll_loss 0.231 | ppl 1.17 | wps 25919.3 | ups 4.88 | wpb 5313.9 | bsz 400 | num_updates 1100 | lr 0.000953463 | gnorm 0.203 | clip 0 | train_wall 4 | gb_free 13.7 | wall 225\n",
      "2022-11-25 09:34:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 045:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:34:59 | INFO | fairseq.trainer | begin training epoch 45\n",
      "2022-11-25 09:34:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 045:  96% 24/25 [00:03<00:00,  5.90it/s]2022-11-25 09:35:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 045 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.85it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:03 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 1.085 | nll_loss 0.155 | ppl 1.11 | wps 86171.4 | wpb 4438 | bsz 333.3 | num_updates 1125 | best_loss 1.071\n",
      "2022-11-25 09:35:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1125 updates\n",
      "2022-11-25 09:35:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 45 @ 1125 updates, score 1.085) (writing took 0.37996503399995163 seconds)\n",
      "2022-11-25 09:35:03 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2022-11-25 09:35:03 | INFO | train | epoch 045 | loss 1.116 | nll_loss 0.279 | ppl 1.21 | wps 28240.9 | ups 5.31 | wpb 5313.9 | bsz 400 | num_updates 1125 | lr 0.000942809 | gnorm 0.346 | clip 8 | train_wall 4 | gb_free 13.7 | wall 229\n",
      "2022-11-25 09:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 046:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:03 | INFO | fairseq.trainer | begin training epoch 46\n",
      "2022-11-25 09:35:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 046:  96% 24/25 [00:03<00:00,  6.68it/s]2022-11-25 09:35:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 046 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.46it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:08 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 1.081 | nll_loss 0.136 | ppl 1.1 | wps 86916.5 | wpb 4438 | bsz 333.3 | num_updates 1150 | best_loss 1.071\n",
      "2022-11-25 09:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1150 updates\n",
      "2022-11-25 09:35:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 46 @ 1150 updates, score 1.081) (writing took 0.42973504200000434 seconds)\n",
      "2022-11-25 09:35:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2022-11-25 09:35:08 | INFO | train | epoch 046 | loss 1.106 | nll_loss 0.263 | ppl 1.2 | wps 27742.3 | ups 5.22 | wpb 5313.9 | bsz 400 | num_updates 1150 | lr 0.000932505 | gnorm 0.28 | clip 0 | train_wall 4 | gb_free 13.6 | wall 234\n",
      "2022-11-25 09:35:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 047:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:08 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2022-11-25 09:35:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 047:  96% 24/25 [00:03<00:00,  6.30it/s]2022-11-25 09:35:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 047 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.85it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:12 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 1.066 | nll_loss 0.121 | ppl 1.09 | wps 85587.5 | wpb 4438 | bsz 333.3 | num_updates 1175 | best_loss 1.066\n",
      "2022-11-25 09:35:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1175 updates\n",
      "2022-11-25 09:35:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 47 @ 1175 updates, score 1.066) (writing took 0.9036097129999803 seconds)\n",
      "2022-11-25 09:35:13 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2022-11-25 09:35:13 | INFO | train | epoch 047 | loss 1.084 | nll_loss 0.238 | ppl 1.18 | wps 25789 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 1175 | lr 0.000922531 | gnorm 0.238 | clip 0 | train_wall 4 | gb_free 13.5 | wall 239\n",
      "2022-11-25 09:35:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 048:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:13 | INFO | fairseq.trainer | begin training epoch 48\n",
      "2022-11-25 09:35:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 048:  96% 24/25 [00:03<00:00,  6.43it/s]2022-11-25 09:35:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 048 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.29it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 1.065 | nll_loss 0.121 | ppl 1.09 | wps 80413.5 | wpb 4438 | bsz 333.3 | num_updates 1200 | best_loss 1.065\n",
      "2022-11-25 09:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1200 updates\n",
      "2022-11-25 09:35:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 48 @ 1200 updates, score 1.065) (writing took 0.8789287380000133 seconds)\n",
      "2022-11-25 09:35:18 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2022-11-25 09:35:18 | INFO | train | epoch 048 | loss 1.07 | nll_loss 0.225 | ppl 1.17 | wps 25785.1 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 1200 | lr 0.000912871 | gnorm 0.203 | clip 0 | train_wall 4 | gb_free 13.6 | wall 244\n",
      "2022-11-25 09:35:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 049:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:18 | INFO | fairseq.trainer | begin training epoch 49\n",
      "2022-11-25 09:35:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 049:  96% 24/25 [00:03<00:00,  6.61it/s]2022-11-25 09:35:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 049 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.68it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:23 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 1.064 | nll_loss 0.123 | ppl 1.09 | wps 85847.1 | wpb 4438 | bsz 333.3 | num_updates 1225 | best_loss 1.064\n",
      "2022-11-25 09:35:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1225 updates\n",
      "2022-11-25 09:35:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 49 @ 1225 updates, score 1.064) (writing took 0.8516554059999635 seconds)\n",
      "2022-11-25 09:35:24 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2022-11-25 09:35:24 | INFO | train | epoch 049 | loss 1.073 | nll_loss 0.226 | ppl 1.17 | wps 25851.7 | ups 4.86 | wpb 5313.9 | bsz 400 | num_updates 1225 | lr 0.000903508 | gnorm 0.208 | clip 0 | train_wall 4 | gb_free 13.7 | wall 250\n",
      "2022-11-25 09:35:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 050:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:24 | INFO | fairseq.trainer | begin training epoch 50\n",
      "2022-11-25 09:35:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 050:  96% 24/25 [00:03<00:00,  5.51it/s]2022-11-25 09:35:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 050 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  9.47it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:28 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 1.069 | nll_loss 0.116 | ppl 1.08 | wps 81903.6 | wpb 4438 | bsz 333.3 | num_updates 1250 | best_loss 1.064\n",
      "2022-11-25 09:35:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1250 updates\n",
      "2022-11-25 09:35:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 50 @ 1250 updates, score 1.069) (writing took 0.4847976749999816 seconds)\n",
      "2022-11-25 09:35:29 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2022-11-25 09:35:29 | INFO | train | epoch 050 | loss 1.075 | nll_loss 0.231 | ppl 1.17 | wps 26801.2 | ups 5.04 | wpb 5313.9 | bsz 400 | num_updates 1250 | lr 0.000894427 | gnorm 0.211 | clip 0 | train_wall 4 | gb_free 13.3 | wall 254\n",
      "2022-11-25 09:35:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 051:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:29 | INFO | fairseq.trainer | begin training epoch 51\n",
      "2022-11-25 09:35:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 051:  96% 24/25 [00:04<00:00,  6.77it/s]2022-11-25 09:35:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 051 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.55it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:33 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 1.078 | nll_loss 0.139 | ppl 1.1 | wps 88619.6 | wpb 4438 | bsz 333.3 | num_updates 1275 | best_loss 1.064\n",
      "2022-11-25 09:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1275 updates\n",
      "2022-11-25 09:35:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 51 @ 1275 updates, score 1.078) (writing took 0.4291491810000707 seconds)\n",
      "2022-11-25 09:35:34 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
      "2022-11-25 09:35:34 | INFO | train | epoch 051 | loss 1.067 | nll_loss 0.22 | ppl 1.16 | wps 25076 | ups 4.72 | wpb 5313.9 | bsz 400 | num_updates 1275 | lr 0.000885615 | gnorm 0.207 | clip 0 | train_wall 4 | gb_free 13.8 | wall 260\n",
      "2022-11-25 09:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 052:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:34 | INFO | fairseq.trainer | begin training epoch 52\n",
      "2022-11-25 09:35:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 052:  96% 24/25 [00:04<00:00,  6.68it/s]2022-11-25 09:35:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 052 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:38 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 1.06 | nll_loss 0.117 | ppl 1.08 | wps 87457.8 | wpb 4438 | bsz 333.3 | num_updates 1300 | best_loss 1.06\n",
      "2022-11-25 09:35:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1300 updates\n",
      "2022-11-25 09:35:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 52 @ 1300 updates, score 1.06) (writing took 0.8804162960000212 seconds)\n",
      "2022-11-25 09:35:39 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
      "2022-11-25 09:35:39 | INFO | train | epoch 052 | loss 1.066 | nll_loss 0.222 | ppl 1.17 | wps 24935.1 | ups 4.69 | wpb 5313.9 | bsz 400 | num_updates 1300 | lr 0.000877058 | gnorm 0.217 | clip 0 | train_wall 4 | gb_free 13.6 | wall 265\n",
      "2022-11-25 09:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 053:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:39 | INFO | fairseq.trainer | begin training epoch 53\n",
      "2022-11-25 09:35:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 053:  96% 24/25 [00:03<00:00,  6.80it/s]2022-11-25 09:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 053 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.73it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:43 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 1.06 | nll_loss 0.121 | ppl 1.09 | wps 97175.1 | wpb 4438 | bsz 333.3 | num_updates 1325 | best_loss 1.06\n",
      "2022-11-25 09:35:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1325 updates\n",
      "2022-11-25 09:35:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 53 @ 1325 updates, score 1.06) (writing took 0.9691189449999911 seconds)\n",
      "2022-11-25 09:35:44 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
      "2022-11-25 09:35:44 | INFO | train | epoch 053 | loss 1.059 | nll_loss 0.213 | ppl 1.16 | wps 25258.2 | ups 4.75 | wpb 5313.9 | bsz 400 | num_updates 1325 | lr 0.000868744 | gnorm 0.2 | clip 0 | train_wall 4 | gb_free 13.4 | wall 270\n",
      "2022-11-25 09:35:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 054:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:44 | INFO | fairseq.trainer | begin training epoch 54\n",
      "2022-11-25 09:35:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 054:  96% 24/25 [00:03<00:00,  5.94it/s]2022-11-25 09:35:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 054 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.10it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:49 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 1.06 | nll_loss 0.12 | ppl 1.09 | wps 86087.2 | wpb 4438 | bsz 333.3 | num_updates 1350 | best_loss 1.06\n",
      "2022-11-25 09:35:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1350 updates\n",
      "2022-11-25 09:35:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 54 @ 1350 updates, score 1.06) (writing took 0.8467721550000533 seconds)\n",
      "2022-11-25 09:35:50 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
      "2022-11-25 09:35:50 | INFO | train | epoch 054 | loss 1.061 | nll_loss 0.216 | ppl 1.16 | wps 25635.4 | ups 4.82 | wpb 5313.9 | bsz 400 | num_updates 1350 | lr 0.000860663 | gnorm 0.234 | clip 0 | train_wall 4 | gb_free 13.5 | wall 276\n",
      "2022-11-25 09:35:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 055:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:50 | INFO | fairseq.trainer | begin training epoch 55\n",
      "2022-11-25 09:35:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 055:  96% 24/25 [00:03<00:00,  6.07it/s]2022-11-25 09:35:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 055 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.00it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:54 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 1.056 | nll_loss 0.112 | ppl 1.08 | wps 85994.2 | wpb 4438 | bsz 333.3 | num_updates 1375 | best_loss 1.056\n",
      "2022-11-25 09:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1375 updates\n",
      "2022-11-25 09:35:54 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 55 @ 1375 updates, score 1.056) (writing took 0.8723238679999668 seconds)\n",
      "2022-11-25 09:35:55 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
      "2022-11-25 09:35:55 | INFO | train | epoch 055 | loss 1.061 | nll_loss 0.217 | ppl 1.16 | wps 25755.4 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 1375 | lr 0.000852803 | gnorm 0.207 | clip 0 | train_wall 4 | gb_free 13.7 | wall 281\n",
      "2022-11-25 09:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 056:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:35:55 | INFO | fairseq.trainer | begin training epoch 56\n",
      "2022-11-25 09:35:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 056:  96% 24/25 [00:03<00:00,  7.09it/s]2022-11-25 09:35:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 056 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.49it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:35:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 1.054 | nll_loss 0.12 | ppl 1.09 | wps 87217.7 | wpb 4438 | bsz 333.3 | num_updates 1400 | best_loss 1.054\n",
      "2022-11-25 09:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1400 updates\n",
      "2022-11-25 09:35:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 56 @ 1400 updates, score 1.054) (writing took 0.8374463730000343 seconds)\n",
      "2022-11-25 09:36:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
      "2022-11-25 09:36:00 | INFO | train | epoch 056 | loss 1.054 | nll_loss 0.209 | ppl 1.16 | wps 25819.8 | ups 4.86 | wpb 5313.9 | bsz 400 | num_updates 1400 | lr 0.000845154 | gnorm 0.18 | clip 0 | train_wall 4 | gb_free 13.6 | wall 286\n",
      "2022-11-25 09:36:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 057:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:00 | INFO | fairseq.trainer | begin training epoch 57\n",
      "2022-11-25 09:36:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 057:  96% 24/25 [00:03<00:00,  6.86it/s]2022-11-25 09:36:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 057 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.95it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:04 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 1.054 | nll_loss 0.12 | ppl 1.09 | wps 103560 | wpb 4438 | bsz 333.3 | num_updates 1425 | best_loss 1.054\n",
      "2022-11-25 09:36:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1425 updates\n",
      "2022-11-25 09:36:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 57 @ 1425 updates, score 1.054) (writing took 0.9054502320000211 seconds)\n",
      "2022-11-25 09:36:05 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
      "2022-11-25 09:36:05 | INFO | train | epoch 057 | loss 1.055 | nll_loss 0.21 | ppl 1.16 | wps 25394.9 | ups 4.78 | wpb 5313.9 | bsz 400 | num_updates 1425 | lr 0.000837708 | gnorm 0.195 | clip 0 | train_wall 4 | gb_free 13.5 | wall 291\n",
      "2022-11-25 09:36:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 058:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:05 | INFO | fairseq.trainer | begin training epoch 58\n",
      "2022-11-25 09:36:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 058:  96% 24/25 [00:03<00:00,  6.67it/s]2022-11-25 09:36:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 058 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.36it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:09 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 1.054 | nll_loss 0.118 | ppl 1.08 | wps 86110.2 | wpb 4438 | bsz 333.3 | num_updates 1450 | best_loss 1.054\n",
      "2022-11-25 09:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1450 updates\n",
      "2022-11-25 09:36:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 58 @ 1450 updates, score 1.054) (writing took 0.8434398009999313 seconds)\n",
      "2022-11-25 09:36:10 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
      "2022-11-25 09:36:10 | INFO | train | epoch 058 | loss 1.053 | nll_loss 0.208 | ppl 1.15 | wps 25602.6 | ups 4.82 | wpb 5313.9 | bsz 400 | num_updates 1450 | lr 0.000830455 | gnorm 0.227 | clip 0 | train_wall 4 | gb_free 13.7 | wall 296\n",
      "2022-11-25 09:36:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 059:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:10 | INFO | fairseq.trainer | begin training epoch 59\n",
      "2022-11-25 09:36:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 059:  96% 24/25 [00:03<00:00,  6.21it/s]2022-11-25 09:36:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 059 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:15 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 1.054 | nll_loss 0.115 | ppl 1.08 | wps 88887.4 | wpb 4438 | bsz 333.3 | num_updates 1475 | best_loss 1.054\n",
      "2022-11-25 09:36:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1475 updates\n",
      "2022-11-25 09:36:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 59 @ 1475 updates, score 1.054) (writing took 0.8571740229999705 seconds)\n",
      "2022-11-25 09:36:16 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
      "2022-11-25 09:36:16 | INFO | train | epoch 059 | loss 1.054 | nll_loss 0.21 | ppl 1.16 | wps 25441 | ups 4.79 | wpb 5313.9 | bsz 400 | num_updates 1475 | lr 0.000823387 | gnorm 0.197 | clip 0 | train_wall 4 | gb_free 13.7 | wall 301\n",
      "2022-11-25 09:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 060:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:16 | INFO | fairseq.trainer | begin training epoch 60\n",
      "2022-11-25 09:36:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 060:  96% 24/25 [00:03<00:00,  6.93it/s]2022-11-25 09:36:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 060 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:20 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 1.058 | nll_loss 0.124 | ppl 1.09 | wps 85562.7 | wpb 4438 | bsz 333.3 | num_updates 1500 | best_loss 1.054\n",
      "2022-11-25 09:36:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1500 updates\n",
      "2022-11-25 09:36:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 60 @ 1500 updates, score 1.058) (writing took 0.3871387719999575 seconds)\n",
      "2022-11-25 09:36:20 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
      "2022-11-25 09:36:20 | INFO | train | epoch 060 | loss 1.062 | nll_loss 0.22 | ppl 1.17 | wps 28367.1 | ups 5.34 | wpb 5313.9 | bsz 400 | num_updates 1500 | lr 0.000816497 | gnorm 0.225 | clip 0 | train_wall 4 | gb_free 13.5 | wall 306\n",
      "2022-11-25 09:36:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 061:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:20 | INFO | fairseq.trainer | begin training epoch 61\n",
      "2022-11-25 09:36:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 061:  96% 24/25 [00:04<00:00,  5.75it/s]2022-11-25 09:36:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 061 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.02it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:25 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 1.06 | nll_loss 0.112 | ppl 1.08 | wps 84734.2 | wpb 4438 | bsz 333.3 | num_updates 1525 | best_loss 1.054\n",
      "2022-11-25 09:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1525 updates\n",
      "2022-11-25 09:36:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 61 @ 1525 updates, score 1.06) (writing took 0.4262411700000257 seconds)\n",
      "2022-11-25 09:36:25 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
      "2022-11-25 09:36:25 | INFO | train | epoch 061 | loss 1.069 | nll_loss 0.226 | ppl 1.17 | wps 27111.1 | ups 5.1 | wpb 5313.9 | bsz 400 | num_updates 1525 | lr 0.000809776 | gnorm 0.238 | clip 0 | train_wall 4 | gb_free 13.6 | wall 311\n",
      "2022-11-25 09:36:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 062:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:25 | INFO | fairseq.trainer | begin training epoch 62\n",
      "2022-11-25 09:36:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 062:  96% 24/25 [00:03<00:00,  6.62it/s]2022-11-25 09:36:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 062 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.00it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:29 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 1.05 | nll_loss 0.112 | ppl 1.08 | wps 85180.6 | wpb 4438 | bsz 333.3 | num_updates 1550 | best_loss 1.05\n",
      "2022-11-25 09:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1550 updates\n",
      "2022-11-25 09:36:29 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 62 @ 1550 updates, score 1.05) (writing took 0.8830309840000155 seconds)\n",
      "2022-11-25 09:36:30 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
      "2022-11-25 09:36:30 | INFO | train | epoch 062 | loss 1.054 | nll_loss 0.209 | ppl 1.16 | wps 25615.6 | ups 4.82 | wpb 5313.9 | bsz 400 | num_updates 1550 | lr 0.000803219 | gnorm 0.191 | clip 0 | train_wall 4 | gb_free 13.5 | wall 316\n",
      "2022-11-25 09:36:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 063:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:30 | INFO | fairseq.trainer | begin training epoch 63\n",
      "2022-11-25 09:36:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 063:  96% 24/25 [00:03<00:00,  6.66it/s]2022-11-25 09:36:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 063 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.62it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:35 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 1.052 | nll_loss 0.114 | ppl 1.08 | wps 82529.7 | wpb 4438 | bsz 333.3 | num_updates 1575 | best_loss 1.05\n",
      "2022-11-25 09:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1575 updates\n",
      "2022-11-25 09:36:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 63 @ 1575 updates, score 1.052) (writing took 0.3701172400000132 seconds)\n",
      "2022-11-25 09:36:35 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
      "2022-11-25 09:36:35 | INFO | train | epoch 063 | loss 1.045 | nll_loss 0.199 | ppl 1.15 | wps 28490.7 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 1575 | lr 0.000796819 | gnorm 0.164 | clip 0 | train_wall 4 | gb_free 13.3 | wall 321\n",
      "2022-11-25 09:36:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 064:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:35 | INFO | fairseq.trainer | begin training epoch 64\n",
      "2022-11-25 09:36:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 064:  96% 24/25 [00:04<00:00,  6.30it/s]2022-11-25 09:36:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 064 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:39 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 1.049 | nll_loss 0.113 | ppl 1.08 | wps 81476.9 | wpb 4438 | bsz 333.3 | num_updates 1600 | best_loss 1.049\n",
      "2022-11-25 09:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1600 updates\n",
      "2022-11-25 09:36:39 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 64 @ 1600 updates, score 1.049) (writing took 0.8828081760000259 seconds)\n",
      "2022-11-25 09:36:40 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
      "2022-11-25 09:36:40 | INFO | train | epoch 064 | loss 1.045 | nll_loss 0.2 | ppl 1.15 | wps 25020 | ups 4.71 | wpb 5313.9 | bsz 400 | num_updates 1600 | lr 0.000790569 | gnorm 0.17 | clip 0 | train_wall 4 | gb_free 13.8 | wall 326\n",
      "2022-11-25 09:36:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 065:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:40 | INFO | fairseq.trainer | begin training epoch 65\n",
      "2022-11-25 09:36:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 065:  96% 24/25 [00:03<00:00,  6.76it/s]2022-11-25 09:36:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 065 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:45 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 1.051 | nll_loss 0.111 | ppl 1.08 | wps 86551.2 | wpb 4438 | bsz 333.3 | num_updates 1625 | best_loss 1.049\n",
      "2022-11-25 09:36:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 1625 updates\n",
      "2022-11-25 09:36:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 65 @ 1625 updates, score 1.051) (writing took 0.37088958799995453 seconds)\n",
      "2022-11-25 09:36:45 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
      "2022-11-25 09:36:45 | INFO | train | epoch 065 | loss 1.042 | nll_loss 0.197 | ppl 1.15 | wps 28141.7 | ups 5.3 | wpb 5313.9 | bsz 400 | num_updates 1625 | lr 0.000784465 | gnorm 0.162 | clip 0 | train_wall 4 | gb_free 13.4 | wall 331\n",
      "2022-11-25 09:36:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 066:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:45 | INFO | fairseq.trainer | begin training epoch 66\n",
      "2022-11-25 09:36:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 066:  96% 24/25 [00:03<00:00,  6.71it/s]2022-11-25 09:36:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 066 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:49 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 1.051 | nll_loss 0.113 | ppl 1.08 | wps 87140.2 | wpb 4438 | bsz 333.3 | num_updates 1650 | best_loss 1.049\n",
      "2022-11-25 09:36:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 1650 updates\n",
      "2022-11-25 09:36:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 66 @ 1650 updates, score 1.051) (writing took 0.419075442999997 seconds)\n",
      "2022-11-25 09:36:50 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
      "2022-11-25 09:36:50 | INFO | train | epoch 066 | loss 1.042 | nll_loss 0.197 | ppl 1.15 | wps 28049.3 | ups 5.28 | wpb 5313.9 | bsz 400 | num_updates 1650 | lr 0.000778499 | gnorm 0.164 | clip 0 | train_wall 4 | gb_free 13.7 | wall 336\n",
      "2022-11-25 09:36:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 067:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:50 | INFO | fairseq.trainer | begin training epoch 67\n",
      "2022-11-25 09:36:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 067:  96% 24/25 [00:03<00:00,  6.62it/s]2022-11-25 09:36:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 067 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:54 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 1.054 | nll_loss 0.11 | ppl 1.08 | wps 86069.7 | wpb 4438 | bsz 333.3 | num_updates 1675 | best_loss 1.049\n",
      "2022-11-25 09:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1675 updates\n",
      "2022-11-25 09:36:54 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 67 @ 1675 updates, score 1.054) (writing took 0.4106445209999947 seconds)\n",
      "2022-11-25 09:36:54 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
      "2022-11-25 09:36:54 | INFO | train | epoch 067 | loss 1.057 | nll_loss 0.216 | ppl 1.16 | wps 28598.1 | ups 5.38 | wpb 5313.9 | bsz 400 | num_updates 1675 | lr 0.000772667 | gnorm 0.268 | clip 0 | train_wall 4 | gb_free 13.3 | wall 340\n",
      "2022-11-25 09:36:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 068:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:36:54 | INFO | fairseq.trainer | begin training epoch 68\n",
      "2022-11-25 09:36:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 068:  96% 24/25 [00:03<00:00,  6.93it/s]2022-11-25 09:36:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 068 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:36:59 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 1.046 | nll_loss 0.11 | ppl 1.08 | wps 85703.6 | wpb 4438 | bsz 333.3 | num_updates 1700 | best_loss 1.046\n",
      "2022-11-25 09:36:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1700 updates\n",
      "2022-11-25 09:36:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:36:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 68 @ 1700 updates, score 1.046) (writing took 0.8301693670000532 seconds)\n",
      "2022-11-25 09:36:59 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
      "2022-11-25 09:36:59 | INFO | train | epoch 068 | loss 1.045 | nll_loss 0.2 | ppl 1.15 | wps 26264 | ups 4.94 | wpb 5313.9 | bsz 400 | num_updates 1700 | lr 0.000766965 | gnorm 0.175 | clip 0 | train_wall 4 | gb_free 13.8 | wall 345\n",
      "2022-11-25 09:37:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 069:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:00 | INFO | fairseq.trainer | begin training epoch 69\n",
      "2022-11-25 09:37:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 069:  96% 24/25 [00:03<00:00,  6.01it/s]2022-11-25 09:37:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 069 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.47it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:04 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 1.05 | nll_loss 0.108 | ppl 1.08 | wps 88738.5 | wpb 4438 | bsz 333.3 | num_updates 1725 | best_loss 1.046\n",
      "2022-11-25 09:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1725 updates\n",
      "2022-11-25 09:37:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 69 @ 1725 updates, score 1.05) (writing took 0.3741974109999546 seconds)\n",
      "2022-11-25 09:37:04 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
      "2022-11-25 09:37:04 | INFO | train | epoch 069 | loss 1.042 | nll_loss 0.197 | ppl 1.15 | wps 28102.2 | ups 5.29 | wpb 5313.9 | bsz 400 | num_updates 1725 | lr 0.000761387 | gnorm 0.163 | clip 0 | train_wall 4 | gb_free 13.2 | wall 350\n",
      "2022-11-25 09:37:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 070:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:04 | INFO | fairseq.trainer | begin training epoch 70\n",
      "2022-11-25 09:37:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 070:  96% 24/25 [00:04<00:00,  6.31it/s]2022-11-25 09:37:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 070 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.78it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:09 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 1.046 | nll_loss 0.113 | ppl 1.08 | wps 83224.4 | wpb 4438 | bsz 333.3 | num_updates 1750 | best_loss 1.046\n",
      "2022-11-25 09:37:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1750 updates\n",
      "2022-11-25 09:37:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 70 @ 1750 updates, score 1.046) (writing took 0.8950821700000233 seconds)\n",
      "2022-11-25 09:37:10 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
      "2022-11-25 09:37:10 | INFO | train | epoch 070 | loss 1.04 | nll_loss 0.195 | ppl 1.14 | wps 24900.4 | ups 4.69 | wpb 5313.9 | bsz 400 | num_updates 1750 | lr 0.000755929 | gnorm 0.164 | clip 0 | train_wall 4 | gb_free 13.5 | wall 355\n",
      "2022-11-25 09:37:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 071:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:10 | INFO | fairseq.trainer | begin training epoch 71\n",
      "2022-11-25 09:37:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 071:  96% 24/25 [00:03<00:00,  6.75it/s]2022-11-25 09:37:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 071 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.31it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:14 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 1.046 | nll_loss 0.112 | ppl 1.08 | wps 87613.6 | wpb 4438 | bsz 333.3 | num_updates 1775 | best_loss 1.046\n",
      "2022-11-25 09:37:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1775 updates\n",
      "2022-11-25 09:37:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 71 @ 1775 updates, score 1.046) (writing took 0.8370644939999465 seconds)\n",
      "2022-11-25 09:37:15 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
      "2022-11-25 09:37:15 | INFO | train | epoch 071 | loss 1.042 | nll_loss 0.198 | ppl 1.15 | wps 26102.7 | ups 4.91 | wpb 5313.9 | bsz 400 | num_updates 1775 | lr 0.000750587 | gnorm 0.182 | clip 0 | train_wall 4 | gb_free 13.6 | wall 361\n",
      "2022-11-25 09:37:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 072:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:15 | INFO | fairseq.trainer | begin training epoch 72\n",
      "2022-11-25 09:37:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 072:  96% 24/25 [00:03<00:00,  6.85it/s]2022-11-25 09:37:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 072 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 072 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:19 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 1.049 | nll_loss 0.113 | ppl 1.08 | wps 87148.5 | wpb 4438 | bsz 333.3 | num_updates 1800 | best_loss 1.046\n",
      "2022-11-25 09:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1800 updates\n",
      "2022-11-25 09:37:19 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 72 @ 1800 updates, score 1.049) (writing took 0.41078564899999037 seconds)\n",
      "2022-11-25 09:37:19 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
      "2022-11-25 09:37:19 | INFO | train | epoch 072 | loss 1.039 | nll_loss 0.193 | ppl 1.14 | wps 28496.5 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 1800 | lr 0.000745356 | gnorm 0.168 | clip 0 | train_wall 4 | gb_free 13.5 | wall 365\n",
      "2022-11-25 09:37:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 073:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:19 | INFO | fairseq.trainer | begin training epoch 73\n",
      "2022-11-25 09:37:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 073:  96% 24/25 [00:03<00:00,  6.44it/s]2022-11-25 09:37:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 073 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 073 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.42it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:24 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 1.047 | nll_loss 0.109 | ppl 1.08 | wps 86364.5 | wpb 4438 | bsz 333.3 | num_updates 1825 | best_loss 1.046\n",
      "2022-11-25 09:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1825 updates\n",
      "2022-11-25 09:37:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 73 @ 1825 updates, score 1.047) (writing took 0.401522863000082 seconds)\n",
      "2022-11-25 09:37:24 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
      "2022-11-25 09:37:24 | INFO | train | epoch 073 | loss 1.041 | nll_loss 0.196 | ppl 1.15 | wps 28300.3 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 1825 | lr 0.000740233 | gnorm 0.182 | clip 0 | train_wall 4 | gb_free 13.5 | wall 370\n",
      "2022-11-25 09:37:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 074:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:24 | INFO | fairseq.trainer | begin training epoch 74\n",
      "2022-11-25 09:37:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 074:  96% 24/25 [00:03<00:00,  6.94it/s]2022-11-25 09:37:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 074 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 074 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.49it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:28 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 1.046 | nll_loss 0.108 | ppl 1.08 | wps 87857.4 | wpb 4438 | bsz 333.3 | num_updates 1850 | best_loss 1.046\n",
      "2022-11-25 09:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 1850 updates\n",
      "2022-11-25 09:37:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 74 @ 1850 updates, score 1.046) (writing took 0.8895109389999334 seconds)\n",
      "2022-11-25 09:37:29 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
      "2022-11-25 09:37:29 | INFO | train | epoch 074 | loss 1.037 | nll_loss 0.193 | ppl 1.14 | wps 25963.8 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 1850 | lr 0.000735215 | gnorm 0.197 | clip 0 | train_wall 4 | gb_free 13.3 | wall 375\n",
      "2022-11-25 09:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 075:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:29 | INFO | fairseq.trainer | begin training epoch 75\n",
      "2022-11-25 09:37:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 075:  96% 24/25 [00:03<00:00,  6.47it/s]2022-11-25 09:37:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 075 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 075 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.85it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:33 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 1.045 | nll_loss 0.107 | ppl 1.08 | wps 89598.3 | wpb 4438 | bsz 333.3 | num_updates 1875 | best_loss 1.045\n",
      "2022-11-25 09:37:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 1875 updates\n",
      "2022-11-25 09:37:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 75 @ 1875 updates, score 1.045) (writing took 0.8678075229998967 seconds)\n",
      "2022-11-25 09:37:34 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
      "2022-11-25 09:37:34 | INFO | train | epoch 075 | loss 1.037 | nll_loss 0.193 | ppl 1.14 | wps 25959.2 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 1875 | lr 0.000730297 | gnorm 0.162 | clip 0 | train_wall 4 | gb_free 13.3 | wall 380\n",
      "2022-11-25 09:37:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 076:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:34 | INFO | fairseq.trainer | begin training epoch 76\n",
      "2022-11-25 09:37:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 076:  96% 24/25 [00:03<00:00,  6.73it/s]2022-11-25 09:37:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 076 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:38 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 1.046 | nll_loss 0.111 | ppl 1.08 | wps 88078.5 | wpb 4438 | bsz 333.3 | num_updates 1900 | best_loss 1.045\n",
      "2022-11-25 09:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 1900 updates\n",
      "2022-11-25 09:37:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 76 @ 1900 updates, score 1.046) (writing took 0.43366481999998996 seconds)\n",
      "2022-11-25 09:37:39 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
      "2022-11-25 09:37:39 | INFO | train | epoch 076 | loss 1.033 | nll_loss 0.187 | ppl 1.14 | wps 28558.6 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 1900 | lr 0.000725476 | gnorm 0.135 | clip 0 | train_wall 4 | gb_free 13.8 | wall 385\n",
      "2022-11-25 09:37:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 077:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:39 | INFO | fairseq.trainer | begin training epoch 77\n",
      "2022-11-25 09:37:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 077:  96% 24/25 [00:03<00:00,  6.25it/s]2022-11-25 09:37:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 077 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  8.04it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:43 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 1.046 | nll_loss 0.11 | ppl 1.08 | wps 100961 | wpb 4438 | bsz 333.3 | num_updates 1925 | best_loss 1.045\n",
      "2022-11-25 09:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 1925 updates\n",
      "2022-11-25 09:37:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 77 @ 1925 updates, score 1.046) (writing took 0.41716510899993864 seconds)\n",
      "2022-11-25 09:37:44 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
      "2022-11-25 09:37:44 | INFO | train | epoch 077 | loss 1.033 | nll_loss 0.188 | ppl 1.14 | wps 27460.3 | ups 5.17 | wpb 5313.9 | bsz 400 | num_updates 1925 | lr 0.00072075 | gnorm 0.153 | clip 0 | train_wall 4 | gb_free 13.6 | wall 390\n",
      "2022-11-25 09:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 078:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:44 | INFO | fairseq.trainer | begin training epoch 78\n",
      "2022-11-25 09:37:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 078:  96% 24/25 [00:03<00:00,  6.64it/s]2022-11-25 09:37:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 078 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.52it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:48 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 1.047 | nll_loss 0.109 | ppl 1.08 | wps 88568.6 | wpb 4438 | bsz 333.3 | num_updates 1950 | best_loss 1.045\n",
      "2022-11-25 09:37:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 1950 updates\n",
      "2022-11-25 09:37:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 78 @ 1950 updates, score 1.047) (writing took 0.6377721659999906 seconds)\n",
      "2022-11-25 09:37:49 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
      "2022-11-25 09:37:49 | INFO | train | epoch 078 | loss 1.033 | nll_loss 0.188 | ppl 1.14 | wps 27275.2 | ups 5.13 | wpb 5313.9 | bsz 400 | num_updates 1950 | lr 0.000716115 | gnorm 0.147 | clip 0 | train_wall 4 | gb_free 13.5 | wall 394\n",
      "2022-11-25 09:37:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 079:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:49 | INFO | fairseq.trainer | begin training epoch 79\n",
      "2022-11-25 09:37:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 079:  96% 24/25 [00:03<00:00,  6.88it/s]2022-11-25 09:37:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 079 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.49it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:53 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 1.044 | nll_loss 0.106 | ppl 1.08 | wps 89149.3 | wpb 4438 | bsz 333.3 | num_updates 1975 | best_loss 1.044\n",
      "2022-11-25 09:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 1975 updates\n",
      "2022-11-25 09:37:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 79 @ 1975 updates, score 1.044) (writing took 0.9064867859999595 seconds)\n",
      "2022-11-25 09:37:54 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
      "2022-11-25 09:37:54 | INFO | train | epoch 079 | loss 1.034 | nll_loss 0.189 | ppl 1.14 | wps 25975 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 1975 | lr 0.000711568 | gnorm 0.164 | clip 0 | train_wall 4 | gb_free 13.6 | wall 400\n",
      "2022-11-25 09:37:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 080:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:54 | INFO | fairseq.trainer | begin training epoch 80\n",
      "2022-11-25 09:37:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 080:  96% 24/25 [00:03<00:00,  7.16it/s]2022-11-25 09:37:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 080 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:37:58 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 1.044 | nll_loss 0.104 | ppl 1.08 | wps 86313.8 | wpb 4438 | bsz 333.3 | num_updates 2000 | best_loss 1.044\n",
      "2022-11-25 09:37:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 2000 updates\n",
      "2022-11-25 09:37:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 80 @ 2000 updates, score 1.044) (writing took 0.8574291319999929 seconds)\n",
      "2022-11-25 09:37:59 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
      "2022-11-25 09:37:59 | INFO | train | epoch 080 | loss 1.032 | nll_loss 0.187 | ppl 1.14 | wps 25949.6 | ups 4.88 | wpb 5313.9 | bsz 400 | num_updates 2000 | lr 0.000707107 | gnorm 0.124 | clip 0 | train_wall 4 | gb_free 13.5 | wall 405\n",
      "2022-11-25 09:37:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 081:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:37:59 | INFO | fairseq.trainer | begin training epoch 81\n",
      "2022-11-25 09:37:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 081:  96% 24/25 [00:03<00:00,  6.01it/s]2022-11-25 09:38:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 081 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:03 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 1.045 | nll_loss 0.108 | ppl 1.08 | wps 85776.6 | wpb 4438 | bsz 333.3 | num_updates 2025 | best_loss 1.044\n",
      "2022-11-25 09:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 2025 updates\n",
      "2022-11-25 09:38:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 81 @ 2025 updates, score 1.045) (writing took 0.4370353049999949 seconds)\n",
      "2022-11-25 09:38:03 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
      "2022-11-25 09:38:03 | INFO | train | epoch 081 | loss 1.032 | nll_loss 0.187 | ppl 1.14 | wps 28456.1 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 2025 | lr 0.000702728 | gnorm 0.167 | clip 0 | train_wall 4 | gb_free 13.7 | wall 409\n",
      "2022-11-25 09:38:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 082:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:03 | INFO | fairseq.trainer | begin training epoch 82\n",
      "2022-11-25 09:38:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 082:  96% 24/25 [00:03<00:00,  5.94it/s]2022-11-25 09:38:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 082 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 11.92it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:08 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 1.047 | nll_loss 0.109 | ppl 1.08 | wps 72281.1 | wpb 4438 | bsz 333.3 | num_updates 2050 | best_loss 1.044\n",
      "2022-11-25 09:38:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 2050 updates\n",
      "2022-11-25 09:38:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 82 @ 2050 updates, score 1.047) (writing took 0.48438768599999094 seconds)\n",
      "2022-11-25 09:38:08 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
      "2022-11-25 09:38:08 | INFO | train | epoch 082 | loss 1.031 | nll_loss 0.186 | ppl 1.14 | wps 27549.8 | ups 5.18 | wpb 5313.9 | bsz 400 | num_updates 2050 | lr 0.00069843 | gnorm 0.161 | clip 0 | train_wall 4 | gb_free 13.4 | wall 414\n",
      "2022-11-25 09:38:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 083:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:08 | INFO | fairseq.trainer | begin training epoch 83\n",
      "2022-11-25 09:38:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 083:  96% 24/25 [00:03<00:00,  6.42it/s]2022-11-25 09:38:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 083 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.23it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:13 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 1.047 | nll_loss 0.112 | ppl 1.08 | wps 84226.3 | wpb 4438 | bsz 333.3 | num_updates 2075 | best_loss 1.044\n",
      "2022-11-25 09:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 2075 updates\n",
      "2022-11-25 09:38:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 83 @ 2075 updates, score 1.047) (writing took 0.44369486400000824 seconds)\n",
      "2022-11-25 09:38:13 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
      "2022-11-25 09:38:13 | INFO | train | epoch 083 | loss 1.033 | nll_loss 0.188 | ppl 1.14 | wps 28199.1 | ups 5.31 | wpb 5313.9 | bsz 400 | num_updates 2075 | lr 0.00069421 | gnorm 0.165 | clip 0 | train_wall 4 | gb_free 13.7 | wall 419\n",
      "2022-11-25 09:38:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 084:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:13 | INFO | fairseq.trainer | begin training epoch 84\n",
      "2022-11-25 09:38:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 084:  96% 24/25 [00:03<00:00,  6.21it/s]2022-11-25 09:38:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 084 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:17 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 1.044 | nll_loss 0.106 | ppl 1.08 | wps 85955.5 | wpb 4438 | bsz 333.3 | num_updates 2100 | best_loss 1.044\n",
      "2022-11-25 09:38:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 2100 updates\n",
      "2022-11-25 09:38:17 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 84 @ 2100 updates, score 1.044) (writing took 0.9963339480000286 seconds)\n",
      "2022-11-25 09:38:18 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
      "2022-11-25 09:38:18 | INFO | train | epoch 084 | loss 1.031 | nll_loss 0.185 | ppl 1.14 | wps 25169.6 | ups 4.74 | wpb 5313.9 | bsz 400 | num_updates 2100 | lr 0.000690066 | gnorm 0.181 | clip 0 | train_wall 4 | gb_free 13.2 | wall 424\n",
      "2022-11-25 09:38:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 085:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:18 | INFO | fairseq.trainer | begin training epoch 85\n",
      "2022-11-25 09:38:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 085:  96% 24/25 [00:03<00:00,  6.52it/s]2022-11-25 09:38:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 085 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:23 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 1.047 | nll_loss 0.109 | ppl 1.08 | wps 84898.3 | wpb 4438 | bsz 333.3 | num_updates 2125 | best_loss 1.044\n",
      "2022-11-25 09:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 2125 updates\n",
      "2022-11-25 09:38:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 85 @ 2125 updates, score 1.047) (writing took 0.5867728010000519 seconds)\n",
      "2022-11-25 09:38:23 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
      "2022-11-25 09:38:23 | INFO | train | epoch 085 | loss 1.03 | nll_loss 0.185 | ppl 1.14 | wps 27485.5 | ups 5.17 | wpb 5313.9 | bsz 400 | num_updates 2125 | lr 0.000685994 | gnorm 0.147 | clip 0 | train_wall 4 | gb_free 13.7 | wall 429\n",
      "2022-11-25 09:38:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 086:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:23 | INFO | fairseq.trainer | begin training epoch 86\n",
      "2022-11-25 09:38:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 086:  96% 24/25 [00:03<00:00,  6.08it/s]2022-11-25 09:38:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 086 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  8.40it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:27 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 1.043 | nll_loss 0.105 | ppl 1.08 | wps 88993.2 | wpb 4438 | bsz 333.3 | num_updates 2150 | best_loss 1.043\n",
      "2022-11-25 09:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 2150 updates\n",
      "2022-11-25 09:38:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 86 @ 2150 updates, score 1.043) (writing took 1.1071250270000519 seconds)\n",
      "2022-11-25 09:38:28 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
      "2022-11-25 09:38:28 | INFO | train | epoch 086 | loss 1.034 | nll_loss 0.189 | ppl 1.14 | wps 24655.3 | ups 4.64 | wpb 5313.9 | bsz 400 | num_updates 2150 | lr 0.000681994 | gnorm 0.179 | clip 0 | train_wall 4 | gb_free 13.5 | wall 434\n",
      "2022-11-25 09:38:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 087:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:29 | INFO | fairseq.trainer | begin training epoch 87\n",
      "2022-11-25 09:38:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 087:  96% 24/25 [00:03<00:00,  6.25it/s]2022-11-25 09:38:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 087 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:33 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 1.047 | nll_loss 0.107 | ppl 1.08 | wps 87695.6 | wpb 4438 | bsz 333.3 | num_updates 2175 | best_loss 1.043\n",
      "2022-11-25 09:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 2175 updates\n",
      "2022-11-25 09:38:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 87 @ 2175 updates, score 1.047) (writing took 0.40277093600002445 seconds)\n",
      "2022-11-25 09:38:33 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
      "2022-11-25 09:38:33 | INFO | train | epoch 087 | loss 1.033 | nll_loss 0.189 | ppl 1.14 | wps 28596.3 | ups 5.38 | wpb 5313.9 | bsz 400 | num_updates 2175 | lr 0.000678064 | gnorm 0.167 | clip 0 | train_wall 4 | gb_free 13.6 | wall 439\n",
      "2022-11-25 09:38:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 088:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:33 | INFO | fairseq.trainer | begin training epoch 88\n",
      "2022-11-25 09:38:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 088:  96% 24/25 [00:03<00:00,  6.40it/s]2022-11-25 09:38:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 088 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.11it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:37 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 1.042 | nll_loss 0.104 | ppl 1.07 | wps 98458.3 | wpb 4438 | bsz 333.3 | num_updates 2200 | best_loss 1.042\n",
      "2022-11-25 09:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 2200 updates\n",
      "2022-11-25 09:38:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 88 @ 2200 updates, score 1.042) (writing took 1.3392695549999871 seconds)\n",
      "2022-11-25 09:38:39 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
      "2022-11-25 09:38:39 | INFO | train | epoch 088 | loss 1.033 | nll_loss 0.189 | ppl 1.14 | wps 23619.4 | ups 4.44 | wpb 5313.9 | bsz 400 | num_updates 2200 | lr 0.0006742 | gnorm 0.182 | clip 0 | train_wall 4 | gb_free 13.3 | wall 445\n",
      "2022-11-25 09:38:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 089:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:39 | INFO | fairseq.trainer | begin training epoch 89\n",
      "2022-11-25 09:38:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 089:  96% 24/25 [00:03<00:00,  7.13it/s]2022-11-25 09:38:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 089 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.02it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:43 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 1.043 | nll_loss 0.108 | ppl 1.08 | wps 84834.6 | wpb 4438 | bsz 333.3 | num_updates 2225 | best_loss 1.042\n",
      "2022-11-25 09:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 2225 updates\n",
      "2022-11-25 09:38:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 89 @ 2225 updates, score 1.043) (writing took 0.4652671519999103 seconds)\n",
      "2022-11-25 09:38:43 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
      "2022-11-25 09:38:43 | INFO | train | epoch 089 | loss 1.031 | nll_loss 0.187 | ppl 1.14 | wps 28242.6 | ups 5.31 | wpb 5313.9 | bsz 400 | num_updates 2225 | lr 0.000670402 | gnorm 0.149 | clip 0 | train_wall 4 | gb_free 13.4 | wall 449\n",
      "2022-11-25 09:38:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 090:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:43 | INFO | fairseq.trainer | begin training epoch 90\n",
      "2022-11-25 09:38:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 090:  96% 24/25 [00:03<00:00,  6.73it/s]2022-11-25 09:38:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 090 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.70it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:48 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 1.041 | nll_loss 0.103 | ppl 1.07 | wps 88391.7 | wpb 4438 | bsz 333.3 | num_updates 2250 | best_loss 1.041\n",
      "2022-11-25 09:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 2250 updates\n",
      "2022-11-25 09:38:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 90 @ 2250 updates, score 1.041) (writing took 1.3237832390000221 seconds)\n",
      "2022-11-25 09:38:49 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
      "2022-11-25 09:38:49 | INFO | train | epoch 090 | loss 1.028 | nll_loss 0.182 | ppl 1.13 | wps 23440.6 | ups 4.41 | wpb 5313.9 | bsz 400 | num_updates 2250 | lr 0.000666667 | gnorm 0.147 | clip 0 | train_wall 4 | gb_free 13.7 | wall 455\n",
      "2022-11-25 09:38:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 091:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:49 | INFO | fairseq.trainer | begin training epoch 91\n",
      "2022-11-25 09:38:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 091:  96% 24/25 [00:03<00:00,  6.74it/s]2022-11-25 09:38:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 091 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.61it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:53 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 1.046 | nll_loss 0.107 | ppl 1.08 | wps 83879.9 | wpb 4438 | bsz 333.3 | num_updates 2275 | best_loss 1.041\n",
      "2022-11-25 09:38:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 2275 updates\n",
      "2022-11-25 09:38:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:38:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 91 @ 2275 updates, score 1.046) (writing took 0.6655238680000366 seconds)\n",
      "2022-11-25 09:38:54 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
      "2022-11-25 09:38:54 | INFO | train | epoch 091 | loss 1.027 | nll_loss 0.181 | ppl 1.13 | wps 26977.2 | ups 5.08 | wpb 5313.9 | bsz 400 | num_updates 2275 | lr 0.000662994 | gnorm 0.131 | clip 0 | train_wall 4 | gb_free 13.5 | wall 460\n",
      "2022-11-25 09:38:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 092:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:38:54 | INFO | fairseq.trainer | begin training epoch 92\n",
      "2022-11-25 09:38:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 092:  96% 24/25 [00:04<00:00,  5.69it/s]2022-11-25 09:38:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 092 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:38:59 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 1.042 | nll_loss 0.105 | ppl 1.08 | wps 72933.3 | wpb 4438 | bsz 333.3 | num_updates 2300 | best_loss 1.041\n",
      "2022-11-25 09:38:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 2300 updates\n",
      "2022-11-25 09:38:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 92 @ 2300 updates, score 1.042) (writing took 0.39519614699997874 seconds)\n",
      "2022-11-25 09:39:00 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
      "2022-11-25 09:39:00 | INFO | train | epoch 092 | loss 1.028 | nll_loss 0.183 | ppl 1.14 | wps 24093.3 | ups 4.53 | wpb 5313.9 | bsz 400 | num_updates 2300 | lr 0.00065938 | gnorm 0.154 | clip 0 | train_wall 5 | gb_free 13.6 | wall 466\n",
      "2022-11-25 09:39:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 093:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:00 | INFO | fairseq.trainer | begin training epoch 93\n",
      "2022-11-25 09:39:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 093:  96% 24/25 [00:03<00:00,  6.13it/s]2022-11-25 09:39:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 093 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.22it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:04 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 1.044 | nll_loss 0.105 | ppl 1.08 | wps 92120.4 | wpb 4438 | bsz 333.3 | num_updates 2325 | best_loss 1.041\n",
      "2022-11-25 09:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 2325 updates\n",
      "2022-11-25 09:39:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 93 @ 2325 updates, score 1.044) (writing took 0.4061030230000142 seconds)\n",
      "2022-11-25 09:39:04 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
      "2022-11-25 09:39:04 | INFO | train | epoch 093 | loss 1.028 | nll_loss 0.183 | ppl 1.14 | wps 28510.4 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 2325 | lr 0.000655826 | gnorm 0.139 | clip 0 | train_wall 4 | gb_free 13.7 | wall 470\n",
      "2022-11-25 09:39:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 094:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:04 | INFO | fairseq.trainer | begin training epoch 94\n",
      "2022-11-25 09:39:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 094:  96% 24/25 [00:03<00:00,  6.30it/s]2022-11-25 09:39:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 094 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.38it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:08 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 1.042 | nll_loss 0.107 | ppl 1.08 | wps 84021.1 | wpb 4438 | bsz 333.3 | num_updates 2350 | best_loss 1.041\n",
      "2022-11-25 09:39:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 2350 updates\n",
      "2022-11-25 09:39:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 94 @ 2350 updates, score 1.042) (writing took 0.41027267199990547 seconds)\n",
      "2022-11-25 09:39:09 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
      "2022-11-25 09:39:09 | INFO | train | epoch 094 | loss 1.029 | nll_loss 0.184 | ppl 1.14 | wps 28346.6 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 2350 | lr 0.000652328 | gnorm 0.169 | clip 0 | train_wall 4 | gb_free 13.7 | wall 475\n",
      "2022-11-25 09:39:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 095:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:09 | INFO | fairseq.trainer | begin training epoch 95\n",
      "2022-11-25 09:39:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 095:  96% 24/25 [00:03<00:00,  6.48it/s]2022-11-25 09:39:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 095 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.17it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:13 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 1.043 | nll_loss 0.104 | ppl 1.07 | wps 86978.9 | wpb 4438 | bsz 333.3 | num_updates 2375 | best_loss 1.041\n",
      "2022-11-25 09:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 2375 updates\n",
      "2022-11-25 09:39:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 95 @ 2375 updates, score 1.043) (writing took 0.40918213599991304 seconds)\n",
      "2022-11-25 09:39:14 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
      "2022-11-25 09:39:14 | INFO | train | epoch 095 | loss 1.029 | nll_loss 0.184 | ppl 1.14 | wps 28501.8 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 2375 | lr 0.000648886 | gnorm 0.173 | clip 0 | train_wall 4 | gb_free 13.4 | wall 480\n",
      "2022-11-25 09:39:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 096:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:14 | INFO | fairseq.trainer | begin training epoch 96\n",
      "2022-11-25 09:39:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 096:  96% 24/25 [00:03<00:00,  6.97it/s]2022-11-25 09:39:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 096 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.54it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:18 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 1.043 | nll_loss 0.106 | ppl 1.08 | wps 87075.7 | wpb 4438 | bsz 333.3 | num_updates 2400 | best_loss 1.041\n",
      "2022-11-25 09:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 2400 updates\n",
      "2022-11-25 09:39:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 96 @ 2400 updates, score 1.043) (writing took 0.4079165200000716 seconds)\n",
      "2022-11-25 09:39:18 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
      "2022-11-25 09:39:18 | INFO | train | epoch 096 | loss 1.027 | nll_loss 0.182 | ppl 1.13 | wps 28744.8 | ups 5.41 | wpb 5313.9 | bsz 400 | num_updates 2400 | lr 0.000645497 | gnorm 0.151 | clip 0 | train_wall 4 | gb_free 13.3 | wall 484\n",
      "2022-11-25 09:39:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 097:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:18 | INFO | fairseq.trainer | begin training epoch 97\n",
      "2022-11-25 09:39:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 097:  96% 24/25 [00:03<00:00,  6.12it/s]2022-11-25 09:39:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 097 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:22 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 1.044 | nll_loss 0.106 | ppl 1.08 | wps 87055.7 | wpb 4438 | bsz 333.3 | num_updates 2425 | best_loss 1.041\n",
      "2022-11-25 09:39:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 2425 updates\n",
      "2022-11-25 09:39:22 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 97 @ 2425 updates, score 1.044) (writing took 0.5604414899999028 seconds)\n",
      "2022-11-25 09:39:23 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
      "2022-11-25 09:39:23 | INFO | train | epoch 097 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 27792.7 | ups 5.23 | wpb 5313.9 | bsz 400 | num_updates 2425 | lr 0.000642161 | gnorm 0.162 | clip 0 | train_wall 4 | gb_free 13.5 | wall 489\n",
      "2022-11-25 09:39:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 098:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:23 | INFO | fairseq.trainer | begin training epoch 98\n",
      "2022-11-25 09:39:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 098:  96% 24/25 [00:03<00:00,  6.66it/s]2022-11-25 09:39:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 098 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.26it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:27 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 1.04 | nll_loss 0.105 | ppl 1.08 | wps 92398 | wpb 4438 | bsz 333.3 | num_updates 2450 | best_loss 1.04\n",
      "2022-11-25 09:39:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 2450 updates\n",
      "2022-11-25 09:39:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 98 @ 2450 updates, score 1.04) (writing took 0.8743317710000156 seconds)\n",
      "2022-11-25 09:39:28 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
      "2022-11-25 09:39:28 | INFO | train | epoch 098 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 25848.8 | ups 4.86 | wpb 5313.9 | bsz 400 | num_updates 2450 | lr 0.000638877 | gnorm 0.126 | clip 0 | train_wall 4 | gb_free 13.2 | wall 494\n",
      "2022-11-25 09:39:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 099:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:28 | INFO | fairseq.trainer | begin training epoch 99\n",
      "2022-11-25 09:39:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 099:  96% 24/25 [00:03<00:00,  6.93it/s]2022-11-25 09:39:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 099 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.88it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:32 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 1.042 | nll_loss 0.103 | ppl 1.07 | wps 85265.1 | wpb 4438 | bsz 333.3 | num_updates 2475 | best_loss 1.04\n",
      "2022-11-25 09:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 2475 updates\n",
      "2022-11-25 09:39:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 99 @ 2475 updates, score 1.042) (writing took 0.4686976740000546 seconds)\n",
      "2022-11-25 09:39:33 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
      "2022-11-25 09:39:33 | INFO | train | epoch 099 | loss 1.023 | nll_loss 0.178 | ppl 1.13 | wps 28040.2 | ups 5.28 | wpb 5313.9 | bsz 400 | num_updates 2475 | lr 0.000635642 | gnorm 0.138 | clip 0 | train_wall 4 | gb_free 13.6 | wall 499\n",
      "2022-11-25 09:39:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 100:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:33 | INFO | fairseq.trainer | begin training epoch 100\n",
      "2022-11-25 09:39:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 100:  96% 24/25 [00:04<00:00,  6.16it/s]2022-11-25 09:39:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 100 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:37 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 1.044 | nll_loss 0.104 | ppl 1.07 | wps 86545.1 | wpb 4438 | bsz 333.3 | num_updates 2500 | best_loss 1.04\n",
      "2022-11-25 09:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 2500 updates\n",
      "2022-11-25 09:39:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 100 @ 2500 updates, score 1.044) (writing took 0.407288628999936 seconds)\n",
      "2022-11-25 09:39:38 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
      "2022-11-25 09:39:38 | INFO | train | epoch 100 | loss 1.026 | nll_loss 0.18 | ppl 1.13 | wps 27723.1 | ups 5.22 | wpb 5313.9 | bsz 400 | num_updates 2500 | lr 0.000632456 | gnorm 0.153 | clip 0 | train_wall 4 | gb_free 13.7 | wall 504\n",
      "2022-11-25 09:39:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 101:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:38 | INFO | fairseq.trainer | begin training epoch 101\n",
      "2022-11-25 09:39:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 101:  96% 24/25 [00:03<00:00,  6.78it/s]2022-11-25 09:39:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 101 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.62it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:42 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 1.043 | nll_loss 0.105 | ppl 1.08 | wps 87629.4 | wpb 4438 | bsz 333.3 | num_updates 2525 | best_loss 1.04\n",
      "2022-11-25 09:39:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 2525 updates\n",
      "2022-11-25 09:39:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:39:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 101 @ 2525 updates, score 1.043) (writing took 0.3940024989999529 seconds)\n",
      "2022-11-25 09:39:42 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
      "2022-11-25 09:39:42 | INFO | train | epoch 101 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 28540.7 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 2525 | lr 0.000629317 | gnorm 0.148 | clip 0 | train_wall 4 | gb_free 13.5 | wall 508\n",
      "2022-11-25 09:39:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 102:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:42 | INFO | fairseq.trainer | begin training epoch 102\n",
      "2022-11-25 09:39:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 102:  96% 24/25 [00:03<00:00,  6.52it/s]2022-11-25 09:39:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 102 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:47 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 87697.3 | wpb 4438 | bsz 333.3 | num_updates 2550 | best_loss 1.04\n",
      "2022-11-25 09:39:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 2550 updates\n",
      "2022-11-25 09:39:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 102 @ 2550 updates, score 1.04) (writing took 0.8812444259999666 seconds)\n",
      "2022-11-25 09:39:47 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
      "2022-11-25 09:39:47 | INFO | train | epoch 102 | loss 1.023 | nll_loss 0.177 | ppl 1.13 | wps 25867.4 | ups 4.87 | wpb 5313.9 | bsz 400 | num_updates 2550 | lr 0.000626224 | gnorm 0.14 | clip 0 | train_wall 4 | gb_free 13.5 | wall 513\n",
      "2022-11-25 09:39:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 103:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:47 | INFO | fairseq.trainer | begin training epoch 103\n",
      "2022-11-25 09:39:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 103:  96% 24/25 [00:03<00:00,  6.44it/s]2022-11-25 09:39:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 103 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.29it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:52 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 1.039 | nll_loss 0.101 | ppl 1.07 | wps 87635.8 | wpb 4438 | bsz 333.3 | num_updates 2575 | best_loss 1.039\n",
      "2022-11-25 09:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 2575 updates\n",
      "2022-11-25 09:39:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 103 @ 2575 updates, score 1.039) (writing took 0.8367332179999494 seconds)\n",
      "2022-11-25 09:39:53 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
      "2022-11-25 09:39:53 | INFO | train | epoch 103 | loss 1.023 | nll_loss 0.178 | ppl 1.13 | wps 26008.8 | ups 4.89 | wpb 5313.9 | bsz 400 | num_updates 2575 | lr 0.000623177 | gnorm 0.155 | clip 0 | train_wall 4 | gb_free 13.7 | wall 519\n",
      "2022-11-25 09:39:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 104:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:53 | INFO | fairseq.trainer | begin training epoch 104\n",
      "2022-11-25 09:39:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 104:  96% 24/25 [00:03<00:00,  6.94it/s]2022-11-25 09:39:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 104 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:39:57 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 1.039 | nll_loss 0.102 | ppl 1.07 | wps 87742.2 | wpb 4438 | bsz 333.3 | num_updates 2600 | best_loss 1.039\n",
      "2022-11-25 09:39:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 2600 updates\n",
      "2022-11-25 09:39:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:39:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 104 @ 2600 updates, score 1.039) (writing took 0.8419314989999975 seconds)\n",
      "2022-11-25 09:39:58 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
      "2022-11-25 09:39:58 | INFO | train | epoch 104 | loss 1.024 | nll_loss 0.179 | ppl 1.13 | wps 26222.7 | ups 4.93 | wpb 5313.9 | bsz 400 | num_updates 2600 | lr 0.000620174 | gnorm 0.134 | clip 0 | train_wall 4 | gb_free 13.3 | wall 524\n",
      "2022-11-25 09:39:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 105:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:39:58 | INFO | fairseq.trainer | begin training epoch 105\n",
      "2022-11-25 09:39:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 105:  96% 24/25 [00:03<00:00,  7.20it/s]2022-11-25 09:40:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 105 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:02 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 1.042 | nll_loss 0.103 | ppl 1.07 | wps 86404.3 | wpb 4438 | bsz 333.3 | num_updates 2625 | best_loss 1.039\n",
      "2022-11-25 09:40:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 2625 updates\n",
      "2022-11-25 09:40:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 105 @ 2625 updates, score 1.042) (writing took 0.3773715799999309 seconds)\n",
      "2022-11-25 09:40:02 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
      "2022-11-25 09:40:02 | INFO | train | epoch 105 | loss 1.022 | nll_loss 0.177 | ppl 1.13 | wps 28720.4 | ups 5.4 | wpb 5313.9 | bsz 400 | num_updates 2625 | lr 0.000617213 | gnorm 0.143 | clip 0 | train_wall 4 | gb_free 13.3 | wall 528\n",
      "2022-11-25 09:40:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 106:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:02 | INFO | fairseq.trainer | begin training epoch 106\n",
      "2022-11-25 09:40:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 106:  96% 24/25 [00:03<00:00,  7.01it/s]2022-11-25 09:40:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 106 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.93it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:07 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 1.042 | nll_loss 0.103 | ppl 1.07 | wps 87105.3 | wpb 4438 | bsz 333.3 | num_updates 2650 | best_loss 1.039\n",
      "2022-11-25 09:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 2650 updates\n",
      "2022-11-25 09:40:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 106 @ 2650 updates, score 1.042) (writing took 0.5161526560000311 seconds)\n",
      "2022-11-25 09:40:07 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
      "2022-11-25 09:40:07 | INFO | train | epoch 106 | loss 1.021 | nll_loss 0.176 | ppl 1.13 | wps 27553.4 | ups 5.19 | wpb 5313.9 | bsz 400 | num_updates 2650 | lr 0.000614295 | gnorm 0.131 | clip 0 | train_wall 4 | gb_free 13.7 | wall 533\n",
      "2022-11-25 09:40:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 107:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:07 | INFO | fairseq.trainer | begin training epoch 107\n",
      "2022-11-25 09:40:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 107:  96% 24/25 [00:03<00:00,  6.72it/s]2022-11-25 09:40:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 107 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:11 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 1.039 | nll_loss 0.102 | ppl 1.07 | wps 85724.9 | wpb 4438 | bsz 333.3 | num_updates 2675 | best_loss 1.039\n",
      "2022-11-25 09:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 2675 updates\n",
      "2022-11-25 09:40:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 107 @ 2675 updates, score 1.039) (writing took 0.9000410459999557 seconds)\n",
      "2022-11-25 09:40:12 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
      "2022-11-25 09:40:12 | INFO | train | epoch 107 | loss 1.023 | nll_loss 0.178 | ppl 1.13 | wps 25858.8 | ups 4.87 | wpb 5313.9 | bsz 400 | num_updates 2675 | lr 0.000611418 | gnorm 0.141 | clip 0 | train_wall 4 | gb_free 13.5 | wall 538\n",
      "2022-11-25 09:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 108:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:12 | INFO | fairseq.trainer | begin training epoch 108\n",
      "2022-11-25 09:40:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 108:  96% 24/25 [00:03<00:00,  7.00it/s]2022-11-25 09:40:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 108 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.45it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:16 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 1.041 | nll_loss 0.1 | ppl 1.07 | wps 80756.3 | wpb 4438 | bsz 333.3 | num_updates 2700 | best_loss 1.039\n",
      "2022-11-25 09:40:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 2700 updates\n",
      "2022-11-25 09:40:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 108 @ 2700 updates, score 1.041) (writing took 0.3810618859999977 seconds)\n",
      "2022-11-25 09:40:17 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
      "2022-11-25 09:40:17 | INFO | train | epoch 108 | loss 1.024 | nll_loss 0.179 | ppl 1.13 | wps 28829.6 | ups 5.43 | wpb 5313.9 | bsz 400 | num_updates 2700 | lr 0.000608581 | gnorm 0.15 | clip 0 | train_wall 4 | gb_free 13.3 | wall 543\n",
      "2022-11-25 09:40:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 109:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:17 | INFO | fairseq.trainer | begin training epoch 109\n",
      "2022-11-25 09:40:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 109:  96% 24/25 [00:03<00:00,  6.10it/s]2022-11-25 09:40:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 109 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.71it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:21 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 1.041 | nll_loss 0.104 | ppl 1.07 | wps 85478.9 | wpb 4438 | bsz 333.3 | num_updates 2725 | best_loss 1.039\n",
      "2022-11-25 09:40:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 2725 updates\n",
      "2022-11-25 09:40:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 109 @ 2725 updates, score 1.041) (writing took 0.43805706000000555 seconds)\n",
      "2022-11-25 09:40:22 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
      "2022-11-25 09:40:22 | INFO | train | epoch 109 | loss 1.022 | nll_loss 0.177 | ppl 1.13 | wps 27532.6 | ups 5.18 | wpb 5313.9 | bsz 400 | num_updates 2725 | lr 0.000605783 | gnorm 0.134 | clip 0 | train_wall 4 | gb_free 13.2 | wall 548\n",
      "2022-11-25 09:40:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 110:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:22 | INFO | fairseq.trainer | begin training epoch 110\n",
      "2022-11-25 09:40:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 110:  96% 24/25 [00:03<00:00,  6.81it/s]2022-11-25 09:40:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 110 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.94it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:26 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 1.041 | nll_loss 0.103 | ppl 1.07 | wps 85666.3 | wpb 4438 | bsz 333.3 | num_updates 2750 | best_loss 1.039\n",
      "2022-11-25 09:40:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 2750 updates\n",
      "2022-11-25 09:40:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 110 @ 2750 updates, score 1.041) (writing took 0.410122079999951 seconds)\n",
      "2022-11-25 09:40:26 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
      "2022-11-25 09:40:26 | INFO | train | epoch 110 | loss 1.022 | nll_loss 0.177 | ppl 1.13 | wps 28696.4 | ups 5.4 | wpb 5313.9 | bsz 400 | num_updates 2750 | lr 0.000603023 | gnorm 0.143 | clip 0 | train_wall 4 | gb_free 13.7 | wall 552\n",
      "2022-11-25 09:40:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 111:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:26 | INFO | fairseq.trainer | begin training epoch 111\n",
      "2022-11-25 09:40:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 111:  96% 24/25 [00:03<00:00,  6.92it/s]2022-11-25 09:40:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 111 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.14it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:31 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 1.042 | nll_loss 0.101 | ppl 1.07 | wps 84251.2 | wpb 4438 | bsz 333.3 | num_updates 2775 | best_loss 1.039\n",
      "2022-11-25 09:40:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 2775 updates\n",
      "2022-11-25 09:40:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 111 @ 2775 updates, score 1.042) (writing took 0.3983804050000117 seconds)\n",
      "2022-11-25 09:40:31 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
      "2022-11-25 09:40:31 | INFO | train | epoch 111 | loss 1.021 | nll_loss 0.175 | ppl 1.13 | wps 28247.8 | ups 5.32 | wpb 5313.9 | bsz 400 | num_updates 2775 | lr 0.0006003 | gnorm 0.131 | clip 0 | train_wall 4 | gb_free 12.9 | wall 557\n",
      "2022-11-25 09:40:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 112:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:31 | INFO | fairseq.trainer | begin training epoch 112\n",
      "2022-11-25 09:40:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 112:  96% 24/25 [00:03<00:00,  6.27it/s]2022-11-25 09:40:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 112 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.96it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:35 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 1.04 | nll_loss 0.1 | ppl 1.07 | wps 84477.2 | wpb 4438 | bsz 333.3 | num_updates 2800 | best_loss 1.039\n",
      "2022-11-25 09:40:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 2800 updates\n",
      "2022-11-25 09:40:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 112 @ 2800 updates, score 1.04) (writing took 0.4049112930000547 seconds)\n",
      "2022-11-25 09:40:36 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
      "2022-11-25 09:40:36 | INFO | train | epoch 112 | loss 1.02 | nll_loss 0.174 | ppl 1.13 | wps 28431.1 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 2800 | lr 0.000597614 | gnorm 0.139 | clip 0 | train_wall 4 | gb_free 13.2 | wall 562\n",
      "2022-11-25 09:40:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 113:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:36 | INFO | fairseq.trainer | begin training epoch 113\n",
      "2022-11-25 09:40:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 113:  96% 24/25 [00:03<00:00,  6.59it/s]2022-11-25 09:40:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 113 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.37it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:40 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 1.042 | nll_loss 0.1 | ppl 1.07 | wps 79049.1 | wpb 4438 | bsz 333.3 | num_updates 2825 | best_loss 1.039\n",
      "2022-11-25 09:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 2825 updates\n",
      "2022-11-25 09:40:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 113 @ 2825 updates, score 1.042) (writing took 0.43071816100007254 seconds)\n",
      "2022-11-25 09:40:40 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
      "2022-11-25 09:40:40 | INFO | train | epoch 113 | loss 1.02 | nll_loss 0.174 | ppl 1.13 | wps 28659.7 | ups 5.39 | wpb 5313.9 | bsz 400 | num_updates 2825 | lr 0.000594964 | gnorm 0.126 | clip 0 | train_wall 4 | gb_free 13.3 | wall 566\n",
      "2022-11-25 09:40:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 114:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:40 | INFO | fairseq.trainer | begin training epoch 114\n",
      "2022-11-25 09:40:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 114:  96% 24/25 [00:03<00:00,  5.67it/s]2022-11-25 09:40:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 114 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 10.19it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:45 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 1.041 | nll_loss 0.1 | ppl 1.07 | wps 58691.4 | wpb 4438 | bsz 333.3 | num_updates 2850 | best_loss 1.039\n",
      "2022-11-25 09:40:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 2850 updates\n",
      "2022-11-25 09:40:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 114 @ 2850 updates, score 1.041) (writing took 0.4213056390000247 seconds)\n",
      "2022-11-25 09:40:45 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
      "2022-11-25 09:40:45 | INFO | train | epoch 114 | loss 1.019 | nll_loss 0.173 | ppl 1.13 | wps 28157.3 | ups 5.3 | wpb 5313.9 | bsz 400 | num_updates 2850 | lr 0.000592349 | gnorm 0.129 | clip 0 | train_wall 4 | gb_free 13.5 | wall 571\n",
      "2022-11-25 09:40:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 115:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:45 | INFO | fairseq.trainer | begin training epoch 115\n",
      "2022-11-25 09:40:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 115:  96% 24/25 [00:03<00:00,  6.34it/s]2022-11-25 09:40:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 115 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:49 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 1.041 | nll_loss 0.104 | ppl 1.08 | wps 81542.8 | wpb 4438 | bsz 333.3 | num_updates 2875 | best_loss 1.039\n",
      "2022-11-25 09:40:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 2875 updates\n",
      "2022-11-25 09:40:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 115 @ 2875 updates, score 1.041) (writing took 0.424353850999978 seconds)\n",
      "2022-11-25 09:40:50 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
      "2022-11-25 09:40:50 | INFO | train | epoch 115 | loss 1.019 | nll_loss 0.173 | ppl 1.13 | wps 28434.8 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 2875 | lr 0.000589768 | gnorm 0.118 | clip 0 | train_wall 4 | gb_free 13.7 | wall 576\n",
      "2022-11-25 09:40:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 116:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:50 | INFO | fairseq.trainer | begin training epoch 116\n",
      "2022-11-25 09:40:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 116:  96% 24/25 [00:03<00:00,  6.37it/s]2022-11-25 09:40:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 116 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.05it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:54 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 1.042 | nll_loss 0.103 | ppl 1.07 | wps 80078.5 | wpb 4438 | bsz 333.3 | num_updates 2900 | best_loss 1.039\n",
      "2022-11-25 09:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 2900 updates\n",
      "2022-11-25 09:40:54 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 116 @ 2900 updates, score 1.042) (writing took 0.39408437700001286 seconds)\n",
      "2022-11-25 09:40:54 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
      "2022-11-25 09:40:54 | INFO | train | epoch 116 | loss 1.021 | nll_loss 0.176 | ppl 1.13 | wps 28407.5 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 2900 | lr 0.00058722 | gnorm 0.136 | clip 0 | train_wall 4 | gb_free 13.2 | wall 580\n",
      "2022-11-25 09:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 117:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:54 | INFO | fairseq.trainer | begin training epoch 117\n",
      "2022-11-25 09:40:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 117:  96% 24/25 [00:03<00:00,  6.15it/s]2022-11-25 09:40:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 117 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:40:59 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 1.041 | nll_loss 0.102 | ppl 1.07 | wps 85920 | wpb 4438 | bsz 333.3 | num_updates 2925 | best_loss 1.039\n",
      "2022-11-25 09:40:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 2925 updates\n",
      "2022-11-25 09:40:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 117 @ 2925 updates, score 1.041) (writing took 0.44483640300006755 seconds)\n",
      "2022-11-25 09:40:59 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
      "2022-11-25 09:40:59 | INFO | train | epoch 117 | loss 1.021 | nll_loss 0.176 | ppl 1.13 | wps 28354.4 | ups 5.34 | wpb 5313.9 | bsz 400 | num_updates 2925 | lr 0.000584705 | gnorm 0.172 | clip 0 | train_wall 4 | gb_free 13.7 | wall 585\n",
      "2022-11-25 09:40:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 118:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:40:59 | INFO | fairseq.trainer | begin training epoch 118\n",
      "2022-11-25 09:40:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 118:  96% 24/25 [00:03<00:00,  6.58it/s]2022-11-25 09:41:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 118 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:03 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 1.04 | nll_loss 0.101 | ppl 1.07 | wps 76628.3 | wpb 4438 | bsz 333.3 | num_updates 2950 | best_loss 1.039\n",
      "2022-11-25 09:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 2950 updates\n",
      "2022-11-25 09:41:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 118 @ 2950 updates, score 1.04) (writing took 0.41219243999989885 seconds)\n",
      "2022-11-25 09:41:04 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
      "2022-11-25 09:41:04 | INFO | train | epoch 118 | loss 1.02 | nll_loss 0.174 | ppl 1.13 | wps 28434.7 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 2950 | lr 0.000582223 | gnorm 0.162 | clip 0 | train_wall 4 | gb_free 13.7 | wall 590\n",
      "2022-11-25 09:41:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 119:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:04 | INFO | fairseq.trainer | begin training epoch 119\n",
      "2022-11-25 09:41:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 119:  96% 24/25 [00:03<00:00,  5.89it/s]2022-11-25 09:41:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 119 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.28it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:08 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 1.039 | nll_loss 0.102 | ppl 1.07 | wps 79875.4 | wpb 4438 | bsz 333.3 | num_updates 2975 | best_loss 1.039\n",
      "2022-11-25 09:41:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 2975 updates\n",
      "2022-11-25 09:41:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:41:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:41:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 119 @ 2975 updates, score 1.039) (writing took 0.9029788209999197 seconds)\n",
      "2022-11-25 09:41:09 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
      "2022-11-25 09:41:09 | INFO | train | epoch 119 | loss 1.02 | nll_loss 0.175 | ppl 1.13 | wps 25775.7 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 2975 | lr 0.000579771 | gnorm 0.152 | clip 0 | train_wall 4 | gb_free 13.2 | wall 595\n",
      "2022-11-25 09:41:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 120:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:09 | INFO | fairseq.trainer | begin training epoch 120\n",
      "2022-11-25 09:41:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 120:  96% 24/25 [00:03<00:00,  6.51it/s]2022-11-25 09:41:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 120 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:13 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 1.042 | nll_loss 0.104 | ppl 1.07 | wps 78388.9 | wpb 4438 | bsz 333.3 | num_updates 3000 | best_loss 1.039\n",
      "2022-11-25 09:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 3000 updates\n",
      "2022-11-25 09:41:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 120 @ 3000 updates, score 1.042) (writing took 0.3917989830000579 seconds)\n",
      "2022-11-25 09:41:14 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
      "2022-11-25 09:41:14 | INFO | train | epoch 120 | loss 1.02 | nll_loss 0.174 | ppl 1.13 | wps 28187.1 | ups 5.3 | wpb 5313.9 | bsz 400 | num_updates 3000 | lr 0.00057735 | gnorm 0.172 | clip 0 | train_wall 4 | gb_free 13.6 | wall 600\n",
      "2022-11-25 09:41:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 121:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:14 | INFO | fairseq.trainer | begin training epoch 121\n",
      "2022-11-25 09:41:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 121:  96% 24/25 [00:04<00:00,  5.47it/s]2022-11-25 09:41:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 121 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:18 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 1.041 | nll_loss 0.102 | ppl 1.07 | wps 90003.5 | wpb 4438 | bsz 333.3 | num_updates 3025 | best_loss 1.039\n",
      "2022-11-25 09:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 3025 updates\n",
      "2022-11-25 09:41:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 121 @ 3025 updates, score 1.041) (writing took 0.39044989100000294 seconds)\n",
      "2022-11-25 09:41:18 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
      "2022-11-25 09:41:18 | INFO | train | epoch 121 | loss 1.02 | nll_loss 0.174 | ppl 1.13 | wps 27269.2 | ups 5.13 | wpb 5313.9 | bsz 400 | num_updates 3025 | lr 0.00057496 | gnorm 0.115 | clip 0 | train_wall 4 | gb_free 13.5 | wall 604\n",
      "2022-11-25 09:41:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 122:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:18 | INFO | fairseq.trainer | begin training epoch 122\n",
      "2022-11-25 09:41:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 122:  96% 24/25 [00:03<00:00,  6.10it/s]2022-11-25 09:41:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 122 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:23 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 1.041 | nll_loss 0.101 | ppl 1.07 | wps 86404 | wpb 4438 | bsz 333.3 | num_updates 3050 | best_loss 1.039\n",
      "2022-11-25 09:41:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 3050 updates\n",
      "2022-11-25 09:41:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 122 @ 3050 updates, score 1.041) (writing took 0.4369268320000401 seconds)\n",
      "2022-11-25 09:41:23 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
      "2022-11-25 09:41:23 | INFO | train | epoch 122 | loss 1.018 | nll_loss 0.173 | ppl 1.13 | wps 28546.9 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 3050 | lr 0.000572598 | gnorm 0.166 | clip 0 | train_wall 4 | gb_free 13.8 | wall 609\n",
      "2022-11-25 09:41:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 123:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:23 | INFO | fairseq.trainer | begin training epoch 123\n",
      "2022-11-25 09:41:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 123:  96% 24/25 [00:03<00:00,  6.32it/s]2022-11-25 09:41:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 123 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.95it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:27 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 81005.9 | wpb 4438 | bsz 333.3 | num_updates 3075 | best_loss 1.039\n",
      "2022-11-25 09:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 3075 updates\n",
      "2022-11-25 09:41:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 123 @ 3075 updates, score 1.04) (writing took 0.4142386160000342 seconds)\n",
      "2022-11-25 09:41:28 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
      "2022-11-25 09:41:28 | INFO | train | epoch 123 | loss 1.018 | nll_loss 0.172 | ppl 1.13 | wps 28283.2 | ups 5.32 | wpb 5313.9 | bsz 400 | num_updates 3075 | lr 0.000570266 | gnorm 0.142 | clip 0 | train_wall 4 | gb_free 13.7 | wall 614\n",
      "2022-11-25 09:41:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 124:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:28 | INFO | fairseq.trainer | begin training epoch 124\n",
      "2022-11-25 09:41:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 124:  96% 24/25 [00:03<00:00,  7.18it/s]2022-11-25 09:41:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 124 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:32 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 1.038 | nll_loss 0.101 | ppl 1.07 | wps 85438.8 | wpb 4438 | bsz 333.3 | num_updates 3100 | best_loss 1.038\n",
      "2022-11-25 09:41:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 3100 updates\n",
      "2022-11-25 09:41:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 124 @ 3100 updates, score 1.038) (writing took 0.858836758999928 seconds)\n",
      "2022-11-25 09:41:33 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
      "2022-11-25 09:41:33 | INFO | train | epoch 124 | loss 1.017 | nll_loss 0.172 | ppl 1.13 | wps 26059.5 | ups 4.9 | wpb 5313.9 | bsz 400 | num_updates 3100 | lr 0.000567962 | gnorm 0.122 | clip 0 | train_wall 4 | gb_free 13.7 | wall 619\n",
      "2022-11-25 09:41:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 125:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:33 | INFO | fairseq.trainer | begin training epoch 125\n",
      "2022-11-25 09:41:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 125:  96% 24/25 [00:03<00:00,  6.61it/s]2022-11-25 09:41:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 125 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.16it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:37 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 1.042 | nll_loss 0.104 | ppl 1.07 | wps 87457 | wpb 4438 | bsz 333.3 | num_updates 3125 | best_loss 1.038\n",
      "2022-11-25 09:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 3125 updates\n",
      "2022-11-25 09:41:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 125 @ 3125 updates, score 1.042) (writing took 0.36808632299994315 seconds)\n",
      "2022-11-25 09:41:37 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
      "2022-11-25 09:41:37 | INFO | train | epoch 125 | loss 1.019 | nll_loss 0.173 | ppl 1.13 | wps 28925.5 | ups 5.44 | wpb 5313.9 | bsz 400 | num_updates 3125 | lr 0.000565685 | gnorm 0.139 | clip 0 | train_wall 4 | gb_free 13.6 | wall 623\n",
      "2022-11-25 09:41:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 126:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:38 | INFO | fairseq.trainer | begin training epoch 126\n",
      "2022-11-25 09:41:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 126:  96% 24/25 [00:03<00:00,  6.24it/s]2022-11-25 09:41:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 126 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.10it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:42 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 1.04 | nll_loss 0.1 | ppl 1.07 | wps 87457.6 | wpb 4438 | bsz 333.3 | num_updates 3150 | best_loss 1.038\n",
      "2022-11-25 09:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 3150 updates\n",
      "2022-11-25 09:41:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 126 @ 3150 updates, score 1.04) (writing took 0.3921773040000289 seconds)\n",
      "2022-11-25 09:41:42 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
      "2022-11-25 09:41:42 | INFO | train | epoch 126 | loss 1.019 | nll_loss 0.174 | ppl 1.13 | wps 28413 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 3150 | lr 0.000563436 | gnorm 0.126 | clip 0 | train_wall 4 | gb_free 13.6 | wall 628\n",
      "2022-11-25 09:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 127:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:42 | INFO | fairseq.trainer | begin training epoch 127\n",
      "2022-11-25 09:41:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 127:  96% 24/25 [00:03<00:00,  6.55it/s]2022-11-25 09:41:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 127 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.98it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:47 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 1.039 | nll_loss 0.103 | ppl 1.07 | wps 79281.6 | wpb 4438 | bsz 333.3 | num_updates 3175 | best_loss 1.038\n",
      "2022-11-25 09:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 3175 updates\n",
      "2022-11-25 09:41:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 127 @ 3175 updates, score 1.039) (writing took 0.406901586999993 seconds)\n",
      "2022-11-25 09:41:47 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
      "2022-11-25 09:41:47 | INFO | train | epoch 127 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 27506.1 | ups 5.18 | wpb 5313.9 | bsz 400 | num_updates 3175 | lr 0.000561214 | gnorm 0.142 | clip 0 | train_wall 4 | gb_free 13.2 | wall 633\n",
      "2022-11-25 09:41:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 128:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:47 | INFO | fairseq.trainer | begin training epoch 128\n",
      "2022-11-25 09:41:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 128:  96% 24/25 [00:03<00:00,  6.63it/s]2022-11-25 09:41:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 128 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:51 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 1.04 | nll_loss 0.102 | ppl 1.07 | wps 87472.1 | wpb 4438 | bsz 333.3 | num_updates 3200 | best_loss 1.038\n",
      "2022-11-25 09:41:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 3200 updates\n",
      "2022-11-25 09:41:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 128 @ 3200 updates, score 1.04) (writing took 0.46432873100002325 seconds)\n",
      "2022-11-25 09:41:52 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
      "2022-11-25 09:41:52 | INFO | train | epoch 128 | loss 1.018 | nll_loss 0.172 | ppl 1.13 | wps 28448.4 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 3200 | lr 0.000559017 | gnorm 0.155 | clip 0 | train_wall 4 | gb_free 13.6 | wall 638\n",
      "2022-11-25 09:41:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 129:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:52 | INFO | fairseq.trainer | begin training epoch 129\n",
      "2022-11-25 09:41:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 129:  96% 24/25 [00:03<00:00,  7.41it/s]2022-11-25 09:41:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 129 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:41:56 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 1.04 | nll_loss 0.102 | ppl 1.07 | wps 86652.1 | wpb 4438 | bsz 333.3 | num_updates 3225 | best_loss 1.038\n",
      "2022-11-25 09:41:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 3225 updates\n",
      "2022-11-25 09:41:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 129 @ 3225 updates, score 1.04) (writing took 0.4031120299999884 seconds)\n",
      "2022-11-25 09:41:56 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
      "2022-11-25 09:41:56 | INFO | train | epoch 129 | loss 1.019 | nll_loss 0.174 | ppl 1.13 | wps 28716.3 | ups 5.4 | wpb 5313.9 | bsz 400 | num_updates 3225 | lr 0.000556846 | gnorm 0.153 | clip 0 | train_wall 4 | gb_free 13.5 | wall 642\n",
      "2022-11-25 09:41:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 130:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:41:56 | INFO | fairseq.trainer | begin training epoch 130\n",
      "2022-11-25 09:41:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 130:  96% 24/25 [00:03<00:00,  6.09it/s]2022-11-25 09:42:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 130 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  9.36it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:01 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 1.042 | nll_loss 0.104 | ppl 1.07 | wps 89048.6 | wpb 4438 | bsz 333.3 | num_updates 3250 | best_loss 1.038\n",
      "2022-11-25 09:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 3250 updates\n",
      "2022-11-25 09:42:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 130 @ 3250 updates, score 1.042) (writing took 0.40655038400007015 seconds)\n",
      "2022-11-25 09:42:01 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
      "2022-11-25 09:42:01 | INFO | train | epoch 130 | loss 1.017 | nll_loss 0.172 | ppl 1.13 | wps 27985.6 | ups 5.27 | wpb 5313.9 | bsz 400 | num_updates 3250 | lr 0.0005547 | gnorm 0.131 | clip 0 | train_wall 4 | gb_free 13.5 | wall 647\n",
      "2022-11-25 09:42:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 131:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:01 | INFO | fairseq.trainer | begin training epoch 131\n",
      "2022-11-25 09:42:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 131:  96% 24/25 [00:03<00:00,  6.21it/s]2022-11-25 09:42:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 131 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:05 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 1.041 | nll_loss 0.102 | ppl 1.07 | wps 85235.2 | wpb 4438 | bsz 333.3 | num_updates 3275 | best_loss 1.038\n",
      "2022-11-25 09:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 3275 updates\n",
      "2022-11-25 09:42:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 131 @ 3275 updates, score 1.041) (writing took 0.4161621239999249 seconds)\n",
      "2022-11-25 09:42:06 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)\n",
      "2022-11-25 09:42:06 | INFO | train | epoch 131 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28536.3 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 3275 | lr 0.000552579 | gnorm 0.128 | clip 0 | train_wall 4 | gb_free 13.3 | wall 652\n",
      "2022-11-25 09:42:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 132:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:06 | INFO | fairseq.trainer | begin training epoch 132\n",
      "2022-11-25 09:42:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 132:  96% 24/25 [00:03<00:00,  7.05it/s]2022-11-25 09:42:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 132 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:10 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 84296.7 | wpb 4438 | bsz 333.3 | num_updates 3300 | best_loss 1.038\n",
      "2022-11-25 09:42:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 3300 updates\n",
      "2022-11-25 09:42:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 132 @ 3300 updates, score 1.039) (writing took 0.4271671019999985 seconds)\n",
      "2022-11-25 09:42:10 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)\n",
      "2022-11-25 09:42:10 | INFO | train | epoch 132 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28618.5 | ups 5.39 | wpb 5313.9 | bsz 400 | num_updates 3300 | lr 0.000550482 | gnorm 0.134 | clip 0 | train_wall 4 | gb_free 13.4 | wall 656\n",
      "2022-11-25 09:42:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 133:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:10 | INFO | fairseq.trainer | begin training epoch 133\n",
      "2022-11-25 09:42:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 133:  96% 24/25 [00:03<00:00,  6.70it/s]2022-11-25 09:42:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 133 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.05it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:15 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 1.04 | nll_loss 0.1 | ppl 1.07 | wps 88936.5 | wpb 4438 | bsz 333.3 | num_updates 3325 | best_loss 1.038\n",
      "2022-11-25 09:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 3325 updates\n",
      "2022-11-25 09:42:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 133 @ 3325 updates, score 1.04) (writing took 0.4153422929999806 seconds)\n",
      "2022-11-25 09:42:15 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)\n",
      "2022-11-25 09:42:15 | INFO | train | epoch 133 | loss 1.017 | nll_loss 0.172 | ppl 1.13 | wps 28538.6 | ups 5.37 | wpb 5313.9 | bsz 400 | num_updates 3325 | lr 0.000548408 | gnorm 0.136 | clip 0 | train_wall 4 | gb_free 12.9 | wall 661\n",
      "2022-11-25 09:42:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 134:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:15 | INFO | fairseq.trainer | begin training epoch 134\n",
      "2022-11-25 09:42:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 134:  96% 24/25 [00:03<00:00,  6.26it/s]2022-11-25 09:42:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 134 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.73it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:19 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 1.039 | nll_loss 0.099 | ppl 1.07 | wps 89738.8 | wpb 4438 | bsz 333.3 | num_updates 3350 | best_loss 1.038\n",
      "2022-11-25 09:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 3350 updates\n",
      "2022-11-25 09:42:19 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 134 @ 3350 updates, score 1.039) (writing took 0.43445924699994976 seconds)\n",
      "2022-11-25 09:42:20 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)\n",
      "2022-11-25 09:42:20 | INFO | train | epoch 134 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28379.5 | ups 5.34 | wpb 5313.9 | bsz 400 | num_updates 3350 | lr 0.000546358 | gnorm 0.146 | clip 0 | train_wall 4 | gb_free 13.2 | wall 666\n",
      "2022-11-25 09:42:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 135:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:20 | INFO | fairseq.trainer | begin training epoch 135\n",
      "2022-11-25 09:42:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 135:  96% 24/25 [00:04<00:00,  6.64it/s]2022-11-25 09:42:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 135 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  8.87it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset: 100% 3/3 [00:00<00:00, 14.72it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:24 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 1.039 | nll_loss 0.099 | ppl 1.07 | wps 77233.1 | wpb 4438 | bsz 333.3 | num_updates 3375 | best_loss 1.038\n",
      "2022-11-25 09:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 3375 updates\n",
      "2022-11-25 09:42:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 135 @ 3375 updates, score 1.039) (writing took 0.5049600859999828 seconds)\n",
      "2022-11-25 09:42:25 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)\n",
      "2022-11-25 09:42:25 | INFO | train | epoch 135 | loss 1.016 | nll_loss 0.17 | ppl 1.13 | wps 26590.3 | ups 5 | wpb 5313.9 | bsz 400 | num_updates 3375 | lr 0.000544331 | gnorm 0.107 | clip 0 | train_wall 4 | gb_free 13.3 | wall 671\n",
      "2022-11-25 09:42:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 136:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:25 | INFO | fairseq.trainer | begin training epoch 136\n",
      "2022-11-25 09:42:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 136:  96% 24/25 [00:04<00:00,  6.62it/s]2022-11-25 09:42:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 136 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:29 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 1.039 | nll_loss 0.101 | ppl 1.07 | wps 88987.4 | wpb 4438 | bsz 333.3 | num_updates 3400 | best_loss 1.038\n",
      "2022-11-25 09:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 3400 updates\n",
      "2022-11-25 09:42:29 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 136 @ 3400 updates, score 1.039) (writing took 0.4287056850000681 seconds)\n",
      "2022-11-25 09:42:30 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)\n",
      "2022-11-25 09:42:30 | INFO | train | epoch 136 | loss 1.016 | nll_loss 0.171 | ppl 1.13 | wps 26098.6 | ups 4.91 | wpb 5313.9 | bsz 400 | num_updates 3400 | lr 0.000542326 | gnorm 0.148 | clip 0 | train_wall 4 | gb_free 13.5 | wall 676\n",
      "2022-11-25 09:42:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 137:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:30 | INFO | fairseq.trainer | begin training epoch 137\n",
      "2022-11-25 09:42:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 137:  96% 24/25 [00:03<00:00,  6.83it/s]2022-11-25 09:42:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 137 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.80it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:34 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 85866.9 | wpb 4438 | bsz 333.3 | num_updates 3425 | best_loss 1.038\n",
      "2022-11-25 09:42:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 3425 updates\n",
      "2022-11-25 09:42:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 137 @ 3425 updates, score 1.039) (writing took 0.43701630099997146 seconds)\n",
      "2022-11-25 09:42:34 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)\n",
      "2022-11-25 09:42:34 | INFO | train | epoch 137 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28321.4 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 3425 | lr 0.000540343 | gnorm 0.116 | clip 0 | train_wall 4 | gb_free 13.3 | wall 680\n",
      "2022-11-25 09:42:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 138:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:34 | INFO | fairseq.trainer | begin training epoch 138\n",
      "2022-11-25 09:42:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 138:  96% 24/25 [00:03<00:00,  7.15it/s]2022-11-25 09:42:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 138 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.17it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:39 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 1.04 | nll_loss 0.102 | ppl 1.07 | wps 85423.8 | wpb 4438 | bsz 333.3 | num_updates 3450 | best_loss 1.038\n",
      "2022-11-25 09:42:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 3450 updates\n",
      "2022-11-25 09:42:39 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 138 @ 3450 updates, score 1.04) (writing took 0.41287941200005207 seconds)\n",
      "2022-11-25 09:42:39 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)\n",
      "2022-11-25 09:42:39 | INFO | train | epoch 138 | loss 1.016 | nll_loss 0.17 | ppl 1.13 | wps 28818.5 | ups 5.42 | wpb 5313.9 | bsz 400 | num_updates 3450 | lr 0.000538382 | gnorm 0.13 | clip 0 | train_wall 4 | gb_free 13.2 | wall 685\n",
      "2022-11-25 09:42:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 139:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:39 | INFO | fairseq.trainer | begin training epoch 139\n",
      "2022-11-25 09:42:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 139:  96% 24/25 [00:03<00:00,  5.95it/s]2022-11-25 09:42:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 139 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:43 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 1.039 | nll_loss 0.101 | ppl 1.07 | wps 87334.6 | wpb 4438 | bsz 333.3 | num_updates 3475 | best_loss 1.038\n",
      "2022-11-25 09:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 3475 updates\n",
      "2022-11-25 09:42:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 139 @ 3475 updates, score 1.039) (writing took 0.43787432700003137 seconds)\n",
      "2022-11-25 09:42:44 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)\n",
      "2022-11-25 09:42:44 | INFO | train | epoch 139 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28429.5 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 3475 | lr 0.000536442 | gnorm 0.12 | clip 0 | train_wall 4 | gb_free 13.8 | wall 690\n",
      "2022-11-25 09:42:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 140:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:44 | INFO | fairseq.trainer | begin training epoch 140\n",
      "2022-11-25 09:42:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 140:  96% 24/25 [00:03<00:00,  6.45it/s]2022-11-25 09:42:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 140 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:48 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 1.041 | nll_loss 0.105 | ppl 1.08 | wps 85947.4 | wpb 4438 | bsz 333.3 | num_updates 3500 | best_loss 1.038\n",
      "2022-11-25 09:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 3500 updates\n",
      "2022-11-25 09:42:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 140 @ 3500 updates, score 1.041) (writing took 0.407432880999977 seconds)\n",
      "2022-11-25 09:42:48 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)\n",
      "2022-11-25 09:42:48 | INFO | train | epoch 140 | loss 1.016 | nll_loss 0.17 | ppl 1.13 | wps 28905.6 | ups 5.44 | wpb 5313.9 | bsz 400 | num_updates 3500 | lr 0.000534522 | gnorm 0.116 | clip 0 | train_wall 4 | gb_free 13.8 | wall 694\n",
      "2022-11-25 09:42:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 141:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:48 | INFO | fairseq.trainer | begin training epoch 141\n",
      "2022-11-25 09:42:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 141:  96% 24/25 [00:03<00:00,  6.57it/s]2022-11-25 09:42:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 141 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.26it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:53 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 1.04 | nll_loss 0.101 | ppl 1.07 | wps 85495.9 | wpb 4438 | bsz 333.3 | num_updates 3525 | best_loss 1.038\n",
      "2022-11-25 09:42:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 3525 updates\n",
      "2022-11-25 09:42:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 141 @ 3525 updates, score 1.04) (writing took 0.41644325600009324 seconds)\n",
      "2022-11-25 09:42:53 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)\n",
      "2022-11-25 09:42:53 | INFO | train | epoch 141 | loss 1.016 | nll_loss 0.17 | ppl 1.12 | wps 28274 | ups 5.32 | wpb 5313.9 | bsz 400 | num_updates 3525 | lr 0.000532624 | gnorm 0.111 | clip 0 | train_wall 4 | gb_free 13.6 | wall 699\n",
      "2022-11-25 09:42:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 142:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:53 | INFO | fairseq.trainer | begin training epoch 142\n",
      "2022-11-25 09:42:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 142:  96% 24/25 [00:03<00:00,  6.65it/s]2022-11-25 09:42:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 142 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.81it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:42:57 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 1.039 | nll_loss 0.101 | ppl 1.07 | wps 89226.8 | wpb 4438 | bsz 333.3 | num_updates 3550 | best_loss 1.038\n",
      "2022-11-25 09:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 3550 updates\n",
      "2022-11-25 09:42:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:42:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 142 @ 3550 updates, score 1.039) (writing took 0.43025651800007836 seconds)\n",
      "2022-11-25 09:42:58 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)\n",
      "2022-11-25 09:42:58 | INFO | train | epoch 142 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 28492.6 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 3550 | lr 0.000530745 | gnorm 0.141 | clip 0 | train_wall 4 | gb_free 13.5 | wall 704\n",
      "2022-11-25 09:42:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 143:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:42:58 | INFO | fairseq.trainer | begin training epoch 143\n",
      "2022-11-25 09:42:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 143:  96% 24/25 [00:03<00:00,  6.35it/s]2022-11-25 09:43:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 143 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.52it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:02 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 83258.1 | wpb 4438 | bsz 333.3 | num_updates 3575 | best_loss 1.038\n",
      "2022-11-25 09:43:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 3575 updates\n",
      "2022-11-25 09:43:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 143 @ 3575 updates, score 1.04) (writing took 0.39931886800002303 seconds)\n",
      "2022-11-25 09:43:02 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)\n",
      "2022-11-25 09:43:02 | INFO | train | epoch 143 | loss 1.016 | nll_loss 0.17 | ppl 1.13 | wps 27973.4 | ups 5.26 | wpb 5313.9 | bsz 400 | num_updates 3575 | lr 0.000528886 | gnorm 0.148 | clip 0 | train_wall 4 | gb_free 13.8 | wall 708\n",
      "2022-11-25 09:43:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 144:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:02 | INFO | fairseq.trainer | begin training epoch 144\n",
      "2022-11-25 09:43:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 144:  96% 24/25 [00:03<00:00,  6.65it/s]2022-11-25 09:43:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 144 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.43it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:07 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 75140.2 | wpb 4438 | bsz 333.3 | num_updates 3600 | best_loss 1.038\n",
      "2022-11-25 09:43:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 3600 updates\n",
      "2022-11-25 09:43:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 144 @ 3600 updates, score 1.039) (writing took 0.41637066099997355 seconds)\n",
      "2022-11-25 09:43:07 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)\n",
      "2022-11-25 09:43:07 | INFO | train | epoch 144 | loss 1.016 | nll_loss 0.171 | ppl 1.13 | wps 28371 | ups 5.34 | wpb 5313.9 | bsz 400 | num_updates 3600 | lr 0.000527046 | gnorm 0.146 | clip 0 | train_wall 4 | gb_free 13.3 | wall 713\n",
      "2022-11-25 09:43:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 145:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:07 | INFO | fairseq.trainer | begin training epoch 145\n",
      "2022-11-25 09:43:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 145:  96% 24/25 [00:03<00:00,  6.90it/s]2022-11-25 09:43:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 145 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:11 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 1.038 | nll_loss 0.101 | ppl 1.07 | wps 77833 | wpb 4438 | bsz 333.3 | num_updates 3625 | best_loss 1.038\n",
      "2022-11-25 09:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 3625 updates\n",
      "2022-11-25 09:43:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:43:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:43:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 145 @ 3625 updates, score 1.038) (writing took 0.8668214810001018 seconds)\n",
      "2022-11-25 09:43:12 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)\n",
      "2022-11-25 09:43:12 | INFO | train | epoch 145 | loss 1.014 | nll_loss 0.169 | ppl 1.12 | wps 26252 | ups 4.94 | wpb 5313.9 | bsz 400 | num_updates 3625 | lr 0.000525226 | gnorm 0.116 | clip 0 | train_wall 4 | gb_free 13.5 | wall 718\n",
      "2022-11-25 09:43:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 146:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:12 | INFO | fairseq.trainer | begin training epoch 146\n",
      "2022-11-25 09:43:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 146:  96% 24/25 [00:03<00:00,  6.16it/s]2022-11-25 09:43:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 146 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:16 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 1.039 | nll_loss 0.102 | ppl 1.07 | wps 84515.8 | wpb 4438 | bsz 333.3 | num_updates 3650 | best_loss 1.038\n",
      "2022-11-25 09:43:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 3650 updates\n",
      "2022-11-25 09:43:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 146 @ 3650 updates, score 1.039) (writing took 0.3840233989999433 seconds)\n",
      "2022-11-25 09:43:17 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)\n",
      "2022-11-25 09:43:17 | INFO | train | epoch 146 | loss 1.015 | nll_loss 0.169 | ppl 1.12 | wps 28417.6 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 3650 | lr 0.000523424 | gnorm 0.126 | clip 0 | train_wall 4 | gb_free 13.7 | wall 723\n",
      "2022-11-25 09:43:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 147:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:17 | INFO | fairseq.trainer | begin training epoch 147\n",
      "2022-11-25 09:43:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 147:  96% 24/25 [00:03<00:00,  6.17it/s]2022-11-25 09:43:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 147 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 11.77it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:21 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 77537.5 | wpb 4438 | bsz 333.3 | num_updates 3675 | best_loss 1.038\n",
      "2022-11-25 09:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 3675 updates\n",
      "2022-11-25 09:43:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 147 @ 3675 updates, score 1.039) (writing took 0.4327882820000468 seconds)\n",
      "2022-11-25 09:43:22 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)\n",
      "2022-11-25 09:43:22 | INFO | train | epoch 147 | loss 1.015 | nll_loss 0.17 | ppl 1.13 | wps 27392 | ups 5.15 | wpb 5313.9 | bsz 400 | num_updates 3675 | lr 0.000521641 | gnorm 0.126 | clip 0 | train_wall 4 | gb_free 13.6 | wall 728\n",
      "2022-11-25 09:43:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 148:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:22 | INFO | fairseq.trainer | begin training epoch 148\n",
      "2022-11-25 09:43:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 148:  96% 24/25 [00:03<00:00,  6.89it/s]2022-11-25 09:43:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 148 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.86it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:26 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 1.041 | nll_loss 0.104 | ppl 1.07 | wps 90993.4 | wpb 4438 | bsz 333.3 | num_updates 3700 | best_loss 1.038\n",
      "2022-11-25 09:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 3700 updates\n",
      "2022-11-25 09:43:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 148 @ 3700 updates, score 1.041) (writing took 0.41447445599999355 seconds)\n",
      "2022-11-25 09:43:26 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)\n",
      "2022-11-25 09:43:26 | INFO | train | epoch 148 | loss 1.015 | nll_loss 0.169 | ppl 1.12 | wps 28467 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 3700 | lr 0.000519875 | gnorm 0.142 | clip 0 | train_wall 4 | gb_free 13.7 | wall 732\n",
      "2022-11-25 09:43:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 149:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:26 | INFO | fairseq.trainer | begin training epoch 149\n",
      "2022-11-25 09:43:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 149:  96% 24/25 [00:03<00:00,  6.24it/s]2022-11-25 09:43:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 149 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:31 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 1.037 | nll_loss 0.099 | ppl 1.07 | wps 86714.5 | wpb 4438 | bsz 333.3 | num_updates 3725 | best_loss 1.037\n",
      "2022-11-25 09:43:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 3725 updates\n",
      "2022-11-25 09:43:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 149 @ 3725 updates, score 1.037) (writing took 0.8827271090000295 seconds)\n",
      "2022-11-25 09:43:32 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)\n",
      "2022-11-25 09:43:32 | INFO | train | epoch 149 | loss 1.015 | nll_loss 0.17 | ppl 1.13 | wps 25797.6 | ups 4.85 | wpb 5313.9 | bsz 400 | num_updates 3725 | lr 0.000518128 | gnorm 0.121 | clip 0 | train_wall 4 | gb_free 13.5 | wall 737\n",
      "2022-11-25 09:43:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 150:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:32 | INFO | fairseq.trainer | begin training epoch 150\n",
      "2022-11-25 09:43:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 150:  96% 24/25 [00:03<00:00,  6.34it/s]2022-11-25 09:43:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 150 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:36 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 1.041 | nll_loss 0.102 | ppl 1.07 | wps 83756.4 | wpb 4438 | bsz 333.3 | num_updates 3750 | best_loss 1.037\n",
      "2022-11-25 09:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 3750 updates\n",
      "2022-11-25 09:43:36 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 150 @ 3750 updates, score 1.041) (writing took 0.37528289799990944 seconds)\n",
      "2022-11-25 09:43:36 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)\n",
      "2022-11-25 09:43:36 | INFO | train | epoch 150 | loss 1.014 | nll_loss 0.169 | ppl 1.12 | wps 28405.6 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 3750 | lr 0.000516398 | gnorm 0.102 | clip 0 | train_wall 4 | gb_free 13.2 | wall 742\n",
      "2022-11-25 09:43:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 151:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:36 | INFO | fairseq.trainer | begin training epoch 151\n",
      "2022-11-25 09:43:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 151:  96% 24/25 [00:03<00:00,  6.47it/s]2022-11-25 09:43:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 151 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 11.16it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:40 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 1.042 | nll_loss 0.105 | ppl 1.08 | wps 59420.9 | wpb 4438 | bsz 333.3 | num_updates 3775 | best_loss 1.037\n",
      "2022-11-25 09:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 3775 updates\n",
      "2022-11-25 09:43:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 151 @ 3775 updates, score 1.042) (writing took 0.4803073169999834 seconds)\n",
      "2022-11-25 09:43:41 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)\n",
      "2022-11-25 09:43:41 | INFO | train | epoch 151 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 27849.7 | ups 5.24 | wpb 5313.9 | bsz 400 | num_updates 3775 | lr 0.000514685 | gnorm 0.112 | clip 0 | train_wall 4 | gb_free 13.3 | wall 747\n",
      "2022-11-25 09:43:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 152:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:41 | INFO | fairseq.trainer | begin training epoch 152\n",
      "2022-11-25 09:43:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 152:  96% 24/25 [00:04<00:00,  6.27it/s]2022-11-25 09:43:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 152 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.24it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:45 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 83349.1 | wpb 4438 | bsz 333.3 | num_updates 3800 | best_loss 1.037\n",
      "2022-11-25 09:43:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 3800 updates\n",
      "2022-11-25 09:43:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 152 @ 3800 updates, score 1.04) (writing took 0.4534439010000142 seconds)\n",
      "2022-11-25 09:43:46 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)\n",
      "2022-11-25 09:43:46 | INFO | train | epoch 152 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 27315.3 | ups 5.14 | wpb 5313.9 | bsz 400 | num_updates 3800 | lr 0.000512989 | gnorm 0.104 | clip 0 | train_wall 4 | gb_free 13.6 | wall 752\n",
      "2022-11-25 09:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 153:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:46 | INFO | fairseq.trainer | begin training epoch 153\n",
      "2022-11-25 09:43:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 153:  96% 24/25 [00:03<00:00,  6.25it/s]2022-11-25 09:43:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 153 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.85it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:50 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 91633.9 | wpb 4438 | bsz 333.3 | num_updates 3825 | best_loss 1.037\n",
      "2022-11-25 09:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 3825 updates\n",
      "2022-11-25 09:43:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 153 @ 3825 updates, score 1.039) (writing took 0.44255110500000683 seconds)\n",
      "2022-11-25 09:43:51 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)\n",
      "2022-11-25 09:43:51 | INFO | train | epoch 153 | loss 1.014 | nll_loss 0.169 | ppl 1.12 | wps 27751.1 | ups 5.22 | wpb 5313.9 | bsz 400 | num_updates 3825 | lr 0.00051131 | gnorm 0.125 | clip 0 | train_wall 4 | gb_free 13.5 | wall 757\n",
      "2022-11-25 09:43:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 154:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:51 | INFO | fairseq.trainer | begin training epoch 154\n",
      "2022-11-25 09:43:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 154:  96% 24/25 [00:03<00:00,  7.42it/s]2022-11-25 09:43:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 154 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:55 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 86048.8 | wpb 4438 | bsz 333.3 | num_updates 3850 | best_loss 1.037\n",
      "2022-11-25 09:43:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 3850 updates\n",
      "2022-11-25 09:43:55 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 154 @ 3850 updates, score 1.04) (writing took 0.35964608800009046 seconds)\n",
      "2022-11-25 09:43:55 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)\n",
      "2022-11-25 09:43:55 | INFO | train | epoch 154 | loss 1.017 | nll_loss 0.172 | ppl 1.13 | wps 28839.6 | ups 5.43 | wpb 5313.9 | bsz 400 | num_updates 3850 | lr 0.000509647 | gnorm 0.124 | clip 0 | train_wall 4 | gb_free 13.3 | wall 761\n",
      "2022-11-25 09:43:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 155:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:43:55 | INFO | fairseq.trainer | begin training epoch 155\n",
      "2022-11-25 09:43:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 155:  96% 24/25 [00:03<00:00,  7.10it/s]2022-11-25 09:43:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 155 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.65it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:43:59 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 1.04 | nll_loss 0.103 | ppl 1.07 | wps 86668.1 | wpb 4438 | bsz 333.3 | num_updates 3875 | best_loss 1.037\n",
      "2022-11-25 09:43:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 3875 updates\n",
      "2022-11-25 09:43:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 155 @ 3875 updates, score 1.04) (writing took 0.46324015499999405 seconds)\n",
      "2022-11-25 09:44:00 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)\n",
      "2022-11-25 09:44:00 | INFO | train | epoch 155 | loss 1.016 | nll_loss 0.171 | ppl 1.13 | wps 28169.1 | ups 5.3 | wpb 5313.9 | bsz 400 | num_updates 3875 | lr 0.000508001 | gnorm 0.131 | clip 0 | train_wall 4 | gb_free 13.6 | wall 766\n",
      "2022-11-25 09:44:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 156:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:00 | INFO | fairseq.trainer | begin training epoch 156\n",
      "2022-11-25 09:44:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 156:  96% 24/25 [00:03<00:00,  6.94it/s]2022-11-25 09:44:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 156 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.92it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:04 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 1.039 | nll_loss 0.1 | ppl 1.07 | wps 84642.3 | wpb 4438 | bsz 333.3 | num_updates 3900 | best_loss 1.037\n",
      "2022-11-25 09:44:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 3900 updates\n",
      "2022-11-25 09:44:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 156 @ 3900 updates, score 1.039) (writing took 0.45041096500017375 seconds)\n",
      "2022-11-25 09:44:05 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
      "2022-11-25 09:44:05 | INFO | train | epoch 156 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 27809 | ups 5.23 | wpb 5313.9 | bsz 400 | num_updates 3900 | lr 0.00050637 | gnorm 0.114 | clip 0 | train_wall 4 | gb_free 13.8 | wall 771\n",
      "2022-11-25 09:44:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 157:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:05 | INFO | fairseq.trainer | begin training epoch 157\n",
      "2022-11-25 09:44:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 157:  96% 24/25 [00:03<00:00,  6.18it/s]2022-11-25 09:44:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 157 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:09 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 1.037 | nll_loss 0.1 | ppl 1.07 | wps 84529.4 | wpb 4438 | bsz 333.3 | num_updates 3925 | best_loss 1.037\n",
      "2022-11-25 09:44:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 3925 updates\n",
      "2022-11-25 09:44:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:44:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 157 @ 3925 updates, score 1.037) (writing took 0.8642523129999518 seconds)\n",
      "2022-11-25 09:44:10 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
      "2022-11-25 09:44:10 | INFO | train | epoch 157 | loss 1.013 | nll_loss 0.168 | ppl 1.12 | wps 26144.8 | ups 4.92 | wpb 5313.9 | bsz 400 | num_updates 3925 | lr 0.000504754 | gnorm 0.123 | clip 0 | train_wall 4 | gb_free 13.6 | wall 776\n",
      "2022-11-25 09:44:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 158:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:10 | INFO | fairseq.trainer | begin training epoch 158\n",
      "2022-11-25 09:44:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 158:  96% 24/25 [00:03<00:00,  5.84it/s]2022-11-25 09:44:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 158 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.87it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:14 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 1.038 | nll_loss 0.1 | ppl 1.07 | wps 77058.6 | wpb 4438 | bsz 333.3 | num_updates 3950 | best_loss 1.037\n",
      "2022-11-25 09:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 3950 updates\n",
      "2022-11-25 09:44:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 158 @ 3950 updates, score 1.038) (writing took 0.39971608900009414 seconds)\n",
      "2022-11-25 09:44:15 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
      "2022-11-25 09:44:15 | INFO | train | epoch 158 | loss 1.013 | nll_loss 0.168 | ppl 1.12 | wps 28277.9 | ups 5.32 | wpb 5313.9 | bsz 400 | num_updates 3950 | lr 0.000503155 | gnorm 0.111 | clip 0 | train_wall 4 | gb_free 13.5 | wall 780\n",
      "2022-11-25 09:44:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 159:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:15 | INFO | fairseq.trainer | begin training epoch 159\n",
      "2022-11-25 09:44:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 159:  96% 24/25 [00:04<00:00,  6.19it/s]2022-11-25 09:44:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 159 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:19 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 1.039 | nll_loss 0.101 | ppl 1.07 | wps 86017.1 | wpb 4438 | bsz 333.3 | num_updates 3975 | best_loss 1.037\n",
      "2022-11-25 09:44:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 3975 updates\n",
      "2022-11-25 09:44:19 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 159 @ 3975 updates, score 1.039) (writing took 0.4185171740000442 seconds)\n",
      "2022-11-25 09:44:19 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
      "2022-11-25 09:44:19 | INFO | train | epoch 159 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 27235.3 | ups 5.13 | wpb 5313.9 | bsz 400 | num_updates 3975 | lr 0.00050157 | gnorm 0.107 | clip 0 | train_wall 4 | gb_free 13.3 | wall 785\n",
      "2022-11-25 09:44:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 160:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:19 | INFO | fairseq.trainer | begin training epoch 160\n",
      "2022-11-25 09:44:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 160:  96% 24/25 [00:03<00:00,  6.23it/s]2022-11-25 09:44:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 160 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.36it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:24 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 1.039 | nll_loss 0.103 | ppl 1.07 | wps 88122.9 | wpb 4438 | bsz 333.3 | num_updates 4000 | best_loss 1.037\n",
      "2022-11-25 09:44:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 4000 updates\n",
      "2022-11-25 09:44:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 160 @ 4000 updates, score 1.039) (writing took 0.4007042470000215 seconds)\n",
      "2022-11-25 09:44:24 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
      "2022-11-25 09:44:24 | INFO | train | epoch 160 | loss 1.013 | nll_loss 0.168 | ppl 1.12 | wps 28481 | ups 5.36 | wpb 5313.9 | bsz 400 | num_updates 4000 | lr 0.0005 | gnorm 0.1 | clip 0 | train_wall 4 | gb_free 13.6 | wall 790\n",
      "2022-11-25 09:44:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 161:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:24 | INFO | fairseq.trainer | begin training epoch 161\n",
      "2022-11-25 09:44:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 161:  96% 24/25 [00:03<00:00,  6.31it/s]2022-11-25 09:44:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 161 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.03it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:28 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 1.038 | nll_loss 0.102 | ppl 1.07 | wps 89437.9 | wpb 4438 | bsz 333.3 | num_updates 4025 | best_loss 1.037\n",
      "2022-11-25 09:44:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 4025 updates\n",
      "2022-11-25 09:44:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 161 @ 4025 updates, score 1.038) (writing took 0.41525419699996746 seconds)\n",
      "2022-11-25 09:44:29 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
      "2022-11-25 09:44:29 | INFO | train | epoch 161 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 28433.9 | ups 5.35 | wpb 5313.9 | bsz 400 | num_updates 4025 | lr 0.000498445 | gnorm 0.084 | clip 0 | train_wall 4 | gb_free 13.6 | wall 795\n",
      "2022-11-25 09:44:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 162:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:29 | INFO | fairseq.trainer | begin training epoch 162\n",
      "2022-11-25 09:44:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 162:  96% 24/25 [00:03<00:00,  5.95it/s]2022-11-25 09:44:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 162 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:33 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 1.038 | nll_loss 0.1 | ppl 1.07 | wps 90215.3 | wpb 4438 | bsz 333.3 | num_updates 4050 | best_loss 1.037\n",
      "2022-11-25 09:44:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 4050 updates\n",
      "2022-11-25 09:44:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 162 @ 4050 updates, score 1.038) (writing took 0.4148490140000831 seconds)\n",
      "2022-11-25 09:44:33 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
      "2022-11-25 09:44:33 | INFO | train | epoch 162 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 28648.5 | ups 5.39 | wpb 5313.9 | bsz 400 | num_updates 4050 | lr 0.000496904 | gnorm 0.122 | clip 0 | train_wall 4 | gb_free 13.6 | wall 799\n",
      "2022-11-25 09:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 163:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:33 | INFO | fairseq.trainer | begin training epoch 163\n",
      "2022-11-25 09:44:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 163:  96% 24/25 [00:03<00:00,  6.80it/s]2022-11-25 09:44:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 163 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 14.42it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:38 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 1.038 | nll_loss 0.099 | ppl 1.07 | wps 81349.5 | wpb 4438 | bsz 333.3 | num_updates 4075 | best_loss 1.037\n",
      "2022-11-25 09:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 4075 updates\n",
      "2022-11-25 09:44:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 163 @ 4075 updates, score 1.038) (writing took 0.4379189200001292 seconds)\n",
      "2022-11-25 09:44:38 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
      "2022-11-25 09:44:38 | INFO | train | epoch 163 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 28194.5 | ups 5.31 | wpb 5313.9 | bsz 400 | num_updates 4075 | lr 0.000495377 | gnorm 0.142 | clip 0 | train_wall 4 | gb_free 13.7 | wall 804\n",
      "2022-11-25 09:44:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 164:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:38 | INFO | fairseq.trainer | begin training epoch 164\n",
      "2022-11-25 09:44:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 164:  96% 24/25 [00:03<00:00,  6.06it/s]2022-11-25 09:44:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 164 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 13.89it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:42 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 1.04 | nll_loss 0.102 | ppl 1.07 | wps 77890.9 | wpb 4438 | bsz 333.3 | num_updates 4100 | best_loss 1.037\n",
      "2022-11-25 09:44:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 4100 updates\n",
      "2022-11-25 09:44:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 164 @ 4100 updates, score 1.04) (writing took 0.40399164900009055 seconds)\n",
      "2022-11-25 09:44:43 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
      "2022-11-25 09:44:43 | INFO | train | epoch 164 | loss 1.015 | nll_loss 0.17 | ppl 1.13 | wps 27885.1 | ups 5.25 | wpb 5313.9 | bsz 400 | num_updates 4100 | lr 0.000493865 | gnorm 0.121 | clip 0 | train_wall 4 | gb_free 13.5 | wall 809\n",
      "2022-11-25 09:44:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 165:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:43 | INFO | fairseq.trainer | begin training epoch 165\n",
      "2022-11-25 09:44:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 165:  96% 24/25 [00:03<00:00,  7.38it/s]2022-11-25 09:44:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 165 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  9.65it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:47 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 1.038 | nll_loss 0.1 | ppl 1.07 | wps 91802.8 | wpb 4438 | bsz 333.3 | num_updates 4125 | best_loss 1.037\n",
      "2022-11-25 09:44:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 4125 updates\n",
      "2022-11-25 09:44:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_last.pt\n",
      "2022-11-25 09:44:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_last.pt (epoch 165 @ 4125 updates, score 1.038) (writing took 0.4195108070000515 seconds)\n",
      "2022-11-25 09:44:48 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
      "2022-11-25 09:44:48 | INFO | train | epoch 165 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 28327.9 | ups 5.33 | wpb 5313.9 | bsz 400 | num_updates 4125 | lr 0.000492366 | gnorm 0.097 | clip 0 | train_wall 4 | gb_free 13.6 | wall 813\n",
      "2022-11-25 09:44:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 166:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:48 | INFO | fairseq.trainer | begin training epoch 166\n",
      "2022-11-25 09:44:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 166:  96% 24/25 [00:03<00:00,  6.95it/s]2022-11-25 09:44:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 166 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  67% 2/3 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "                                                                      \u001b[A2022-11-25 09:44:52 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 1.037 | nll_loss 0.101 | ppl 1.07 | wps 85845.7 | wpb 4438 | bsz 333.3 | num_updates 4150 | best_loss 1.037\n",
      "2022-11-25 09:44:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 4150 updates\n",
      "2022-11-25 09:44:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:44:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/fairseqexample/checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/esp-models/checkpoint_best.pt (epoch 166 @ 4150 updates, score 1.037) (writing took 0.8758308870001201 seconds)\n",
      "2022-11-25 09:44:53 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
      "2022-11-25 09:44:53 | INFO | train | epoch 166 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 25897.4 | ups 4.87 | wpb 5313.9 | bsz 400 | num_updates 4150 | lr 0.000490881 | gnorm 0.115 | clip 0 | train_wall 4 | gb_free 13.2 | wall 819\n",
      "2022-11-25 09:44:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 25\n",
      "epoch 167:   0% 0/25 [00:00<?, ?it/s]2022-11-25 09:44:53 | INFO | fairseq.trainer | begin training epoch 167\n",
      "2022-11-25 09:44:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 831, in train_step\n",
      "    **extra_kwargs,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/fairseq_task.py\", line 519, in train_step\n",
      "    optimizer.backward(loss)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "  # Train with default parameters, roughly the baseline in SIGMORPHON 2020 shared task\n",
    "# Let this run until the loss on the validation (dev) test no longer improves. (Maybe 10 minutes with a GPU).\n",
    "!bash ./train.sh esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jtZVNsAS3Fa",
    "outputId": "a912f5a8-4543-407d-bc24-ca94139f94bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-25 09:45:06 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/esp-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'tst.esp.input'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/esp/', 'source_lang': 'esp.input', 'target_lang': 'esp.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-25 09:45:06 | INFO | fairseq.tasks.translation | [esp.input] dictionary: 64 types\n",
      "2022-11-25 09:45:06 | INFO | fairseq.tasks.translation | [esp.output] dictionary: 40 types\n",
      "2022-11-25 09:45:06 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/esp-models/checkpoint_best.pt\n",
      "2022-11-25 09:45:08 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-11-25 09:45:08 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-11-25 09:46:36 | INFO | fairseq_cli.interactive | Total time: 90.177 seconds; translation time: 83.342\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test data - read in all the inputs from tst.esp.input \n",
    "# and generate outputs to the file tst.esp.output (this is slow and takes about a minute)\n",
    "!fairseq-interactive data-bin/esp/ --source-lang=esp.input --target-lang=esp.output --path=checkpoints/esp-models/checkpoint_best.pt --input=tst.esp.input | grep -P \"D-[0-9]+\" | cut -f3 > tst.esp.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hs1yRUYbTDoQ",
    "outputId": "aba30ad6-11cf-405c-c1b5-e803edadd1c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('< m e r c a d e a r > V NEG IMP 3 SG', '< n o # m e r c a d e e >'),\n",
       " ('< t r a p e a r > V NFIN', '< t r a p e a r >'),\n",
       " ('< a s i l a r > V SBJV PRS 1 SG', '< a s i l e >'),\n",
       " ('< n a d a r > V NEG IMP 3 PL', '< n o # n a d e n >'),\n",
       " ('< e n m a r a ñ a r > V POS IMP 2 SG', '< e n m a r a ñ a >'),\n",
       " ('< u b i c a r > V SBJV PST 3 SG LGSPEC1', '< u b i c a r a >'),\n",
       " ('< b u r l a r > V IND FUT 2 SG', '< b u r l a r á s >'),\n",
       " ('< c a r e c e r > V SBJV PST 2 SG LGSPEC1', '< c a r e c i e r a s >'),\n",
       " ('< e n t r a ñ a r > V SBJV FUT 3 SG', '< e n t r a ñ a r e >'),\n",
       " ('< a d e n t r a r > V SBJV FUT 2 SG', '< a d e n t r a r e s >'),\n",
       " ('< e n c a b e z a r > V IND PST 2 SG IPFV', '< e n c a b e z a b a s >'),\n",
       " ('< a r g u m e n t a r > V SBJV PST 2 SG', '< a r g u m e n t a s e s >'),\n",
       " ('< d e s a s i r > V IND PST 3 PL PFV', '< d e s a s i e r o n >'),\n",
       " ('< e n t e r a r > V SBJV FUT 1 PL', '< e n t e r á r e m o s >'),\n",
       " ('< v o l c a r > V SBJV FUT 1 SG', '< v o l c a r e >'),\n",
       " ('< c i a r > V COND 3 SG', '< c i a r í a >'),\n",
       " ('< c e n t r i f u g a r > V.PTCP PST FEM PL',\n",
       "  '< c e n t r i f u g a d a s >'),\n",
       " ('< d e s n a t a r > V SBJV FUT 3 PL', '< d e s n a t a r e n >'),\n",
       " ('< d e s p e r t a r s e > V SBJV PST 3 PL LGSPEC1',\n",
       "  '< s e # d e s p e r t a r a n >'),\n",
       " ('< m a s t u r b a r > V IND PST 2 SG IPFV', '< m a s t u r b a b a s >'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the generated outputs and inputs and display the first 20 side-by-side\n",
    "linesinput = [l.strip() for l in open(\"tst.esp.input\")]\n",
    "linesoutput = [l.strip() for l in open(\"tst.esp.output\")]\n",
    "tuple(zip(linesinput, linesoutput))[:20] # Look at 20 first test inputs and predicted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4raFo3n-aAHc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}